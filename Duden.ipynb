{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json \n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mentor_call(text):\n",
    "    url = \"https://mentor.duden.de/api/grammarcheck?_format=json\"\n",
    "    payload = {\"text\": text}\n",
    "    headers = {\n",
    "      'Content-Type':     'application/json',\n",
    "      'Cookie':           'premiumPlan=premium_monthly_pt_3; SSESS381362f01410e0efe1ed5d21ee33277c=_8zL6wcQbAvEJU3_TWy2TYSBFQEsJODo_qN7g9tzYQA',\n",
    "      'Accept':           '*/*',\n",
    "      'Accept-Encoding':  'gzip, deflate, br',\n",
    "      'Accept-Language':  'de,en-US;q=0.7,en;q=0.3',\n",
    "      'cache-control':    'no-cache',\n",
    "      'Connection':       'keep-alive',\n",
    "      'User-Agent':       'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:83.0) Gecko/20100101 Firefox/83.0',\n",
    "      'Host':             'mentor.duden.de',\n",
    "    }\n",
    "\n",
    "    cookies = {'premiumPlan': 'premium_monthly_pt_2',\n",
    "               'SSESS381362f01410e0efe1ed5d21ee33277c': '_8zL6wcQbAvEJU3_TWy2TYSBFQEsJODo_qN7g9tzYQA'} # Random generated cookie that worked.\n",
    "\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, cookies=cookies, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return json.loads(response.text)\n",
    "    else:\n",
    "        return 'FAILED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'Künstliche Intelligenz beeinflusst immer mehr Bereiche die das tägliche Leben begleiten. Dazu gehören zum Beispiel Autonomes Fahren, Produktempfehlungen oder medizinische Analysen, um ein paar Beispiele zu nennen. Einige Anwendungen dienen der Unterhaltung, andere haben das Ziel die Arbeit der Menschen zu erleichtern. Dabei gibt es sowohl kritische als weniger kritische Bereiche. Zu den weniger kritischen Bereichen gehören zum Beispiel die elektrische Zahnbürste oder Filmempfehlungen auf Netflix, aber auch KI-Systeme, die massiv das Leben der Menschen beeinflussen wie z. B. autonomes Fahren, oder durch computergestützte Diagnosen im medizinischen Bereich. Doch wer versichert dem Anwender, dass die Ergebnisse des Systemes korrekt sind? Immer mehr Unternehmen verwenden KI auch in kritischen Bereichen. So werden z.B. Mitarbeiter auf Basis der Entscheidung einer künstlichen Intelligenz entlassen. Die KI bewertet Mitarbeiter, um so deren Wert für das Unternehmen zu ermitteln. Ist der Wert des Mitarbeiters zu gering, wird dieser häufig ohne Begründung entlassen. Dem Ergebnis der KI wurde vertraut, ohne dass es möglich ist, die Bewertung des Angestellten nachvollziehen zu können. Die Leidtragenden einer solchen KI Entscheidung haben nur selten die Möglichkeit, sich gegen das Ergebnis zu wehren. Dies ist nur ein Beispiel im ethischen Bereich, in dem rationale KI immer häufiger zum Einsatz kommt, um Menschen unangenehme Entscheidungen abzunehmen. Das Vertrauen in diese Systeme ist zurecht gering, da eine verständliche Erklärung fehlt, mit der es möglich ist, die Entscheidung der KI nachvollziehen zu können. Häufig werden solche KI-Systeme auch als Blackbox bezeichnet. Die Eingaben werden entgegengenommen und es wird dazu ein passendes Ergebnis erzeugt. Wie das Ergebnis genau zustande kommt, kann nicht gesagt werden, da es durch die Blackbox erstellt wurde. Durch die Unverständlichkeit der Ergebnisse entwickeln Menschen ein starkes Misstrauen gegenüber künstlicher Intelligenz, was die Entwicklung weiterer KI-Systeme erschwert. Um dem entgegenzuwirken, ist es wichtig, dass KI-Systeme nicht nur ein Ergebnis produzieren, sondern gleichzeitig eine für Menschen verständliche Erklärung, wie das Ergebnis genau zustande kommt. In dieser Arbeit wird erklärbare künstliche Intelligenz untersucht. Dabei werden wichtige Faktoren vorgestellt, die das Vertrauen in eine KI beeinflussen. Diese wurden in einer Umfrage verwendet, um zu Untersuchen, wie die aktuelle Einstellung von Menschen gegenüber KI Anwendungen ist. Im Anschluss werden einige Verfahren vorgestellt, mit denen es möglich ist, die Entscheidung einer KI zu erklären. Zum Schluss werden noch zwei Anwendungsbeispiele vorgestellt, die stark von erklärbarer künstlicher Intelligenz profitieren könnten.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fehler: Hier Kommas einfügen? Bereiche die ---> \u001b[32mBereiche, die\u001b[0m\n",
      "Künstliche Intelligenz beeinflusst immer mehr \u001b[31mBereiche, die\u001b[0m das tägliche Leben begleiten.\n",
      "\n",
      "Fehler: Hier kleinschreiben? Autonomes ---> \u001b[32mautonomes\u001b[0m\n",
      "Dazu gehören zum Beispiel \u001b[31mautonomes\u001b[0m Fahren, Produktempfehlungen oder medizinische Analysen, um ein paar Beispiele zu nennen.\n",
      "\n",
      "Fehler: Hier ein Komma einfügen? Ziel die ---> \u001b[32mZiel, die\u001b[0m\n",
      "Einige Anwendungen dienen der Unterhaltung, andere haben das \u001b[31mZiel, die\u001b[0m Arbeit der Menschen zu erleichtern.\n",
      "\n",
      "Dabei gibt es sowohl kritische als weniger kritische Bereiche.\n",
      "\n",
      "Fehler: Hier das Komma löschen? Fahren, oder ---> \u001b[32mFahren oder\u001b[0m\n",
      "Zu den weniger kritischen Bereichen gehören zum Beispiel die elektrische Zahnbürste oder Filmempfehlungen auf Netflix, aber auch KI-Systeme, die massiv das Leben der Menschen beeinflussen wie z. B. autonomes \u001b[31mFahren oder\u001b[0m durch computergestützte Diagnosen im medizinischen Bereich.\n",
      "\n",
      "Doch wer versichert dem Anwender, dass die Ergebnisse des Systemes korrekt sind?\n",
      "\n",
      "Immer mehr Unternehmen verwenden KI auch in kritischen Bereichen.\n",
      "\n",
      "Fehler: Diese Abkürzung korrigieren? z.B. ---> \u001b[32mz. B.\u001b[0m\n",
      "So werden \u001b[31mz. B.\u001b[0m Mitarbeiter auf Basis der Entscheidung einer künstlichen Intelligenz entlassen.\n",
      "\n",
      "Die KI bewertet Mitarbeiter, um so deren Wert für das Unternehmen zu ermitteln.\n",
      "\n",
      "Ist der Wert des Mitarbeiters zu gering, wird dieser häufig ohne Begründung entlassen.\n",
      "\n",
      "Dem Ergebnis der KI wurde vertraut, ohne dass es möglich ist, die Bewertung des Angestellten nachvollziehen zu können.\n",
      "\n",
      "Die Leidtragenden einer solchen KI Entscheidung haben nur selten die Möglichkeit, sich gegen das Ergebnis zu wehren.\n",
      "\n",
      "Dies ist nur ein Beispiel im ethischen Bereich, in dem rationale KI immer häufiger zum Einsatz kommt, um Menschen unangenehme Entscheidungen abzunehmen.\n",
      "\n",
      "Das Vertrauen in diese Systeme ist zurecht gering, da eine verständliche Erklärung fehlt, mit der es möglich ist, die Entscheidung der KI nachvollziehen zu können.\n",
      "\n",
      "Häufig werden solche KI-Systeme auch als Blackbox bezeichnet.\n",
      "\n",
      "Die Eingaben werden entgegengenommen und es wird dazu ein passendes Ergebnis erzeugt.\n",
      "\n",
      "Wie das Ergebnis genau zustande kommt, kann nicht gesagt werden, da es durch die Blackbox erstellt wurde.\n",
      "\n",
      "Durch die Unverständlichkeit der Ergebnisse entwickeln Menschen ein starkes Misstrauen gegenüber künstlicher Intelligenz, was die Entwicklung weiterer KI-Systeme erschwert.\n",
      "\n",
      "Um dem entgegenzuwirken, ist es wichtig, dass KI-Systeme nicht nur ein Ergebnis produzieren, sondern gleichzeitig eine für Menschen verständliche Erklärung, wie das Ergebnis genau zustande kommt.\n",
      "\n",
      "In dieser Arbeit wird erklärbare künstliche Intelligenz untersucht.\n",
      "\n",
      "Dabei werden wichtige Faktoren vorgestellt, die das Vertrauen in eine KI beeinflussen.\n",
      "\n",
      "Diese wurden in einer Umfrage verwendet, um zu Untersuchen, wie die aktuelle Einstellung von Menschen gegenüber KI Anwendungen ist.\n",
      "\n",
      "Im Anschluss werden einige Verfahren vorgestellt, mit denen es möglich ist, die Entscheidung einer KI zu erklären.\n",
      "\n",
      "Zum Schluss werden noch zwei Anwendungsbeispiele vorgestellt, die stark von erklärbarer künstlicher Intelligenz profitieren könnten.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(input_text)\n",
    "for sent in doc.sents:\n",
    "    sentence = sent.text\n",
    "    mentor = mentor_call(sentence)\n",
    "    \n",
    "    mentor = mentor['data']['spellAdvices']\n",
    "    mentor.reverse()\n",
    "    \n",
    "    for fail in mentor:\n",
    "        print('Fehler: {} {} ---> \\x1b[32m{}\\x1b[0m'.format(fail['shortMessage'], fail['originalError'], fail['proposals'][0]))\n",
    "        sentence = sentence[ : fail['offset']] + '\\x1b[31m' + fail['proposals'][0] + '\\x1b[0m' + sentence[fail['offset'] + fail['length'] : ]\n",
    "        \n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "input_text = textract.process(\"D:\\\\Nico\\\\OneDrive\\\\Studium\\\\Bachelor\\\\Bachelor Thesis\\\\Abschlussarbeit_Nico_Wellermann.pdf\").decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "input_text = re.sub('\\\\n([0-9]+\\.)+.+\\\\r\\\\n', '', input_text)\n",
    "input_text = re.sub('\\(\\S+ [0-9]+\\)', '', input_text)\n",
    "input_text = re.sub('\\(\\S+ u. a. [0-9]+\\)', '', input_text)\n",
    "input_text = re.sub('\\((\\S+,? )+[0-9]+\\)', '', input_text)\n",
    "input_text = input_text.replace('\\r', '')\n",
    "input_text = input_text.replace('\\n', ' ')\n",
    "input_text = input_text.replace('  ', ' ')\n",
    "input_text = input_text.replace('. .', '')\n",
    "input_text = re.sub('[0-9]+ \\\\x0cKapitel [0-9]+ \\S+', '', input_text)\n",
    "input_text = input_text.replace('  ', ' ')\n",
    "input_text = input_text.replace('\\x0c', '')\n",
    "input_text = input_text.replace('“', '\"')\n",
    "input_text = input_text.replace('”', '\"')\n",
    "input_text = input_text.replace(':)', '')\n",
    "input_text = input_text.replace(':(', '')\n",
    "input_text = re.sub('\\s+', ' ', input_text)\n",
    "input_text = input_text.replace('  ', ' ')\n",
    "input_text = input_text.replace('  ', ' ')\n",
    "\n",
    "input_text = re.sub(r'^.*?1950 erfand der britische Mathematiker Alan Turing den nach', '1950 erfand der britische Mathematiker Alan Turing den nach', input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fehler: Groß- bzw. Kleinschreibung überprüfen. TuringTest ---> \u001b[32mTuringtest\u001b[0m\n",
      "1950 erfand der britische Mathematiker Alan Turing den nach ihm benannten \"\u001b[31mTuringtest\u001b[0m\".\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? verfügen ---> \u001b[32mverfugen\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? über ---> \u001b[32mAber\u001b[0m\n",
      "Fehler: Hier ein Komma einfügen? feststellen ob ---> \u001b[32mfeststellen, ob\u001b[0m\n",
      "Dieser Test sollte \u001b[31mfeststellen, ob\u001b[0m Menschen und Maschinen \u001b[31mAber\u001b[0m die gleiche Art von Intelligenz \u001b[31mverfugen\u001b[0m.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? Gegenüber ---> \u001b[32mGegenüber\u001b[0m\n",
      "Hierbei versucht der Mensch durch Konversation zu entscheiden, ob es sich bei seinem \u001b[31mGegenüber\u001b[0m um eine Maschine oder eine Person handelt.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? führte ---> \u001b[32mfuchste\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? künstlichen ---> \u001b[32mkünstlichen\u001b[0m\n",
      "Jedoch besteht dieser Test nur als theoretischer Versuch, der erst mit dem Durchbruch der \u001b[31mkünstlichen\u001b[0m Intelligenz zu praktischen Versuchen \u001b[31mfuchste\u001b[0m.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Groß- bzw. Kleinschreibung überprüfen. TuringTests ---> \u001b[32mTuringtests\u001b[0m\n",
      "Fehler: Unbekannter Eigenname. Searle ---> \u001b[32mSearle\u001b[0m\n",
      "Bereits 1980 widerlegte der amerikanische Philosoph John \u001b[31mSearle\u001b[0m die Theorie des \"\u001b[31mTuringtests\u001b[0m\".\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? müssen ---> \u001b[32mmüssen\u001b[0m\n",
      "Mit dem Gedankenexperiment durch das sogenannte \"chinesische Zimmer\" zeigte er auf, dass Computer, nur weil diese immer die korrekte Antwort geben, keine Experten auf dem Gebiet sein \u001b[31mmüssen\u001b[0m.\n",
      "-----------------------------\n",
      "\n",
      "Das Experiment sieht folgendermaßen aus:\n",
      "-----------------------------\n",
      "\n",
      "In einem geschlossenen Raum sitzt eine Person, die weder Chinesisch spricht noch versteht.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Hier großschreiben? chinesisch ---> \u001b[32mChinesisch\u001b[0m\n",
      "Diese hat die Aufgabe, auf chinesisch gestellte Fragen anhand einer in seiner Muttersprache verfassten Anleitung auf \u001b[31mChinesisch\u001b[0m zu beantworten.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Hier ein Komma einfügen? ist chinesisch ---> \u001b[32mist, chinesisch\u001b[0m\n",
      "Personen außerhalb des Zimmers denken, dass der Mensch in dem Zimmer in der Lage \u001b[31mist, chinesisch\u001b[0m zu sprechen, obwohl dieser nur einfache Regeln befolgt.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Hier ein Komma einfügen? sind die ---> \u001b[32msind, die\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? Übertragen ---> \u001b[32mUmhertragen\u001b[0m\n",
      "\u001b[31mUmhertragen\u001b[0m auf Computer zeigt das Gedankenexperiment, dass die Computer nur weil sie richtige Ergebnisse produzieren, noch lange nicht in der Lage \u001b[31msind, die\u001b[0m Fragen bzw. die Situationen zu verstehen.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? künstliche ---> \u001b[32mkünstliche\u001b[0m\n",
      "Fehler: Hier ein Komma einfügen? verstehen — ---> \u001b[32mverstehen, —\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? über ---> \u001b[32mAber\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? Karnowski ---> \u001b[32mTarkowski\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? Arel ---> \u001b[32mAbel\u001b[0m\n",
      "Dennoch wird daran geforscht (\u001b[31mAbel\u001b[0m, Rose und \u001b[31mTarkowski\u001b[0m 2010), , dass Computer mithilfe von verschiedensten Algorithmen eine Art Bewusstsein \u001b[31mAber\u001b[0m Situationen erlernen und sogar \u001b[31mverstehen, —\u001b[0m also eine \u001b[31mkünstliche\u001b[0m Intelligenz entwickeln.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? natürliche ---> \u001b[32mnatürliche\u001b[0m\n",
      "Die \u001b[31mnatürliche\u001b[0m Sprachverarbeitung ist dabei auch ein Baustein, an dem stetig geforscht wird.\n",
      "-----------------------------\n",
      "\n",
      "Dabei wird unter anderem versucht, dem Computer die menschliche Sprache zu vermitteln.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? verändert ---> \u001b[32mverändert\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? Einführung ---> \u001b[32mEinführung\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? Marktplätzen ---> \u001b[32mMarktplatzes\u001b[0m\n",
      "Bei virtuellen \u001b[31mMarktplatzes\u001b[0m hat sich seit der \u001b[31mEinführung\u001b[0m viel \u001b[31mverändert\u001b[0m.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? Vorschläge ---> \u001b[32mVorschlage\u001b[0m\n",
      "Fehler: Diese Abkürzung korrigieren? z.B. ---> \u001b[32mz. B.\u001b[0m\n",
      "So wird \u001b[31mz. B.\u001b[0m das Kundenverhalten mithilfe neuronaler Netze analysiert, um gezielt \u001b[31mVorschlage\u001b[0m zu platzieren.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? verändert ---> \u001b[32mverändert\u001b[0m\n",
      "Die eigentliche Produktsuche hat sich hingegen kaum \u001b[31mverändert\u001b[0m.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? herkömmliche ---> \u001b[32mherkömmliche\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? natürlichen ---> \u001b[32mnaturreichen\u001b[0m\n",
      "In dieser Arbeit wird ein alternatives Verfahren untersucht, das mithilfe der \u001b[31mnaturreichen\u001b[0m Sprachverarbeitung die \u001b[31mherkömmliche\u001b[0m Produktsuche ersetzen soll.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Hier ein Komma einfügen? schwieriger Angebot ---> \u001b[32mschwieriger, Angebot\u001b[0m\n",
      "In der heutigen Zeit ist es \u001b[31mschwieriger, Angebot\u001b[0m und Nachfrage im E–Commerce zusammenzubringen.\n",
      "-----------------------------\n",
      "\n",
      "Die existierenden Systeme bieten nicht genug Freiheiten, um die Anfrage genau zu spezifizieren.\n",
      "-----------------------------\n",
      "\n",
      "Daher wird nicht nur das gesuchte Produkt, sondern auch viele andere Ergebnisse gefunden.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? umständlich ---> \u001b[32mumständlich\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? Fällen ---> \u001b[32mFallen\u001b[0m\n",
      "Mit verschiedenen Eingabefeldern kann die Auswahl genauer spezifiziert werden, was aber in den meisten \u001b[31mFallen\u001b[0m zu generisch oder schlicht zu \u001b[31mumständlich\u001b[0m ist und deshalb nicht verwendet wird.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? Für ---> \u001b[32mFuhr\u001b[0m\n",
      "\u001b[31mFuhr\u001b[0m\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? natürlicher ---> \u001b[32mnatürlicher\u001b[0m\n",
      "Anwender ist es viel \u001b[31mnatürlicher\u001b[0m etwas direkt zu beschreiben, als es in starre, vorgefertigte Formulare einzutragen.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? überführt ---> \u001b[32müberführt\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? verständliche ---> \u001b[32mverständliche\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? für ---> \u001b[32mfuhr\u001b[0m\n",
      "Diese Art der Eingabemethode setzt allerdings voraus, dass die Maschine den Nutzer versteht und dessen unstrukturierte Eingabe in eine \u001b[31mfuhr\u001b[0m den Computer \u001b[31mverständliche\u001b[0m Struktur \u001b[31müberführt\u001b[0m.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? benötigt ---> \u001b[32mbenötigt\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? für ---> \u001b[32mfuhr\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? können ---> \u001b[32mkönnen\u001b[0m\n",
      "Mit diesem Vorgehen \u001b[31mkönnen\u001b[0m sowohl Angebote als auch Nachfragen ohne mehrere Formularfelder auskommen, da nicht mehr \u001b[31mfuhr\u001b[0m jedes Attribut ein eigenes Feld \u001b[31mbenötigt\u001b[0m wird.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? möglich ---> \u001b[32mFolglich\u001b[0m\n",
      "Fehler: Hier großschreiben? eins ---> \u001b[32mEins\u001b[0m\n",
      "Ein weiterer Vorteil dieses Vorgehens ist, dass eine \u001b[31mEins\u001b[0m zu eins Umsetzung zu Sprachassistenten \u001b[31mFolglich\u001b[0m ist.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? übergeben ---> \u001b[32mübergeben\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? Assistant ---> \u001b[32mAssistent\u001b[0m\n",
      "Fehler: Diese Abkürzung korrigieren? z.B. ---> \u001b[32mz. B.\u001b[0m\n",
      "Dazu wird die Sprache von einem bereits existierenden Assistenten (\u001b[31mz. B.\u001b[0m Amazon Alexa oder Google \u001b[31mAssistent\u001b[0m) erkannt und als Textform an das System \u001b[31mübergeben\u001b[0m.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? häufiger ---> \u001b[32mhautiger\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? können ---> \u001b[32mkönnen\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? Vorschläge ---> \u001b[32mVorschlage\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? für ---> \u001b[32mfuhr\u001b[0m\n",
      "Durch den Einsatz von solchen Technologien ergeben sich Mehrwerte \u001b[31mfuhr\u001b[0m Unternehmen, da Kunden bessere \u001b[31mVorschlage\u001b[0m gemacht werden \u001b[31mkönnen\u001b[0m und diese somit \u001b[31mhautiger\u001b[0m zum Kaufen angeregt werden.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? häufiger ---> \u001b[32mhautiger\u001b[0m\n",
      "Auch kann durch die optimale Voraussetzung eines Sprachassistenten eine bequeme Alternative geboten werden, wodurch die Plattform \u001b[31mhautiger\u001b[0m verwendet und das Unternehmen attraktiver wird.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? Bedürfnisse ---> \u001b[32mBedürfnisse\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? Marktplätzen ---> \u001b[32mMarktplätzen\u001b[0m\n",
      "Der Einsatz von virtuellen \u001b[31mMarktplätzen\u001b[0m erfordert ein System, welches die Anwender sowie ihre \u001b[31mBedürfnisse\u001b[0m versteht und somit von Nutzern verwendet wird.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? geprüft ---> \u001b[32mgeprunkt\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? benötigt ---> \u001b[32mbenötigt\u001b[0m\n",
      "Zudem werden weitere Informationen der Waren \u001b[31mbenötigt\u001b[0m, damit die Eingabe auf diese Attribute \u001b[31mgeprunkt\u001b[0m werden kann.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? Machine ---> \u001b[32mMaschine\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? Language ---> \u001b[32mLangage\u001b[0m\n",
      "Fehler: Hier kleinschreiben? Natural ---> \u001b[32mnatural\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? könnten ---> \u001b[32mkonnten\u001b[0m\n",
      "Um diese Herausforderungen anzugehen, \u001b[31mkonnten\u001b[0m Technologien wie \u001b[31mnatural\u001b[0m \u001b[31mLangage\u001b[0m Processing (NLP) oder \u001b[31mMaschine\u001b[0m Learning (ML) verwendet werden.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Hier ein Komma einfügen? entwickeln anhand ---> \u001b[32mentwickeln, anhand\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? natürlicher ---> \u001b[32mnaturreicher\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? für ---> \u001b[32mfuhr\u001b[0m\n",
      "Ziel dieser Arbeit ist es, ein Konzept und Prototyp \u001b[31mfuhr\u001b[0m einen virtuellen Marktplatz mit \u001b[31mnaturreicher\u001b[0m Sprachverarbeitung zu \u001b[31mentwickeln, anhand\u001b[0m dessen die Bedienbarkeit und Performance untersucht wird.\n",
      "-----------------------------\n",
      "\n",
      "Im Vordergrund steht dabei das Erkennen bzw. Klassifizieren der Attribute aus einer Eingabesequenz.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? müssen ---> \u001b[32mmüssen\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? über ---> \u001b[32mAber\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? verfügt ---> \u001b[32mverfugt\u001b[0m\n",
      "Jede Produktkategorie \u001b[31mverfugt\u001b[0m \u001b[31mAber\u001b[0m eigene Attribute, die von einem solchen System erkannt werden \u001b[31mmüssen\u001b[0m.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? beschränkt ---> \u001b[32mbeschrankt\u001b[0m\n",
      "Zu Beginn werden diese reduziert und auf die Produktkategorie der Smartphones mit den Attributen: Produkt, Hersteller, Preis, Farbe, Kamera und Speicher \u001b[31mbeschrankt\u001b[0m.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Hier das Komma löschen? grammatikalisch, korrekte ---> \u001b[32mgrammatikalisch korrekte\u001b[0m\n",
      "Zudem wurde als Beispiel ein virtueller Marktplatz in Form eines Chatrooms betrachtet, was bedeutet, dass keine \u001b[31mgrammatikalisch korrekte\u001b[0m Schreibweise den Anfragen vorausgesetzt wird.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? größere ---> \u001b[32mgrößere\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? können ---> \u001b[32mkönnen\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? Abkürzungen ---> \u001b[32mAbkürzungen\u001b[0m\n",
      "Die Anwendung sollte in der Lage sein, sowohl \u001b[31mAbkürzungen\u001b[0m als auch Emojis handhaben zu \u001b[31mkönnen\u001b[0m, was eine \u001b[31mgrößere\u001b[0m Herausforderung an die Algorithmen stellt.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? Für ---> \u001b[32mFuhr\u001b[0m\n",
      "\u001b[31mFuhr\u001b[0m\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? Verständnis ---> \u001b[32mVerständnis\u001b[0m\n",
      "Fehler: Bitte am Satzanfang großschreiben. ein ---> \u001b[32mEin\u001b[0m\n",
      "\u001b[31mEin\u001b[0m besseres \u001b[31mVerständnis\u001b[0m kann folgendes Beispiel betrachtet werden:\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Hier ein Komma einfügen? gebraucht aber ---> \u001b[32mgebraucht, aber\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? iphone ---> \u001b[32miPhone\u001b[0m\n",
      "Fehler: Hier ein Komma einfügen? gegangen ich ---> \u001b[32mgegangen, ich\u001b[0m\n",
      "Fehler: Bitte großschreiben. handy ---> \u001b[32mHandy\u001b[0m\n",
      "Fehler: Bitte großschreiben. woche ---> \u001b[32mWoche\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? lz ---> \u001b[32mAz\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? Hiiii ---> \u001b[32mHaiti\u001b[0m\n",
      "\"\u001b[31mHaiti\u001b[0m, \u001b[31mAz\u001b[0m \u001b[31mWoche\u001b[0m ist mein \u001b[31mHandy\u001b[0m kaputt \u001b[31mgegangen, ich\u001b[0m suche deshalb jetzt ein \u001b[31miPhone\u001b[0m x gerne auch \u001b[31mgebraucht, aber\u001b[0m nicht teurer als 500 e\".\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? iphone ---> \u001b[32miPhone\u001b[0m\n",
      "In diesem Szenario sollte das System \"\u001b[31miPhone\u001b[0m x\" als Produkt und \"500 e\" als Preis erkennen.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? würde ---> \u001b[32mwurde\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? Gefühl ---> \u001b[32mGefühl\u001b[0m\n",
      "Anwendern soll das \u001b[31mGefühl\u001b[0m vermittelt werden, als \u001b[31mwurde\u001b[0m das System sie verstehen.\n",
      "-----------------------------\n",
      "\n",
      "Dieses soll zudem mit einer geringen Menge von Daten erreicht werden.\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Leerzeichen löschen? ): \" ---> \u001b[32m): \"\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? FF1 ---> \u001b[32mF1\u001b[0m\n",
      "Beschrieben wird dieses Verhalten durch die Forschungsfrage (\u001b[31mF1\u001b[0m \u001b[31m): \"\u001b[0m\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? natürlicher ---> \u001b[32mnaturreicher\u001b[0m\n",
      "Wie kann aus \u001b[31mnaturreicher\u001b[0m unstrukturierter menschlicher Eingabe eine strukturierte Ausgabe erzeugt werden, die von einem Computer weiterverarbeitet werden kann?\".\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Leerzeichen löschen? 1 ) ---> \u001b[32m1)\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? NF1 ---> \u001b[32mF1\u001b[0m\n",
      "Begleitend dazu werden die folgenden Nebenfragen (\u001b[31mF1\u001b[0m[31m1)\u001b[0m beachtet:\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Leerzeichen löschen? ) \" ---> \u001b[32m)\"\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? NF2 ---> \u001b[32mF2\u001b[0m\n",
      "Fehler: Bitte am Satzanfang großschreiben. und ---> \u001b[32mUnd\u001b[0m\n",
      "\"Wie kann ein akzeptables Ergebnis mit einer sehr geringen Menge von Daten erzielt werden?\" \u001b[31mUnd\u001b[0m (\u001b[31mF2\u001b[0m \u001b[31m)\"\u001b[0m\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Dieses Wort korrigieren? Schlüsselwörter ---> \u001b[32mSchlüsselwörter\u001b[0m\n",
      "Fehler: Dieses Wort korrigieren? können ---> \u001b[32mkönnen\u001b[0m\n",
      "Wie \u001b[31mkönnen\u001b[0m die wichtigsten \u001b[31mSchlüsselwörter\u001b[0m aus einem Text ausgewertet werden?\".\n",
      "-----------------------------\n",
      "\n",
      "Fehler: Hier ein Komma einfügen? erreichen werden ---> \u001b[32merreichen, werden\u001b[0m\n",
      "Um dies zu \u001b[31merreichen, werden\u001b[0m verschiedene Methoden und Tools untersucht.\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(input_text)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for sent in doc.sents:\n",
    "    sentence = sent.text\n",
    "    count += 1\n",
    "    \n",
    "    \n",
    "    mentor = mentor_call(sentence)\n",
    "    \n",
    "    mentor = mentor['data']['spellAdvices']\n",
    "    mentor.reverse()\n",
    "    \n",
    "    for fail in mentor:\n",
    "        correction = fail['originalError']\n",
    "        if len(fail['proposals']) > 0:\n",
    "            correction = fail['proposals'][0]\n",
    "        \n",
    "        print('Fehler: {} {} ---> \\x1b[32m{}\\x1b[0m'.format(fail['shortMessage'], fail['originalError'], correction))\n",
    "        sentence = sentence[ : fail['offset']] + '\\x1b[31m' + correction + '\\x1b[0m' + sentence[fail['offset'] + fail['length'] : ]\n",
    "        \n",
    "    print(sentence)\n",
    "    print('-----------------------------')\n",
    "    print()\n",
    "    \n",
    "    if count > 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1950 erfand der britische Mathematiker Alan Turing den nach ihm benannten \"TuringTest\". Dieser Test sollte feststellen ob Menschen und Maschinen über die gleiche Art von Intelligenz verfügen. Hierbei versucht der Mensch durch Konversation zu entscheiden, ob es sich bei seinem Gegenüber um eine Maschine oder eine Person handelt. Jedoch besteht dieser Test nur als theoretischer Versuch, der erst mit dem Durchbruch der künstlichen Intelligenz zu praktischen Versuchen führte. Bereits 1980 widerlegte der amerikanische Philosoph John Searle die Theorie des \"TuringTests\". Mit dem Gedankenexperiment durch das sogenannte \"chinesische Zimmer\" zeigte er auf, dass Computer, nur weil diese immer die korrekte Antwort geben, keine Experten auf dem Gebiet sein müssen. Das Experiment sieht folgendermaßen aus: In einem geschlossenen Raum sitzt eine Person, die weder Chinesisch spricht noch versteht. Diese hat die Aufgabe, auf chinesisch gestellte Fragen anhand einer in seiner Muttersprache verfassten Anleitung auf chinesisch zu beantworten. Personen außerhalb des Zimmers denken, dass der Mensch in dem Zimmer in der Lage ist chinesisch zu sprechen, obwohl dieser nur einfache Regeln befolgt. Übertragen auf Computer zeigt das Gedankenexperiment, dass die Computer nur weil sie richtige Ergebnisse produzieren, noch lange nicht in der Lage sind die Fragen bzw. die Situationen zu verstehen. Dennoch wird daran geforscht (Arel, Rose und Karnowski 2010), , dass Computer mithilfe von verschiedensten Algorithmen eine Art Bewusstsein über Situationen erlernen und sogar verstehen — also eine künstliche Intelligenz entwickeln. Die natürliche Sprachverarbeitung ist dabei auch ein Baustein, an dem stetig geforscht wird. Dabei wird unter anderem versucht, dem Computer die menschliche Sprache zu vermitteln. Bei virtuellen Marktplätzen hat sich seit der Einführung viel verändert. So wird z.B. das Kundenverhalten mithilfe neuronaler Netze analysiert, um gezielt Vorschläge zu platzieren. Die eigentliche Produktsuche hat sich hingegen kaum verändert. In dieser Arbeit wird ein alternatives Verfahren untersucht, das mithilfe der natürlichen Sprachverarbeitung die herkömmliche Produktsuche ersetzen soll. In der heutigen Zeit ist es schwieriger Angebot und Nachfrage im E–Commerce zusammenzubringen. Die existierenden Systeme bieten nicht genug Freiheiten, um die Anfrage genau zu spezifizieren. Daher wird nicht nur das gesuchte Produkt, sondern auch viele andere Ergebnisse gefunden. Mit verschiedenen Eingabefeldern kann die Auswahl genauer spezifiziert werden, was aber in den meisten Fällen zu generisch oder schlicht zu umständlich ist und deshalb nicht verwendet wird. Für Anwender ist es viel natürlicher etwas direkt zu beschreiben, als es in starre, vorgefertigte Formulare einzutragen. Diese Art der Eingabemethode setzt allerdings voraus, dass die Maschine den Nutzer versteht und dessen unstrukturierte Eingabe in eine für den Computer verständliche Struktur überführt. Mit diesem Vorgehen können sowohl Angebote als auch Nachfragen ohne mehrere Formularfelder auskommen, da nicht mehr für jedes Attribut ein eigenes Feld benötigt wird. Ein weiterer Vorteil dieses Vorgehens ist, dass eine eins zu eins Umsetzung zu Sprachassistenten möglich ist. Dazu wird die Sprache von einem bereits existierenden Assistenten (z.B. Amazon Alexa oder Google Assistant) erkannt und als Textform an das System übergeben. Durch den Einsatz von solchen Technologien ergeben sich Mehrwerte für Unternehmen, da Kunden bessere Vorschläge gemacht werden können und diese somit häufiger zum Kaufen angeregt werden. Auch kann durch die optimale Voraussetzung eines Sprachassistenten eine bequeme Alternative geboten werden, wodurch die Plattform häufiger verwendet und das Unternehmen attraktiver wird. Der Einsatz von virtuellen Marktplätzen erfordert ein System, welches die Anwender sowie ihre Bedürfnisse versteht und somit von Nutzern verwendet wird. Zudem werden weitere Informationen der Waren benötigt, damit die Eingabe auf diese Attribute geprüft werden kann. Um diese Herausforderungen anzugehen, könnten Technologien wie Natural Language Processing (NLP) oder Machine Learning (ML) verwendet werden. Ziel dieser Arbeit ist es, ein Konzept und Prototyp für einen virtuellen Marktplatz mit natürlicher Sprachverarbeitung zu entwickeln anhand dessen die Bedienbarkeit und Performance untersucht wird. Im Vordergrund steht dabei das Erkennen bzw. Klassifizieren der Attribute aus einer Eingabesequenz. Jede Produktkategorie verfügt über eigene Attribute, die von einem solchen System erkannt werden müssen. Zu Beginn werden diese reduziert und auf die Produktkategorie der Smartphones mit den Attributen: Produkt, Hersteller, Preis, Farbe, Kamera und Speicher beschränkt. Zudem wurde als Beispiel ein virtueller Marktplatz in Form eines Chatrooms betrachtet, was bedeutet, dass keine grammatikalisch, korrekte Schreibweise den Anfragen vorausgesetzt wird. Die Anwendung sollte in der Lage sein, sowohl Abkürzungen als auch Emojis handhaben zu können, was eine größere Herausforderung an die Algorithmen stellt. Für ein besseres Verständnis kann folgendes Beispiel betrachtet werden: \"Hiiii, lz woche ist mein handy kaputt gegangen ich suche deshalb jetzt ein iphone x gerne auch gebraucht aber nicht teurer als 500 e\". In diesem Szenario sollte das System \"iphone x\" als Produkt und \"500 e\" als Preis erkennen. Anwendern soll das Gefühl vermittelt werden, als würde das System sie verstehen. Dieses soll zudem mit einer geringen Menge von Daten erreicht werden. Beschrieben wird dieses Verhalten durch die Forschungsfrage (FF1 ): \"Wie kann aus natürlicher unstrukturierter menschlicher Eingabe eine strukturierte Ausgabe erzeugt werden, die von einem Computer weiterverarbeitet werden kann?\". Begleitend dazu werden die folgenden Nebenfragen (NF1 ) beachtet: \"Wie kann ein akzeptables Ergebnis mit einer sehr geringen Menge von Daten erzielt werden?\" und (NF2 ) \"Wie können die wichtigsten Schlüsselwörter aus einem Text ausgewertet werden?\". Um dies zu erreichen werden verschiedene Methoden und Tools untersucht. In Kapitel 2 werden die verwendeten Techniken und Modelle zum Erreichen des Ziels dieser Arbeit vorgestellt. Kapitel 3 beginnt mit einer ausführlichen Betrachtung der Annahmen, gefolgt von den Abschnitten: Anforderungen, Komponenten, Prozesse und Bewertungskriterien. Im Anschluss wird in Kapitel 4 eine Pipeline konzipiert, die verschiedenen Algorithmen auf das Problem anwendet. Parallel zur Entwicklung der Pipeline wurde ein Prototyp erstellt, der mehrere Aufgaben erfüllt: einfacheres Sammeln von Daten, visuell unterstützte Bedienung der Pipeline, sowie das Aufstellen von Metriken bezüglich der Pipeline. Die Implementierung des Prototyps wird im Abschnitt 4.5 beschrieben. Eine ausführliche Betrachtung bezüglich der Performance des Prototyps sowie Auswertung der Oberfläche wird in Kapitel 5 erläutert. Zum Abschluss wird in Kapitel 6 das Ergebnis zusammengefasst und weitere zukünftige mögliche Erweiterungen werden in Kapitel 7 vorgestellt. In diesem Kapitel werden die verwendeten Technologien und theoretischen Grundlagen vorgestellt, welche bei der Bearbeitung dieser Arbeit verwendet wurden. Zu Beginn werden Word Embeddings erläutert, welche ein wesentlicher Bestandteil dieser Arbeit darstellen. Im Anschluss werden die relevanten Eigenschaften der Modelle aus dem Bereich Machine Learning (ML) vorgestellt. Sätze bzw. Wörter können nicht direkt von einem Computer verstanden werden. Dieser benötigt eine andere Repräsentation, um den Inhalt des Satzes zu verstehen. Wenn zum Beispiel die beiden Sätze: \"Das Wetter heute ist schön\" und \"Das Wetter heute ist toll\" betrachtet werden, ergibt sich ein Vokabular mit den Worten: [Das, Wetter, heute, ist, schön, toll]. Das Vokabular muss alle Wörter umfassen, die vom System erkannt werden sollen. Mit der \"Bag–of–words\" Codierung können die beiden Sätze für den Computer verständlich gemacht werden . Dazu wird ein leerer Vektor der Dimension entsprechend der Länge des Vokabulars erstellt und mit 0 gefüllt, jede Position entspricht somit einem Wort. Im Anschluss wird für jedes Wort der Eingabe geprüft, an welcher Position es sich befindet und diese Stelle im Vektor inkrementiert. Für die beiden Beispielsätze würde die \"Bag–of–words\" Codierung so aussehen: [1, 1, 1, 1, 1, 0] und [1, 1, 1, 1, 0, 1]. Der Nachteil dieser Repräsentation ist, dass die Reihenfolge der Wörter verloren geht und somit keine Beziehungen mehr erkannt werden können. Auch können ähnliche Bedeutungen bei verschiedenen Wörtern nicht abgebildet werden, was bedeutet die Worte wie: \"schön\" und \"toll\" genauso verschieden sind wie \"Wetter\" und \"heute\". Word Embeddings bestehen aus vielen Wortvektoren die Wörter für den Computer verständlich darstellen. Mit Wortvektoren wird versucht die Beziehungen der Wörter zueinander beizubehalten und das Wort durch einen Vektor zu repräsentieren. Dabei liegen Wörter mit ähnlicher Bedeutung im Vektorraum nah zusammen, wohingegen Wörter mit unterschiedlicher Bedeutung weit auseinander liegen. Die Dimension des Vektors spiegelt die Genauigkeit des Word Embeddings wieder. Eine größere Dimension beschreibt jedes Wort genauer, benötigt aber auch mehr Speicherplatz sowie mehr Zeit zum Erstellen der Vektoren. Zum Erstellen der Wortvektoren können verschiedene Verfahren verwendet werden . Die Verfahren Word2Vec und Global Vectors (GloVe) erzeugen pro Wort einen kontextbasierten Wortvektor. Die Embeddings from Language Models (ELMo) Repräsentation kann für ein Wort mehrere Wortvektoren erstellen um verschiedene Kontexte abzubilden. Word2vec ist ein Verfahren, das Algorithmen verwendet die basierend auf dem Kontext einer Eingabe einen numerischen Vektor erzeugen. Im Gegensatz zu Bag–of–words (BoW) erhalten damit verschiedene Wörter in demselben Kontext einen ähnlichen Vektor. Dazu werden vier Schritte benötigt . Als Erstes werden die Daten für eine unüberwachte Vorhersage vorbereitet, also Eingabe und Ziel der Vorhersage als Tupel. Betrachtet wird dieser Satz: \"Als es an der Tür klingelte, rannte der Hund los.\", die ersten Tupel wären [als, es], [es, als] und [es, an]. Es wird über jedes Wort der Eingabe iteriert, dass aktuelle Wort ist dabei immer der erste Wert in dem Tupel. Die Fenstergröße gibt an, wie viele Wörter vor und nach dem aktuellen Wort beachtet werden sollen . Die betrachteten Wörter durch die Fenstergröße bilden den zweiten Wert des Tupel. Ein größeres Fenster bringt bessere Performance zulasten der Berechnungszeit. Im Anschluss wird eine Matrix erstellt, die für jedes Wort der gesamten Trainingsdaten einen zufällig erstellten Wortvektor bereitstellt. Dann folgt die Optimierung durch das neuronale Netzwerk. Dazu werden die Tupel einzeln verarbeitet und für die Eingabe wird der Wortvektor aus der Matrix verwendet. Das Netz berechnet basierend auf der Eingabe eine Vorhersage des nächsten Wortes, welches mit dem tatsächlichen nächsten Wort verglichen wird. Basierend auf dem Ergebnis wird sowohl das Netz als auch der Wortvektor optimiert. Nach dem Training wird die Matrix gespeichert und kann als Word Embedding verwendet werden. Ein Nachteil der Word2Vec Repräsentation ist, dass diese nur die umliegenden lokalen Wörter betrachtet, um daraus die Wortvektoren zu erstellen. Für den Satz \"Der Hund spielt auf der Couch.\" ist nicht eindeutig ob \"der\" in besonderer Beziehung zu \"Hund\" und \"Couch\" steht oder ob es sich bei \"der\" um ein Stoppwort handelt. Global Vectors (GloVe) betrachtet beide Repräsentationen, die globale sowie lokale Sicht. Wie von wird das Vokabular der Trainingsdaten in einer co–occurrence Matrix abgebildet. Eine beispielhafte co–occurrence Matrix kann der Abbildung 1 entnommen werden. Durch ein stochastisches Verfahren lässt sich berechnen wie relevant ein Wort zu einem gegebenen anderen Wort ist. Dabei gilt, dass ein hoher Wert (>1) eine hohe Relevanz und ein niedriger Wert (<1) irrelevantes Verhalten repräsentiert. Abbildung 1: Die co–occurrence Matrix für den Satz \"Heute gibt es Kuchen da es regnet\"mit einer Fenstergröße von 1 Neuronale Netze benötigen eine spezielle Darstellung von Wörtern. Bei Word Embeddings wird die Semantik von Wörtern in Form von Vektoren dargestellt. Ein bei diesem Ansatz nicht beachtetes Problem ist die Tatsache, dass ein Wort in unterschiedlichen Kontexten verschiedene Bedeutungen haben kann. \"Für meine Familie suche ich ein neues Schloss zum Wohnen\" und \"Für meine Tür suche ich ein neues Schloss\", in beiden Sätzen wird das Wort \"Schloss\" verwendet, aber die Bedeutung ist offensichtlich eine andere. Einfache Wortvektoren wie die vorgestellten GloVe und Word2Vec sind nicht in der Lage den Unterschied dieser Wörter zu erkennen, sie hätten dieselbe Bedeutung. Um dieses Verhalten besser abbilden zu können, werden contextualized word–embeddings verwendet. Anders als bei einfachen Word Embeddings, wird eine ganze Sequenz anstelle eines einzelnen Wortes eingegeben. Dadurch ist das Modell in der Lage, den Kontext der einzelnen Wörter zu ermitteln und kann so genauere Wortvektoren zurückgeben. Wie bereits bei den Word Embeddings werden diese Modelle nur in Ausnahmefällen selbst trainiert. Die benötigte Datenmenge, sowie die Zeit um diese Vektoren zu berechnen ist sehr groß . Deshalb wird auf vortrainierte Modelle zurückgegriffen, welche dann auf den eigenen Anwendungsfall optimiert werden. Zum Erstellen der Embeddings from Language Models (ELMo) wird ein Modell verwendet, dessen Aufgabe darin besteht, das nächste Wort einer Sequenz vorherzusagen. Diese Aufgabe kann unüberwacht ausgeführt werden und vereinfacht somit das Trainieren. Das Modell setzt dabei auf bidirectional Long Short–Term Memory (biLSTM) um ein Gefühl der vorherigen sowie nachfolgenden Wörter zu erhalten. Zum Erstellen der endgültigen Wortvektoren werden die Ergebnisse der einzelnen biLSTM Zellen verwendet. Im ersten Schritt werden die Ergebnisse aus der Vorhersagerichtung sowie der Rückrichtung verkettet. Im Anschluss werden die Vektoren mit einer Gewichtung des Modells multipliziert und zum Schluss summiert. Das Ergebnis ist ein kontextsensitives Word Embedding jedes Wortes eines Satzes. Abbildung 2: Beispielhafte Visualisierung der Wortvektoren für das Wort Schloss Abbildung 2 zeigt eine stark vereinfachte Visualisierung des Word Embedding. Der Wortvektor für das Wort \"Schloss\" ist, basierend auf den Kontext, verschieden. Wortvektoren können beliebig viele Dimensionen haben, weshalb dieses eine vereinfachte Darstellung ist. Um eine genauere Vorhersage über die Bedeutung der Worte treffen zu können, ist die Betrachtung des Kontextes hilfreich. Es gibt verschiedene modellbasierte Ansätze, um diesen Kontext zu erfassen. Für die Convolutional Neural Network (CNN) liegt der Schwerpunkt in der Bildverarbeitung . Die Pixelwerte eines Bildes werden verwendet, um daraus Vorhersagen zu treffen, was auf dem Bild zu erkennen ist. Um dieses Ziel zu erreichen werden einfache, Hardware unterstützte Verfahren verwendet, die im Folgenden vorgestellt werden. Generell bestehen CNN aus den drei folgenden, miteinander verknüpften Ebenen (Kalchbrenner, Grefenstette und Blunsom 2014): Eine Faltungsebene, bei der die Eingabematrix mit Hilfe eines Filterkerns auf eine kleinere Matrix reduziert wird, z.B. 5 x 5 als Eingabematrix, 3 x 3 als Filterkern bei einer Schrittweite von 1 erzeugt eine 3 x 3 Matrix. Dazu wird der Filterkern über die Eingabematrix um die Schrittweite verschoben. Bei jedem Schritt werden die übereinander liegende Werte der Filter- und Eingabematrix multipliziert und anschließend alle Werte addiert, um den neuen Wert der Ergebnismatrix zu erhalten. Durch Reduzieren der Matrix und Beibehalten der wesentlichen Informationen verringert die Pooling–Schicht die Anzahl der Parameter für die folgenden Ebenen. Dies wird durch Unterteilung der Eingabematrix erreicht. Die Werte der einzelnen Abschnitte werden auf verschiedene Arten verarbeitet, wie z.B. Durchschnitts–Pooling oder Max–Pooling. Bei dem Durchschnitts–Pooling wird der Durchschnitt der Werte eines Abschnitts gebildet und als neuer Wert in die Ergebnismatrix eingetragen. Bei Max–Pooling wird der maximale Wert eines Abschnittes übernommen. Die vollständig verbundene Schicht bildet die vorletzte Ebene eines CNN und ist eine normale neuronale Netzstruktur. Die Matrix der vorherigen Schicht wird ausgerollt und an die Eingabe–Neuronen übergeben. Diese sind jeweils mit den Neuronen der nächsten Schicht vollständig verknüpft, bis eine Verbindung zu den Ausgabe–Neuronen besteht. Zuletzt wird die Aktivierungsfunktion z.B. Softmax aufgerufen. Softmax–Aktivierung sorgt dafür, dass alle Werte der Ausgabe–Neuronen sich zu 1 addieren und so die Wahrscheinlichkeit der jeweiligen Ausgabe repräsentieren. Abbildung 3: Visualisierung eines CNN–Modells zur Satz Klassifizierung Abbildung 3 zeigt, wie Convolutional Neural Networks in der Sprachverarbeitung verwendet werden können. Die Eingabe muss dafür in Form einer Matrix vorhanden sein. Texte müssen zum Erfüllen dieses Kriteriums zunächst vorverarbeitet werden. Wie im Abschnitt 2.2 beschrieben, können Wörter auch als Vektoren repräsentiert werden. Für jedes Wort der Eingabe wird der zugehörige Vektor verwendet. Die resultierende Matrix hat die Größe n x m, wobei n der Länge des Satzes und m der Dimension des Wortvektors entspricht. Der Filterkern, der in der Faltungsebene angewendet wird, umfasst alle Dimensionen der Wortvektoren in x–Richtung. Die y–Richtung umfasst typischerweise 2 – 5 Wörter. Die nachfolgenden Schichten funktionieren wie für Pixel bereits beschrieben. Im letzten Schritt wird die Eingabe in Wahrscheinlichkeitswerten den möglichen Klassen zugewiesen. In vielen Fällen der Sprachverarbeitung ist der Kontext der Eingabe essenziell für das Erzielen des gewünschten Ergebnisses. Um diesen Kontext in einem neuronalen Netz darstellen zu können, muss eine gewisse Abhängigkeit bei den Eingabe–Neuronen gegeben sein. Bei anderen neuronalen Netzen agieren die Neuronen unabhängig voneinander. In der aktuellen Verarbeitung wird der vorherigen Eingabe sowie deren Ergebnis keine Bedeutung zuteil. Recurrent Neural Networks beziehen diese Informationen der vorherigen Schritte in die folgenden Verarbeitungen mit ein um ein kontextsensitives Ergebnis zu erzielen. Um die vorherige Sequenz von Wörtern mit einzubeziehen ist die grundlegende Architektur von RNN eine Schleife. Abbildung 4: Ausgerollte Darstellung eines RNN–Modells Wie in der Abbildung 4 zu erkennen ist, wird zunächst das erste Wort der Sequenz als Eingabe an das RNN übergeben. Das Netz berechnet basierend auf der Eingabe ein Ergebnis, welches zusätzlich mit dem nächsten Wort der Sequenz erneut an das Modell gegeben wird. Dieser Prozess wiederholt sich bis das letzte Wort der Sequenz verarbeitet wurde und ein endgültiges Ergebnis entsteht. Durch dieses Verfahren ist das Ergebnis abhängig von dem vorherigen Ergebnis, welches wiederum selbst abhängig von seinem vorherigen Ergebnis ist. Dadurch wird die gesamte Sequenz beachtet. Diese Art der Struktur lässt sich auch als Weiterleitung mit Speicherfunktion betrachten. Zum Anwenden von RNN–Modellen werden die Wörter der Eingabesequenz in Vektoren umgewandelt. Dafür können verschiedene Repräsentationen verwendet werden, welche bereits im Abschnitt Word Embeddings vorgestellt wurden. Die Sequenz von Vektoren wird nacheinander von dem RNN verarbeitet. Dazu wird der Vektor der Eingabe mit dem vorherigen Ergebnis verbunden und der entstehende Vektor wird in die Aktivierungsfunktion Tangens hyperbolicus (Tanh) gereicht. Die Funktion Tanh sorgt dafür, dass die Werte in dem Vektor zwischen -1 und +1 bleiben, da ohne diese Funktion einzelne Werte eine zu starke Gewichtung bekommen und die übrigen Werte keine Auswirkung haben. Das Ergebnis der Tanh Funktion ist die Ausgabe für den Schritt, der in der nächsten Iteration wieder als Eingabe verwendet wird. Ein Nachteil dieses Modells ist das Trainieren, da jede Eingabe von demselben Modell verarbeitet wird. So haben längere Sequenzen, bei denen das Ergebnis über den hinteren Teil der Eingabe entschieden wird, mehr Einfluss auf die Bewertung der einzelnen Neuronen als Wörter zu Beginn der Sequenz. Dadurch entsteht ein Ungleichgewicht und Sequenzen mit stark variierender Länge werden von dem Modell nur schwer bis gar nicht erlernt. Dieses Problem ist unter dem Namen vanishing gradient problem bekannt. Der Gradient, der für das Lernen verantwortlich ist, wird durch backpropagation so weit verkleinert, bis dieser keine Auswirkung mehr auf die Gewichtung der Neuronen nimmt. Durch diesen Effekt ist das Modell nicht in der Lage Neues zu erlernen. Auf den ersten Blick wirken RNNs und CNNs identisch, da beide den Kontext der Eingabe betrachten. Der wesentliche Unterschied ist, dass RNNs nur aus einer Schicht bestehen und das Ergebnis der vorherigen Berechnung als Eingabe für das nächste Wort betrachten. RNNs werden meistens für die Bearbeitung von Sequenzen verwendet. Bei CNNs wird die Eingabe durch mehrere Schichten verarbeitet und es werden direkt mehrere Wörter in einem Durchlauf betrachtet. Durch dieses Vorgehen wird der lokale, umliegende Kontext berücksichtigt und nicht die gesamte Eingabe. Der Hauptanwendungsbereich dieses Modells liegt in der Bildverarbeitung. In der Arbeit von wurde gezeigt, dass RNNs durch CNNs ersetzt werden können und diese bei Sequenzmodellierung deutlich bessere Ergebnisse erzielen als die betrachteten RNN–Modelle. RNN–Modelle haben Schwierigkeiten, die Informationen der längeren Sequenzen von früheren Schritten bis hin zu den späteren zu propagieren. Um das Problem zu lösen werden Long Short–Term Memory (LSTM)–Modelle (Cummins, Gers und Schmidhuber 1999) eingesetzt, welche eine Ergänzung zu RNN darstellen . Diese Modelle verwenden eine Art Schalter, mit dem reguliert werden kann, ob und welche Informationen gespeichert werden sollen. Mit diesem Vorgehen können wesentliche Informationen der Sequenz gezielt gespeichert werden. Das Modell bezieht nicht mehr die volle Sequenz zur Verarbeitung ein, wodurch das Auftreten des vanishing gradient problem reduziert wird. Die Architektur des Modells basiert auf der Verwendung von drei Gates. Diese entscheiden was mit der aktuellen Eingabe geschehen soll. Zusätzlich bietet die Architektur einen Zustand, der als Gedächtnis verwendet wird. Wie bereits RNN–Modelle verwenden auch LSTM–Modelle zusätzlich das vorherige Ergebnis um die Ausgabe zu erzeugen. Als Eingabe wird auch eine Vektor Repräsentation der Wörter verwendet, wie sie im Abschnitt Word Embeddings erläutert wurde. Alle Gates erhalten die Wortvektoren und das Ergebnis aus der vorherigen Berechnung als Eingabe. Für jedes Wort einer Sequenz wird das LSTM–Modell aufgerufen und ab dem ersten Wort wird der Zustand und das vorherige Ergebnis in die nächste Berechnung übergeben. Das Forget Gate entscheidet, welche Informationen der vorherigen Schritte behalten werden. Das Input Gate bestimmt, welche Informationen aktuell relevant sind und im Gedächtniszustand gespeichert werden sollen. Das Output Gate berechnet das Ergebnis, welches für das nächste Wort wiederverwendet wird. Abbildung 5: Verkettung und Aufbau der einzelnen LSTM Elemente Das Forget Gate entscheidet welche Informationen beibehalten oder verworfen werden. Dazu wird eine Sigmoidfunktion auf den Eingabevektor angewendet, um die Werte des Vektors zwischen 0 und 1 abzubilden. Dabei bedeutet eine 1, dass die Informationen beibehalten und die 0 das diese verworfen werden. Bei dem Input Gate wird der Eingabevektor von zwei Aktivierungsfunktionen verarbeitet. Die Sigmoidfunktion entscheidet, welche Informationen wichtig (1) oder unwichtig (0) sind. Die Tanh Funktion reguliert die Werte, damit sich diese zwischen -1 und 1 befinden und sich besser für die spätere Verarbeitung eignen. Im Anschluss werden die Ergebnisvektoren beider Funktionen multipliziert, um einen Vektor zu erhalten. Um den neuen Gedächtniszustand zu berechnen, wird der vorherige Zustand mit dem Ergebnis des Forget Gates multipliziert. Der daraus resultierende Vektor wird mit dem Ergebnis des Input Gates addiert, daraus ergibt sich ein neuer Zustand. Als letztes wird das Output Gate verwendet. Eine Sigmoidfunktion wird auf den Eingabevektor angewendet, der aktuelle Zustand wird an eine Tanh Funktion gereicht. Die Ergebnisse beider Funktionen werden multipliziert und wird als vorheriges Ergebnis in dem folgenden Schritt wieder verwendet. Das Output Gate berechnet also die Vektoren, die für den nächsten Schritt benötigt werden. Für ein besseres Verständnis der Elemente kann Abbildung 5 betrachtet werden. Eine Erweiterung sind Bidirektionalen Netze , welche auch die vorherigen Wörter für die Verarbeitung betrachten. Sowohl LSTM als auch RNN Netze können um die bidirektionale Komponente erweitert werden, um ein besseres Ergebnis zu erzielen. Dazu wird die Anzahl der verwendeten Zellen dupliziert und in umgekehrter Reihenfolge miteinander verbunden. Dadurch wird die Sequenz in beide Richtungen verarbeitet und Beziehungen — sowohl vor als auch nach dem Wort — werden beachtet. Conditional Random Field (CRF) sind diskriminierend und modellieren die bedingte Wahrscheinlichkeitsverteilung . Eingesetzt werden diese Modelle unter anderem in der Bild- und Textverarbeitung. In der grundlegenden Funktionsweise beschreibt das Modell die Abhängigkeiten sowie Unabhängigkeiten zwischen zufälligen Variablen. Diese Variablen bilden einen Graphen, aus dem sich die Wahrscheinlichkeiten berechnen lassen mit der die jeweilige Variable zutrifft. Bei CRF wird die bedingte Wahrscheinlichkeitsverteilung betrachtet, dazu wird die Wahrscheinlichkeit der Klasse Y — unter der Annahme, dass die Eingabe X gilt — gesucht . Für ein besseres Verständnis wird im folgendem ein Beispiel aus dem Bereich der natürlichen Sprachverarbeitung betrachtet. Abbildung 6: Beispielhafte Darstellung eines CRF Die Eingabedaten der CRFs sind sequentiell und der frühere Kontext wird berücksichtigt um eine Vorhersage treffen zu können. Um dieses Verhalten modellieren zu können, werden Feature–Funktionen mit vier Eingabewerten verwendet. Diese sind: • die Wortvektoren für jedes Wort der Eingabe • die Position des Wortes, für die der Bezeichner bestimmt werden soll • die korrekte Bezeichnung des vorherigen Wortes • die korrekte Bezeichnung des gesuchten Wortes Im Anschluss wird eine Merkmalsfunktion definiert, die das gewünschte Verhalten abbildet. Zum Trainieren werden die Gewichtungen zufällig bestimmt und mit dem Gradientenabstiegsverfahren optimiert bis die Parameterwerte konvergieren. Dieses Verfahren ist der logistischen Regression ähnlich, da beide die bedingte Wahrscheinlichkeitsverteilung verwenden. Der Unterschied besteht darin, dass durch die Erweiterung von Feature– Funktionen eine sequenzielle Eingabe möglich ist. Die zweiwertige Logik ermöglicht das Modellieren von Verhalten und umfasst die Wahrheitswerte \"wahr\" und \"falsch\". Fuzzylogik erweitert die Menge der Wahrheitswerte (z.B. \"ein bisschen\", \"wenig\" und \"sehr\") um eine unscharfe Beschreibung zu ermöglichen . Abgebildet auf reelle Zahlen bedeutet das die Werte in dem Intervall [0,1]. Fuzzy (Unschärfe) ist eine Form der Ungenauigkeit bei der Abbildung eines Sachverhalts. Als Beispiel wird ein Zimmer betrachtet welches zwei Zustände haben kann: warm und kalt. Die zweiwertige Logik legt einen Grenzwert fest, ab wann der Übergang zwischen kalt zu warm ist z.B. 20 Grad Celsius. Bei Fuzzylogik wird eine weiche Grenze zwischen den Zuständen definiert und Werte wie 18,9 Grad Celsius werden beschrieben durch z.B. ein bisschen warm oder weniger kalt. Bei der unscharfen Suche auf Zeichenketten wird nicht auf die exakte Zeichenfolge, sondern ähnliche Zeichenketten geprüft. Die Levenshtein–Distanz ist ein Verfahren zur Messung der Differenz zwischen zwei Sequenzen . Die gesamte Distanz setzt sich dabei aus der benötigten Anzahl von Einfüge-, Lösch- und Ersetzung-Operationen zusammen, die benötigt werden, um ein Wort in das andere zu ändern. Betrachtet man die Wörter \"Tier\" und \"Tor\" kann der Buchstabe \"i\" durch ein \"o\" ersetzt werden und das \"e\" muss gelöscht werden. Somit beträgt die Levenshtein–Distanz 2. Die Distanz repräsentiert wie hoch eine Übereinstimmung dieser Wörter ist. Eine geringe Levenshtein– Distanz bedeutet dabei hohe Übereinstimmung. Ziel dieser Arbeit ist es, aus natürlicher unstrukturierter menschlicher Eingabe eine strukturierte Ausgabe zu erzeugen, die von einem Computer weiterverarbeitet werden kann. Hier könnte es sich beispielsweise um Empfehlungen von Produkten auf eine Suchanfrage handeln. Dabei ist es wichtig, dass die natürliche Eingabe des Menschen korrekt verstanden und ausgewertet wird. Im folgendem werden verschiedene Annahmen vorgestellt, die im Rahmen dieser Arbeit getroffen wurden.Sprachmodelle die mit mehreren Sprachen interagieren sollen sind wesentlich komplexer. Die Charakteristiken einer Sprache variieren sehr stark, weshalb ein Modell, welches auf die englische Sprache trainiert wurde, nicht direkt mit deutscher Eingabe bedient werden kann. Um dieser Problematik nicht zu begegnen wird nur die deutsche Sprache unterstützt. Für eine Lösung muss das Modell selbst in der Lage sein, auf die verschiedenen Sprachen zu reagieren oder es wird ein Modell verwendet, welches die eingegebene Sprache ermittelt und abhängig davon das passende Sprachmodell bereitstellt.Über virtuelle Marktplätze werden sämtliche Produkte gehandelt, die verschiedensten Attribute besitzen. Im Rahmen dieser Arbeit wird die Domäne beschränkt, mit der Möglichkeit diese nach Belieben zu erweitern. Technologische Artikel sind beliebte Produkte welche häufig über online Marktplätzen gehandelt werden, weshalb die Domäne zu Beginn auf diese Produkte begrenzt wird. Um diese noch weiter einzuschränken wurde sich an der Produktgruppe der Smartphones orientiert. Basierend auf dieser Gruppe wurden 6 Attribute — die häufig verwendet werden — gewählt, um Smartphones zu beschreiben. Diese sind in der Regel: Produkt, Hersteller, Preis, Farbe, Speicher und Kamera. Das in dieser Arbeit beschriebene Verfahren kann verwendet werden, um weitere Attribute bzw. Produktgruppen zu ergänzen.Ein bekanntes Problem bei Sprachmodellen ist, dass diese Schwierigkeiten haben Attribute zu bestimmen, bei Wörtern die nicht zusammenstehen. Aus diesem Grund wird angenommen, dass mehrere Wörter, die zu einem Attribut gehören, zusammen stehen und nicht von anderen Wörtern unterbrochen werden. Ein Gegenbeispiel dafür ist: \"Ich suche ein iPhone, am besten das 10.\", da hier das gesuchte Produkt \"iPhone 10\" nicht zusammenhängt. Wird diese Annahme nicht getroffen, könnte die Eingabe durch eine Vorverarbeitung umstrukturiert werden, sodass die Attribute wieder zusammenstehen.Eingaben zwischen normaler Konversation und Handelsanfragen zu unterscheiden ist ein zusätzliches Problem, welches nicht im Fokus dieser Arbeit steht. Es wird angenommen, dass jede Eingabe mindestens das Attribut \"Produkt\" enthält und somit eine Handelsanfrage darstellt — dadurch kann sich auf das Klassifizieren der Attribute fokussiert werden. Alternativ müsste ein zusätzliches Modell eingesetzt werden, welches auf die Differenzierung zwischen normaler Konversation und Handelsanfragen trainiert ist. Virtuelle Marktplätze werden von Jahr zu Jahr bedeutsamer. Auf elektronischen Marktplätzen finden viele Käufer–Verkäufer Situationen statt. Anwender erstellen ein digitales Angebot, das von potenziellen Käufern gefunden werden möchte. Dieser Ablauf beschreibt grob einen Anwendungsfall, welcher durch die natürliche Sprachverarbeitung unterstützt werden soll. Die genauen Abläufe könnten wie folgt aussehen.Ein Anwender kann eine Anfrage in Form einer Texteingabe an das System stellen. Die Eingabe wird von einer Komponente verarbeitet, in der die wesentlichen Informationen extrahiert werden. Das Ergebnis der Verarbeitung wird mit der Eingabe in einer Datenbank abgespeichert, damit zukünftige Handelsanfragen diese Anfrage finden können. Ebenfalls wird das Ergebnis an eine andere Komponente übergeben, welche basierend auf den extrahierten Informationen ein passendes Gegenangebot zurückgibt. Dieses wird im Anschluss dem Anwender mit den erkannten Attributen aus seiner Eingabe präsentiert.In einem spezielleren Szenario sucht ein Anwender nach Produkten von einem bestimmten Hersteller, ohne das gesuchte Produkt genau zu spezifizieren. Die Attribute aus der Eingabe werden wie in dem vorherigen Szenario von einer Komponente bestimmt. Die zweite Komponente, welche das passende Gegenangebot ermittelt reagiert auf die Suche nach einer Menge von Produkten. Basierend auf die übrigen Attribute der Eingabe wird dem Anwender ein passendes Gegenangebot angezeigt. Diese Szenarien verdeutlichen den Ablauf einer Suche über einen virtuellen Marktplatz.In dem letzten Szenario erstellt ein Anwender ein Inserat. Die Eingabe wird verarbeitet und das eigene Ergebnis dem Anwender präsentiert. Dieser entscheidet dann, ob die erkannten Attribute korrekt sind. Ist dies nicht der Fall, so soll dem Anwender die Möglichkeit geboten werden die Attribute in seiner Eingabe manuell zu bestimmen. Im Anschluss werden die Attribute sowie die Eingabe in einer Datenbank gespeichert. Der Nutzer erhält kein Gegenangebot. Anwender sollen mit dem System interagieren können. Deshalb wird die direkte Eingabe der Nutzer verwendet, was als natürliche Eingabe bezeichnet wird. Als Beispiel dafür wird eine virtuelle Verhandlung über einen Chat betrachtet in dem Produkte gehandelt werden. Jeder Mensch verfügt über eine eigene Art wie er sich in einem Chatroom ausdrückt, weshalb keine korrekte Rechtschreibung angenommen wird. Auch muss das System in der Lage sein mit Abkürzungen sowie Emojis umzugehen. Aus den genannten Anforderungen wird deutlich, welche Funktionen der Prototyp bereitstellen muss. Für die ersten beiden Anwendungsfälle wird eine Oberfläche erwartet, in der ein Anwender Eingaben tätigen kann und die Möglichkeit hat eine Antwort zu erhalten. Chatsysteme werden häufig bei Consumer–to–Consumer Transaktionen in virtuellen Marktplätzen eingesetzt, weshalb der Aufbau dem eines Chatrooms ähnlich sein soll. Der Anwender kann seine Anfrage dann in Form einer Nachricht in diesem Chat an das System senden. Wie es in einen Chatroom üblich ist, wird dem Anwender seine eigene Nachricht angezeigt und nach der Verarbeitung auch die Antwort des Systems. Die Verarbeitung darf nicht zu lange dauern, da sonst das Interesse der Anwender verloren geht. In der Oberfläche soll es dem Benutzer möglich sein, mehrere Anfragen nacheinander an das System zu senden mit der Möglichkeit weiterhin die vorherigen Ergebnisse angezeigt zu bekommen. Die Nachrichten des Anwenders und Systems sollten farblich differenzierbar und links bzw. rechtsbündig ausgerichtet sein. Anwender sind an diesen Aufbau von anderen Nachrichtensystem vertraut, wodurch der Einstieg in die Bedienung erleichtert wird. Für das letzte Szenario wird eine andere, simplere Oberfläche verwendet. Es wird keine Antwort von dem System erwartet, weshalb nur die Eingabe des Anwenders im Vordergrund steht. Durch eine Texteingabe wird die Anfrage des Anwenders entgegengenommen und ausgewertet. Der Prototyp stellt im Anschluss die Eingabe mit den gefundenen Attributen sowie der Möglichkeit das Ergebnis zu bestätigen oder abzulehnen dem Anwender dar. Der Anwendungsfall bietet dem Anwender die Möglichkeit die eigene Anfrage manuell mit Attributen zu versehen. Um diese Funktionalität anzubieten wird für jedes Attribut eine Schaltfläche verwendet, mit der die ausgewählte Sequenz dem jeweiligen Attribut zugewiesen werden kann. Formalisiert lassen sich aus den Beschreibungen der Szenarien sowie dem Ziel der Arbeit die folgenden Anforderungen erfassen. Die Anforderungen werden mit den Buchstaben FA und NA für funktionale bzw. nicht funktionale Anforderungen gekennzeichnet. FA1: Das System muss Eingaben in Form von Handelsanfragen von Anwendern ermöglichen. FA2: Das System muss basierend auf die Handelsanfragen passend antworten. FA3: Das System muss den Anwendern die Möglichkeit bieten, die Eingabe manuell mit Attributen zu versehen. FA4: Das System muss Gruppensuchen ermöglichen und mit einem passenden Angebot reagieren. FA5: Das System muss alle Anfragen persistieren. FA6: Das System muss mit bestimmte Anwenderfehler wie z.B. Rechtschreibfehler umgehen können. NA1: Aufrufe der Anwender müssen schnell (<3 s) verarbeitet und beantwortet werden. NA2: Anwendern wird das Gefühl vermittelt, von dem System verstanden zu werden. NA3: Die Antworten des Systems sind begründet und können von den Anwendern nachvollzogen werden. Abbildung 7: Aufbau der Anwendung mit den Komponenten Um die vorgestellten Szenarien aus Abschnitt 3.2 zu erfüllen werden fünf Komponenten benötigt, um einen Prototyp zu erstellen wie in Abbildung 7 dargestellt. In den folgenden Abschnitten werden die Komponenten genauer vorgestellt. Der Prototyp kann mit verschiedenen Oberflächen realisiert werden z.B. Webanwendung, Desktop–Anwendung, Android–App oder Kommandozeile. Was verwendet wird ist abhängig von dem Ziel, dass der Prototyp verfolgt. Ein Kriterium ist die einfache Zugänglichkeit, sodass viele Anwender den Prototypen problemlos benutzen können. Am einfachsten zugänglich ist eine Webanwendung, da diese im Webbrowser aufgerufen werden kann. Die anderen Möglichkeiten benötigen eine Installation oder zumindest eine ausführbare Projektdatei auf dem Endgerät. Die wesentliche Aufgabe des Prototyps ist es, Text entgegenzunehmen und zu verarbeiten (FA1 ). Aus diesem Grund ist ein weiteres Kriterium die Unterstützung der einfachen Texteingabe von der Oberfläche. In Smartphone Apps wird Text meist nur über die Bildschirmtastatur eingegeben was umständlicher ist als z.B. an einen Computer. In einer Kommandozeilen–Anwendung ist der Umgang mit langen Texteingaben ebenfalls nicht optimal da per Mausklick nicht an die gewünschte Stelle gesprungen wird. Das nächste Kriterium ist eine leicht verständliche Oberfläche, die intuitiv bedient werden kann. Die Oberfläche der Kommandozeile ist nicht benutzerfreundlich, weshalb viele Anwender davor zurückschrecken würden eine solche Anwendung zu benutzen. Bei Desktop– Anwendungen neigen Entwickler dazu ein komplett eigenes und kein einheitliches Design zu verwenden. Die Folge davon ist, dass Anwender sich erst an die Bedienung gewöhnen müssen. Bei Webanwendungen und Smartphone Apps wird mehr Wert auf ein einheitliches Design gelegt und sich an bereits existierende Anwendungen orientiert. Zum Erstellen dieser Arbeit wird eine Webanwendung erstellt. Webanwendungen bieten den Anwendern eine vertraute Oberfläche, die einfach zu bedienen ist. Webseiten können von den meisten Endgeräten aus aufgerufen werden, was die Verwendung von Smartphones miteinschließt. Das häufige Eingeben von Text wird durch die Verwendung einer Computertastatur erleichtert. Anwender müssen keinen Client updaten, um die neueste Version des Prototyps verwenden zu können. Es wird ein Server benötigt, auf dem die Webanwendung ausgeführt wird damit diese verfügbar ist. Die natürliche Eingabe des Anwenders wird an eine Komponente übergeben, welche die Attribute bestimmt um später eine passende Antwort generieren zu können. Bei der erhaltenen Eingabe kann von keiner korrekten Rechtschreibung ausgegangen werden (FA6 ). Die Attribute zeichnen sich durch besondere Charaktereigenschaften aus. So besteht der Preis meist aus einem beschreibenden Wort z.B. \"mindestens\" oder \"maximal\" gefolgt von einer Zahl mit einer abschließenden Einheit (z.B. \"e\", \"euro\"). Der strukturelle Aufbau der Attribute SSpeicheründ \"Kameraı̈st identisch, nur die Einheit ist eine Andere (\"Gigabyte (GB)\" bzw. \"Megapixel (MP)\"). Attribute wie Hersteller und Farbe bestehen in den allermeisten Fällen nur aus einzelnen Wörtern oder einer kleinen Wortkette z.B. \"Apple\" und \"helles grau\". Das Produkt hingegen besteht nicht nur aus Wörtern oder Wortketten, sondern es beinhaltet häufig genaue Artikelbezeichnungen, die in Form von Buchstaben konkateniert mit Zahlen dargestellt werden (z.B. \"Galaxy S10\"). Auch können Produkte den Namen des Herstellers beinhalten, was das Differenzieren beider Attribute erschwert. Abbildung 8 zeigt eine beispielhafte Eingabe in der die Attribute farblich hervorgehoben wurden: Hersteller (grau), Produkt (blau), Farbe (schwarz), Kamera (türkis), Speicher (gelb) und Preis (rot). Abbildung 8: Visuelle Unterstützung einer beispielhaften Eingabe Aufgrund der unterschiedlichen Eigenschaften der Attribute, wird eine Menge von Algorithmen verwendet, die gemeinsam eine Pipeline bilden. Diese erzeugt aus einer unstruktu- rierten Eingabe eine mit korrekten Bezeichnungen versehene Ausgabe. Die Pipeline setzt sich dabei aus unterschiedlichen Algorithmen zusammen, die nacheinander angewendet werden, um so ein optimales Ergebnis zu erzielen. Der Fokus dieser Arbeit beschränkt sich auf das Erkennen des Attributes \"Produkt\", da dieses bei allen Handelsanfragen enthalten sein muss. Die übrigen Attribute (Hersteller, Preis, Farbe, Speicher und Kamera) sollen zeigen, dass eine Erweiterung auf mehrere Attribute möglich ist. Die einzelnen Schritte der Pipeline sind aufsteigend gewichtet. Das bedeutet, dass die späteren Schritte Teilergebnisse der vorherigen überschreiben, weshalb die Stärken verschiedener Algorithmen kombiniert werden können. Der Aufbau der Pipeline ermöglicht ein einfaches Hinzufügen, Verschieben oder Entfernen von Algorithmen, welches Anpassungen an spezielle Anforderungen ermöglicht. Abbildung 9: Aufbau der Pipeline Abbildung 9 zeigt den konzeptionellen Aufbau der Pipeline. Wie zu erkennen besteht die Pipeline aus mehreren Bauteilen, die sich auf verschiedene charakteristische Eigenschaften der Attribute fokussieren. Mit diesem Aufbau werden die Stärken der einzelnen Schritte kombiniert, wodurch die Erkennung optimiert werden kann. Den ersten Schritt bildet ein neuronales Netz, dessen Hauptaufgabe das Erkennen des Attributes \"Produkt\" ist. Es gibt sehr viele verschiedene Produktbezeichnungen mit variierender Länge und Anzahl der Wörter, sodass neuronale Netze für diese Aufgabe benötigt werden. Die Netze sind durch Trainieren in der Lage, bestimmte Muster in den verschiedenen Eingaben zu erkennen, die zuvor von Menschen nicht erkannt wurden. Durch die Vielzahl von möglichen Produkten und dem Ziel alle möglichen Produkte zu erkennen wurde ein einfaches Vergleichsverfahren an dieser Stelle ausgeschlossen. Das System würde nur die zuvor definierten Produkte erkennen und neue Produkte müssten dauerhaft manuell hinzugefügt werden was nicht zielführend ist. Das neuronale Netz muss in der Lage sein, Wörter bzw. ganze Sätze als Eingabe entgegenzunehmen und eine Wortsequenz — die dem passenden Attribut zugewiesen wird — als Ausgabe zu erzeugen. Der nächste Schritt dient zum Erkennen von Attributen, die durch einen regelbasierten Ansatz erkannt werden können. Einige Attribute wie Preis, Speicher und Kamera folgen immer einem ähnlichen Muster, das durch reguläre Ausdrücke beschrieben werden kann. Die Werte dieser Attribute sind deutlich weniger variabel als die der übrigen Attribute. Zudem ist es unwahrscheinlich, dass in naher Zukunft neue Werte zu den Attributen hinzugefügt werden und diese somit sehr starr sind. Ein alternatives Vorgehen zur Erkennung dieser Attribute ist mit neuronalen Netzen. Das Trainieren eines neuronalen Netzes zum Erkennen dieser Attribute ist wesentlich aufwändiger und fehleranfälliger. Aus diesem Grund wurde sich gegen dieses Vorgehen und für die regulären Ausdrücke entschieden. In dem letzten Schritt wird ein einfacher Vergleich der Eingabe mit zuvor definierten Attribut Ausprägungen vorgenommen. Dieser Schritt wird für Attribute verwendet, welche nicht durch reguläre Ausdrücke beschrieben werden können. Die Eingabe wird mit zuvor definierten Werten, wie z.B. \"iPhone\" abgeglichen, um sicher zu stellen das zumindest diese Werte korrekt klassifiziert werden. Anders als der erste Schritt, in dem sämtliche Produkte identifiziert werden sollen, ist das Ziel des letzten Schrittes nur wenige, bestimmte Attribute zu erkennen. Aufgrund des anderen Zieles wird das folgende Verfahren in Betracht gezogen. Die gegebene Eingabe wird Wort für Wort mit den zuvor definierten Ausprägungen abgeglichen und bei einer Übereinstimmung ist das Attribut in der Eingabe enthalten. Der Nachteil dieses Ansatzes ist, dass die definierten Werte eine exakte Übereinstimmung in der Eingabe voraussetzen, da diese sonst nicht gefunden werden. Einen besseren Ansatz verfolgt die Fuzzylogik . Die Wörter benötigen keine exakte Übereinstimmung da auch ungenaue Ergebnisse zugelassen werden. Der Nachteil der exakten Übereinstimmung ist dadurch nicht mehr gegeben, weshalb für den letzten Schritt die Fuzzylogik betrachtet wird . Falsche Ergebnisse sind bei der Verwendung von neuronalen Netzen nicht auszuschließen, weshalb diese berücksichtigt werden müssen. Um zu verhindern, das falschen Daten persistiert werden wird eine Komponente benötigt, mit der das Ergebnis der Pipeline manuell nachgebessert werden kann (FA3 ). Die Komponente unterteilt sich dabei in zwei Unterfunktionen, zum einen das manuelle Setzen von Attributen und zum anderen das Bestimmen der Rolle (Käufer oder Verkäufer) aus einer Anfrage. Durch das richtige Setzen von Attributen können diese Anfragen korrekt in der Auswertungs–Komponente verwendet werden. Aus falsch persistiert Daten können unpassende Ergebnisse entstehen die dem Anwender auf seine Anfrage als Antwort vorgeschlagen werden. Selbiges gilt für falsch zugewiesene Rollen, auf eine Kaufanfrage könnte mit einer weiteren Kaufanfrage von dem System geantwortet werden. Eine solche Komponente, in der Anwender die Daten selbst annotieren können, kann verwendet werden, um Daten zu sammeln. Es kann auf keine Datengrundlage aufgebaut werden, die die Anforderungen erfüllen, weshalb dieses Vorgehen geeignet ist. Die gesammelten Daten können verwendet werden, um das benötigte neuronale Netz zu trainieren und so die Performance zu verbessern. Durch die manuelle Annotation der Daten können diese, ohne zusätzliche Bearbeitungsschritte verwendet werden, um den Ablauf zu vereinfachen. Mit dieser Struktur kann ein iterativer Ansatz zum Verbessern der Modelle erstellt werden. Es werden Daten gesammelt, die durch den Anwender korrekt annotiert sind. Ab einer gewissen Menge von neuen Daten wird der gesamte Datenbestand verwendet, um ein neues Modell zu trainieren welches das vorherige ersetzt. Anhand der Daten aus den späteren Iterationen lassen sich die Modelle evaluieren und können so verglichen werden. Zudem kann die Skalierung der Modelle mit mehr Daten gemessen werden, indem diese erneut trainiert und auf eine Verbesserung der Genauigkeit geprüft werden. Durch eine Automatisierung der Iteration wäre das System in der Lage sich kontinuierlich selbst zu verbessern und so immer mehr Eingaben korrekt zu erkennen. Der Handel zwischen Kunden ist ein wesentlicher Bestandteil eines virtuellen Marktplatzes. Um diesen zu ermöglichen, muss das System in der Lage sein, die Angebote und ggf. Nachfragen zu persistieren, damit diese von Kunden gefunden werden können (FA5 ). Zudem ist ein virtueller Marktplatz erst dann für Kunden attraktiv, wenn dieser über eine Vielzahl von Produkten im Sortiment verfügt. Zum Verwalten von großen Mengen an Daten wird meist auf Datenbanken zurückgegriffen. In der Datenbank werden die Benutzereingaben sowie die erkannten Attribute gespeichert. Wurde das Ergebnis der Verarbeitung von dem Anwender manuell verbessert wird das ebenfalls persistiert, um einen korrekt klassifizierten Datensatz zu erhalten. Für diese Arbeit wurde zwischen Graphdatenbanken und relationale Datenbank nach entschieden. Aus den vorgestellten Anwendungsfällen in Abschnitt 3.2 ist ersichtlich, dass die Daten keine Indizierung benötigen und überwiegend nur aus Text bestehen. Bezogen auf die Performance, sind Relationale Datenbanken den Graphdatenbanken in diesem Szenario unterlegen. Der zweite Anwendungsfall bezieht sich auf eine Gruppensuche, die viele Beziehungen der einzelnen Daten voraussetzt (z.B. Knowledge Graph). Vordefinierte Produkte können Beziehungen zu Herstellern haben und es können Obergruppen für bestimmte Kategorien angelegt werden. Die Komponente welche passende Gegenangebote ermittelt kann auf diese Daten zurückgreifen, um so bessere Ergebnisse zu erzielen. Dieses ist ein weiterer Vorteil der Graphdatenbanken nach weshalb diese im Rahmen dieser Arbeit verwendet werden. Die letzte Komponente erstellt die Antwort zu einer gegebenen Eingabe. Damit vorherigen Anfragen ausgewertet werden können, benötigt die Komponente Zugriff auf die Datenbank. Dabei muss die Rolle des Benutzers (Käufer oder Verkäufer) beachtet werden damit nur Anfragen der anderen Rolle ausgewertet werden. In dem speziellen Szenario aus Abschnitt 3.2 muss die Komponente erkennen, dass eine Gruppensuche erwartet wird (FA4 ). Aus den gefundenen Mengen der Ergebnisse muss anhand einer Strategie ein optimales Gegenangebot gefunden werden, welches zurückgegeben wird (FA2 ). Eine geeignete Strategie sucht das Angebot mit den meisten übereinstimmenden Attributen aus der Anfrage. Das Attribut \"Produkt\" muss mindestens übereinstimmen da es sich sonst um verschiedene Produkte handelt. Anhand der Rolle kann eine Regel für den Preis erstellt werden, als Verkäufer wird der höchste Preis bevorzugt, als Käufer der niedrigste. Mit dieser Strategie kann ein Matchmaking erstellt werden welches immer das passende Gegenangebot für eine Anfrage findet. Wenn kein Angebot gefunden werden konnte, sollte dieses ebenfalls wiedergegeben werden. Die erkannten Attribute aus der Eingabe sind auch in der zurückgegeben Antwort enthalten, um diese dem Anwender zu zeigen (NA3 ). Aus den in Abschnitt 3.2 vorgestellten Anwendungsfällen sind die im Abschnitt 3.3 erläuterten Komponente entstanden, welche alle Anforderungen (FA1 - FA6 ) erfüllen. Die Anwendungsfälle beschreiben bereits einen groben Ablauf in welcher Reihenfolge die Komponente aufgerufen werden. Im folgendem wird anhand der Szenarien deutlich, wie die Kommunikation zwischen den Komponenten dargestellt wird und welche Funktionen diese bereitstellen müssen.Die Szenarien 1. und 2. unterscheiden sich nur in der Art der Suche. In dem einen Szenario wird ein explizites Produkt gesucht und in dem anderen wird nach einem Produkt aus einer Produktgruppe gesucht. Der Ablauf der Szenarien wird durch diesen Unterschied nicht beeinflusst, weshalb beide Anwendungsfälle gemeinsam betrachtet werden. Eingeleitet wird die Suche durch das Eingeben einer Anfrage durch den Anwender. Diese wird an die Hybrid Named Entity Recognition (NER)–Komponente übergeben, welche eine Funktion bereitstellen muss, die eine Anfrage entgegennimmt. Die Eingabe sowie das Ergebnis der Verarbeitung wird an die Datenverwaltung zum Persistieren übergeben. Die Daten aus diesen Szenarien müssen mit einem zusätzlichen Vermerk gespeichert werden, da das Ergebnis nicht manuell verifiziert wurde und möglicherweise fehlerhaft sein kann. Im Anschluss wird das Ergebnis an die Auswertungs–Komponente übergeben. Dort wird bestimmt ob es sich um eine Gruppen- oder Produktsuche handelt und eine passende Anfrage an die Datenbank gestellt. Die Datenverwaltung benötigt zwei Funktionen. Die erste Funktion nimmt das gesuchte Produkt und die Rolle der Anfrage entgegen und gibt die gefundenen Anfragen mit enthaltenen Attributen zurück. Der zweiten Funktion wird eine Produktgruppe anstelle eines bestimmten Produktes mit der Rolle übergeben, die Rückgabe bleibt identisch. Die Ergebnisse der Datenbankanfrage werden ausgewertet und ein mögliches Gegenangebot mit den erkannten Attributen der eigenen Eingabe an die Oberfläche zurück übergeben. Die Oberfläche wird um die Antwort erweitert und die Suchanfrage ist beendet.Die Erstellung eines Inserats wird durch eine Benutzereingabe eingeleitet. Die Hybrid NER–Komponente muss eine Schnittstelle bereitstellen, die diese Eingabe entgegennimmt. Das Ergebnis wird zurück an die Oberfläche übergeben, um es dem Anwender zu präsentieren. Basierend auf der Rückmeldung wird die Eingabe sofort gespeichert oder einer manuellen Bearbeitung unterzogen. Der Tagging–Komponente wird die Eingabe übergeben und bietet dem Anwender in einer Oberfläche die Möglichkeit die Attribute der Eingabe manuell zu bestimmen. Die Datenverwaltung benötigt eine Funktion, an die die Eingabe, sowie das erkannte bzw. manuell bestimmte Ergebnis übergeben werden kann. Das Inserat wurde erstellt und der Ablauf ist abgeschlossen. Die Auswertungs– Komponente wird in diesem Szenario nicht benötigt, da der Anwender nach dem Erstellen eines Inserats keine Antwort des Systems erhält. Es werden verschiedene Algorithmen verwendet, um alle Attribute aus einer Eingabe zu erkennen. Damit die Algorithmen verglichen werden können, werden einheitliche Metriken verwendet. Die meisten neuronalen Netze werden anhand der Werte: Accuracy, Precision, Recall und F1–Score bemessen . Diese Werte werden aus den Feldern einer Confusion Matrix berechnet. Damit die berechneten Ergebnisse aus der Evaluation auch mit anderen Arbeiten vergleichbar sind, werden dieselben Metriken aufgestellt. Für eine ausführliche Evaluation wird eine zusätzliche Bewertung nach durchgeführt. Die Attribute können dabei in folgende Klassen eingeordnet werden: • Erkannt, richtige Klasse • Erkannt, falsche Klasse • Zu viel/wenig erkannt, richtige Klasse • Zu viel/wenig erkannt, falsche Klasse • Falsch erkanntes Wort • Attribut nicht erkannt Durch eine genauere Aufteilung können so die Algorithmen gezielter untersucht werden, um eine mögliche spätere Nachverarbeitung zu vereinfachen. In dem Bereich der natürlichen Sprachverarbeitung gibt es viele Modelle die Named Entity Tagging (NET) unterstützen. Aus diesem Grund soll der Prototyp die Metriken automatisch generieren. Dadurch wären die Modelle einheitlich gestaltet und sind somit besser nachvollziehbar. So kann die Performance der Pipeline jederzeit nachvollzogen werden. Zeitmessungen wurden auf einem Laptop (i7-4710MQ mit 2,50 GHz und 16 GB RAM) mit den vorhandenen Testdaten ausgeführt. Dabei wurden externe Einflussfaktoren so weit wie möglich ausgeschlossen (keine weiteren laufenden Programme, keine Internetverbindung) und die Testdurchläufe wurden zehnmal wiederholt, um ein möglichst genaues Ergebnis zu erzielen. Der folgende Abschnitt beschreibt das Sammeln der Daten, die in dieser Arbeit verwendet wurden. Im Anschluss wird die Implementierung der einzelnen Komponenten, die zusammen eine Pipeline bilden, vorgestellt. Abschließend wird auf die Oberfläche des Prototyps eingegangen. Die Umsetzung erfolgt in zwei Iterationsschritten, die auf Abbildung 10 dargestellt werden. In der ersten Iteration wird eine gewisse Menge von Daten akquiriert, auf die die verschiedenen Modelle trainiert und evaluiert werden, um diese vergleichen zu können. Anhand dieser Ergebnisse werden die Modelle gewichtet und zusammen kombiniert, um die Pipeline zu erstellen. Im Anschluss wird die erste Version des Prototyps realisiert, der das Erstellen von Inserats unterstützt. Abbildung 10: Ablaufdiagramm der Umsetzung dieser Arbeit In der zweiten Iteration werden mehr Daten mithilfe des Prototyps gesammelt. Die bereits trainierten Modelle werden anhand dieser Daten evaluiert, um die Genauigkeit der Modelle besser zu bestimmen. Die verwendeten Modelle werden mit dem gesamten Datensatz erneut trainiert und die vorherigen Ergebnisse mit den neuen verglichen. Die neuen Modelle werden mit den vorhandenen in der Pipeline ausgetauscht, um das Ergebnis des Prototyps zu verbessern. Ein wichtiger Bestandteil, um mit maschinellen Lernen Probleme lösen zu können sind relevante Daten. Mit einer großen Menge von Daten können die verwendeten Modelle bessere Ergebnisse erzielen, da Abweichungen weniger Gewichtung haben. Zudem sind die Daten vielfältiger und können somit mehrere verschiedene Situationen abdecken. Zu Beginn der Arbeit waren keine Daten vorhanden weshalb die Datenakquise ein wesentlicher Bestandteil darstellt. Die ersten Daten wurden in einer Umfrage erhoben. Die Arbeit beschränkt sich auf das Erkennen des Attributes \"Produkt\" weshalb die Teilnehmer Sätze bilden sollten, in denen nach einem Produkt gesucht wird. Das enthaltene Produkt sollte zudem in einem zusätzlichen Feld eingetragen werden, um so die spätere Vorverarbeitung der Daten zu vereinfachen. In der ersten Iteration wurden so 70 Datensätze erhoben, die in dem nächsten Schritt vorverarbeitet wurden. Die Algorithmen sollen nur Produkte erkennen, welche in dem gegebenen Satz enthalten sind. So wurden Sätze wie: \"Am Wochenende lade ich wieder zu einer Grillparty ein, ich suche noch jemand der Fleisch mitbringen kann\" mit dem angegebenen Produkt \"Grillfleisch\" geändert, sodass das gesuchte Wort exakt in dem Satz enthalten ist, in diesem Fall: \"Fleisch\". Basierend auf diesen Daten wurden die Algorithmen in der ersten Iteration trainiert. In dem zweiten Iterationsschritt konnte auf einen lauffähigen Prototyp aufgebaut werden, um so das Sammeln der Daten zu unterstützen. Die Probanden wurden gebeten, sich auf die Produktgruppe der Smartphones zu fokussieren damit die vorgegebenen möglichen Attribute in der Eingabe enthalten sein können. In einer Eingabemaske wird das Angebot bzw. die Nachfrage eingegeben und auf der nächsten Seite wird das Ergebnis des Algorithmus dargestellt. Dabei wurde sich auf 6 mögliche Attribute beschränkt: Produkt, Hersteller, Preis, Farbe, Speicher und Kamera. Die Probanden sollten entscheiden, ob ihre Eingabe richtig erkannt wurde, oder ob Attribute falsch gesetzt wurden bzw. komplett fehlen. Im letzten Fall sollten die Anwender selbst die Attribute markieren, um die Daten für eine spätere Verarbeitung vorzubereiten. Der Vorteil in dieser Methode liegt darin, dass deutlich mehr Datensätze direkt verwendet werden können und keine Vorverarbeitung der Daten wie in der ersten Iteration nötig ist. Die Oberfläche erlaubt nur das Markieren von zusammenstehenden Wörtern, die tatsächlich in dem Satz enthalten sind und so das Attribut bilden. Beides sind Annahmen die im Rahmen dieser Arbeit getroffen wurden und im Abschnitt 3.1 genauer beschrieben werden. Dieses Vorgehen ermöglicht das Sammeln eines dynamisch wachsenden Datensatzes, welcher zum Evaluieren und Optimieren der Algorithmen verwendet wird. Zum Erkennen der Attribute werden verschiedene, bereits existierende Verfahren kombiniert, um gemeinsam eine Pipeline zu bilden. Die Algorithmen können vorherige Teilergebnisse überschreiben, um das endgültige Resultat zu verbessern. Die Verfahren werden passend zu der Reihenfolge in der Pipeline in den folgenden Abschnitten vorgestellt. Abbildung 11 zeigt einen Überblick, welche Algorithmen verwendet werden. Abbildung 11: Reihenfolge der verwendeten Algorithmen Zu Beginn der Arbeit wurde das vorhandene und bereits trainierte deutsche Modell von SpaCy evaluiert. Das Modell unterstützt das Setzen von Part of Speech (POS), Abhängigkeiten und NER welches für diesen Teil verwendet wurde. Das Modell verwendet eine eigene Word Embedding Strategie mit Unterwortmerkmalen und \"Bloom\"–Einbettungen sowie ein tiefes CNN um die Ergebnisse zu berechnen (SpaCy– Dokumentation 2019). Trainiert wurde das Modell auf einem Korpus von mehreren tausend, deutschen Wikipedia Artikeln mit 4 ausschlaggebenden Attributen: Lokation, Organisation, Personen und sonstigen. Getestet wurde auf Erkennen der Organisation, welches — in dem gegebenen Anwendungsfall — gleichbedeutend mit dem Hersteller des Produktes ist. Das Ergebnis eines Testszenarios zeigte, dass die Satzstruktur zwischen dem Anwendungsfall und dem Wikipediakorpus zu unterschiedlich ist, sodass das Attribut nicht erkannt wurde. Basierend auf dem Testszenario–Datensatz, der 14 verschiedene Eingaben enthält, wurde ein neues Modell trainiert. Abbildung 12 zeigt die Ergebnisse beider Modelle mit derselben Eingabe. Abbildung 12: Ergebnisse der verschiedenen SpaCy–Modelle Das neue Modell wurde auf das Erkennen aller sechs Attribute trainiert. Dieses sollte zeigen, ob das Framework mit einer sehr geringen Menge an Daten lernen kann. Wie der Abbildung 12 zu entnehmen ist hat sich das Ergebnis gegenüber dem vortrainierten Modell deutlich verbessert. Das bedeutet, dass SpaCy, selbst mit einer geringen Menge an Daten, in der Lage ist entsprechende Ergebnisse zu erzeugen. Mit den Trainingsdaten aus dem ersten Iterationsschritt zeigte sich, dass sich das Modell stark verbesserte. Basierend auf der Annahme, dass mehr Trainingsdaten zu einem deutlich besseren und weiterhin performanten Modell führen, bildet der Named Enitity Tagger aus dem SpaCy Framework den ersten Schritt in der Pipeline. SpaCy bietet zudem eine Tokenizing Funktion welche die Eingabe in einzelne Token (z.B. Wörter, Satzzeichen usw.) unterteilt die in den nachfolgenden Schritten verwendet werden. Ein wesentlicher Nachteil der SpaCy NER Funktion ist, dass es die Eingaben von Bezeichnungen an der falschen Stelle erkennt. So werden Wörter als Produkt klassifiziert die keine sind. Aus diesem Grund wurde in der Pipeline eine Gewichtung eingebaut, die den folgenden Algorithmen das Recht gibt, vorherige Teilergebnisse zu überschreiben nicht jedoch zu löschen. Dieses ist von Vorteil da sich einzelne Schritte nur auf das Finden einiger Attribute fokussieren können, welche zum Ende der Pipeline ausgeführt werden, die zum Ausbessern vorheriger Fehler geeignet sind. Um eine der Schwächen des SpaCy–Modells auszugleichen, wurde ein regelbasierter Ansatz verwendet. Das sprachliche Modell ist für das Erkennen und richtige Unterscheiden von Zahlenwerten weniger geeignet. So wurden die Attribute Preis, Speicher und Kamera von der SpaCy Komponente häufig falsch klassifiziert. Der einzige wesentliche Unterschied ist die Einheit nach dem Zahlenwert wie z.B. 300 e, 256 GB und 13 MP. Zum Erkennen solcher Attribute werden reguläre Ausdrücke verwendet. Die Eingabe wird nach einem Zahlenwert durchsucht und anhand der folgenden Einheit klassifiziert. Um das Erstellen der regulären Ausdrücke zu vereinfachen, wird ein Ausdruck dreigeteilt. Das Präfix steht vor dem gesuchten Wert und beschreibt diesen z.B. \"max\", \"höchstens\" oder \"mid\". Nach dem Präfix kommt der Stamm, dieser beschreibt den Aufbau des gesuchten Zahlenwertes, in Python könnte es für europäische Preise wie folgt aussehen: 1 r e T a g g e r = ReTagging ( ) 2 3 4 5 p r e f i x = [ ’ max ’ , ’ maximal ’ , ’ b i s zu ’ , ’ ab ’ , ’ mid ’ , stam = [ ’ ( \\\\ d+\\\\ ,\\\\d { 1 , 2 } ) ’ , ’ ( \\\\ d+) ’ ] s u f f i x = [ ’ e ’ , ’ euro ’ ] ’ ’] 6 7 r e T a g g e r . add ( ’MONEY’ , p r e f i x , s u f f i x , stam ) Listing 1: Regel zum Erstellen des regulären Ausdruck zum Erkennen des Preises Zum Schluss folgt das Suffix, es beschreibt die Einheit, die dieses Attribut haben könnte. Diese drei Mengen werden mit dem Attributbezeichner (z.B \"MONEY\") der Methode übergeben, die alle Kombinationen aus den Mengen bildet, um daraus den regulären Ausdruck zu erzeugen. Die Tatsache das es ein regelbasierter Ansatz ist, erübrigt das genaue Evaluieren. Dieses Verfahren bietet sich für Attribute an, die hauptsächlich aus Zahlen bestehen und diese Anhand von Regeln erkannt werden können. Es ist von Vorteil, da keine Modelle trainiert werden müssen und direkt eingesetzt werden können. Dadurch wird ein schnelles Reagieren auf Ausdrücke, die neu hinzugefügt werden müssen, ermöglicht. Selbiges zeigt die Unflexibilität von regulären Ausdrücken, da diese nur Werte erkennen, die zuvor mit Regeln beschrieben wurden und zudem manuell gepflegt werden müssen. Zum Klassifizieren von Produkten sind reguläre Ausdrücke nicht geeignet, da weder ein Stamm noch ein Suffix genau definiert werden kann. Das Präfix allein ist nicht ausreichend (z.B. \"suche []\", \"verkaufe []\"). Reguläre Ausdrücke sind schlicht zu unflexibel. Der regelbasierte Ansatz bildet den zweiten Schritt der Pipeline, damit dieser mögliche Fehler bei Attributen mit Zahlenwerten der SpaCy Komponente korrigieren kann. In der ersten Iterationsstufe wurden einige Regeln zum passenden Erkennen der Testdaten erstellt. Diese wurden geringfügig in der späteren Iterationen der Pipeline modifiziert. Wörter haben neben ihrer Bedeutung noch Wortklassen, die basierend der grammatikalischen Eigenschaften des Wortes gesetzt werden. Abbildung 13 zeigt die erkannten Wortklassen (z.B.Verben, Nomen, numerisch oder Satzzeichen) sowie die Beziehungen zwischen den einzelnen Wörter. Das trainierte Modell von SpaCy ist in der Lage die Wortklassen den Wörtern zuzuordnen. Auch werden weitere Metadaten von SpaCy erkannt und den einzelnen Wörtern der Eingabe zugeordnet. Diese sind unter anderem, ob das Token nur aus Buchstaben besteht und ob es sich hierbei um ein Stoppwort handelt. Stoppwörter sind Wörter, die keinen Mehrwert für die Aussage beinhalten, weshalb diese in der Regel in der Datenauswertung ignoriert werden. Auch sind es Wörter, die überdurchschnittlich häufig in einer Sprache vorkommen und häufig als Füllwörter eingesetzt werden. Diese Wörter sind unter anderem: \"also\", \"bei\" oder \"hat\" und sind bei SpaCy in dem Modell hinterlegt. Weitere Metadaten, die erfasst werden, sind die Position des Wortes in dem Text, die Länge des Wortes und ob der erste Buchstabe des Wortes großgeschrieben ist. Abbildung 13: Beispiel für erkannte Wortklassen des SpaCy Frameworks Basierend auf diese Informationen wurde ein einfaches, neuronales Netz trainiert, dass die Metadaten des einzelnen Wortes als Eingabe verwendet. Das Modell besteht aus drei voll vernetzten Schichten, von der eine Schicht ein versteckter Layer ist. Der letzte Layer des Netzes verwendet die \"Binärer Schritt\" Aktivierungsfunktion welche das Ergebnis auf die Werte 0 (kein Produkt) und 1 (Produkt) beschränkt. Trainiert wurde das Modell auf demselben Datensatz wie auch schon zuvor das SpaCy–Modell. Der Datensatz besteht aus ausschließlich grammatikalisch, korrekten Anfragen mit einer Maximallänge von 12 Wörtern. Die folgenden Abbildungen 14 und 15 zeigen die Performance des Modells nach der ersten Iteration der Datenakquise. Abbildung 14: Auswertung der Metadaten Analyse auf 70 Datensätzen Abbildung 15: Confusion Matrix der Metadaten Analyse auf 70 Datensätzen Kürzere Anfragen sind die Stärke des Netzes, da es bei diesen über eine höhere Genauigkeit verfügt. Wie der Metrik zu entnehmen ist, erkennt das Modell etwas mehr als ein Drittel der Produkte korrekt. Unter optimalen Bedingungen ist die Erkennung des Produktes sehr hoch und die Laufzeit des Algorithmus mit im Durchschnitt 25 ms sehr kurz. Einer genaueren Evaluation der Daten zeigte, dass das Modell keine Produkte erkennt, die aus mehreren Wörtern bestehen. So wird zum Beispiel \"iPhone 8\" nicht erkannt, sondern nur der erste Teil des Wortes: \"iPhone\". Dieses ist auf die Auswertung, in der die Tokens einzeln bewertet werden, zurückzuführen. Auch setzt das Modell eine korrekte Rechtschreibung der Eingabe voraus, da sonst den Wörtern falsche Metadaten zugeordnet werden, die das Ergebnis verfälschen. In der zweiten Iteration der Datenakquise wurde das Modell erneut auf die dann vorhandenen Daten trainiert. Der Datensatz erhöhte die maximale Länge der Eingabe und beinhaltete grammatikalisch, inkorrekte Eingaben. Das Ergebnis verschlechterte sich im Vergleich zum vorherigen erheblich, was auf die grammatikalisch, inkorrekte Eingabe zurückzuführen ist. Nach einer Vorverarbeitung, in der fehlerhafte Daten entfernt wurden, wurde das Modell erneut trainiert. Dieses erbrachte keine Verbesserung des ursprünglichen Modells, weshalb das erste Modell in dem Prototyp verwendet wurde. Werden nur die Metadaten betrachtet, gehen viele Informationen der Eingabe verloren, die bei einer Auswertung nicht beachtet werden. Auch werden Produkte, die aus mehreren Wörtern bestehen nie erkannt, da sich die Metadaten zu stark zu den einfachen Produkten unterscheiden. Aus diesen Gründen ist die Genauigkeit der Vorgehensweise limitiert. Durch die gute Performance bei optimaler Eingabe wird dieses Verfahren an dritter Stelle der Pipeline verwendet. Fehler der vorherigen Schritte werden korrigiert, wobei die Klassifizierung der Metadaten kaum bis gar keine Wörter falsch zuordnet. Das Attribut Produkt wurde in den vorherigen Schritten nur unter bestimmten Umständen richtig klassifiziert, da unter anderem der Aufbau der Eingabe nicht beachtet wurde. In dem Abschnitt 2.2 wurden Word Embeddings vorgestellt, welche Wörter in Form eines Vektors beschreiben. Diese Repräsentation beinhaltet Informationen, anhand das folgende Modell in der Lage ist, Wörter mit Attributen zu versehen. Abbildung 16: Aufbau und Funktionsweise des Targer–Modells nach Das Targer–Modell besteht aus der Kombination der Architekturen bidirectional Recurrent Neural Network (biRNN), CNN und CRF. Als RNN wird ein biLSTM–Modell verwendet welche im Kapitel 2 genauer erläutert wurde. Eine beispielhafte visuelle Darstellung des Modells kann Abbildung 16 entnommen werden. Bei dem Trainieren des Modells werden die GloVe Word Embeddings verwendet, um mit diesen Informationen die Buchstaben–Features der Eingabe zu erstellen. Die Buchstaben–Features oder auch Char–level features werden nach dem Trainieren mit dem Modell gespeichert. Bei der Verwendung des Modells werden nur die Char–level features benötigt und nicht mehr die gesamten Wortvektoren. Für den Ablauf des Netzes wird zunächst die Eingabe in die Buchstaben–Features umgewandelt, welche dann an die erste Ebene — also biLSTM — übergeben werden. Dort werden die Vektoren wie bereits in dem Abschnittdem CNN Netz verarbeitet. In dem letzten Schritt werden die Werte von einem CRF verarbeitet. Für diese Arbeit wurden GloVe Wortvektoren mit 300–Dimension verwendet, um die höchstmögliche Genauigkeit zu erzielen. Aufgrund der langen Trainingsdauer, sowie der großen Menge an benötigten Trainingsdaten wurde ein bereits trainiertes Modell verwendet (Deepset o.D.). Das Vokabular des Modells umfasst 400000 Wörter und wurde auf deutsche Wikipedia Artikel trainiert. Dem trainierten Modell wird eine Liste mit den einzelnen Wörtern der Eingabe übergeben. Die Rückgabe ist eine Liste mit gesetzten Attributen in dem CoNLL Format (Tjong Kim Sang und De Meulder 2003). In diesem Format entspricht ein \"O\" das für dieses Token kein Attribut gefunden wurde. Für das Produkt \"iPhone X\" werden die Bezeichnungen \"B– PRODUCT\" und \"I–PRODUCT\" verwendet. Der erste Teil der Bezeichnung beschreibt, ob es sich um einen Anfang des gesuchten Wortes handelt (\"B–\"). Besteht das gefundene Attribut aus mehreren Wörtern, werden alle folgenden Wörter mit \"I–\" als Präfix markiert.Trainiert wurde das Modell nur auf das Erkennen des Attributes Produkt welches in den folgenden Metriken evaluiert wurde. Abbildung 17: Auswertung des Targer–Modells auf 70 Datensätzen Abbildung 18: Confusion Matrix des Targer–Modells auf 70 Datensätzen Wie auf der Abbildung 17 zu erkennen erreicht das Modell eine hohe Genauigkeit, bei dem Erkennen des Produktes. Dieses ist darauf zurückzuführen, dass der Aufbau bzw. die Bedeutung des Satzes betrachtet wird und nicht die einzelnen Wörter. Die benötigte Laufzeit, um die Eingabe mit den entsprechenden Attributen zu versehen, wird durch diesen Schritt geringfügig (im Durchschnitt 125 ms pro Anfrage) beeinflusst, welches bei der Verarbeitung von einzelnen Anfragen den Ablauf nicht merkbar verlängert. Die Berechnung der Metriken wird stärker beeinflusst, da der gesamte bisherige Datensatz von der Pipeline nacheinander verarbeitet wird. Die gute Performance ist auf die Verwendung der Char–level features zurückzuführen, da nicht mehr die gesamten Wortvektoren benötigt werden. Das Targer–Modell ist nicht kontextsensitiv weshalb es Schwierigkeiten hat, Sätze mit mehreren Produkten bzw. Beziehungen zwischen den Produkten korrekt zu klassifizieren. Als Beispiel dient die folgende Eingabe \"Mein altes Smartphone ist leider kaputt gegangen weshalb ich dringend ein neues iPhone X benötige!\" richtig zu erkennen. Es tendiert dazu, das Attribut \"Produkt\"doppelt zu setzen, zum einen für das Wort \"Smartphone\" und \"iPhone X\". Ein alternatives Verhalten ist, dass nur das erste Wort der beiden Elemente mit dem Attribut versehen wird, was in diesem Fall \"Smartphone\" wäre und somit falsch ist. Durch eine Regel, die besagt, dass immer das zuletzt gefundene Element für ein Attribut wiedergegeben werden soll, konnte das erste Szenario teilweise gelöst werden, aber nicht alle Eingaben folgen dieser Regel. In dem Abschnitt 2.2.3 wurde das ELMo Word Embedding vorgestellt. Der Vorteil dieser Repräsentation liegt darin, dass die Vektoren der Wörter abhängig von dem Kontext sind und somit das Szenario expliziter beschreiben. Wie bereits erwähnt, ist das Targer– Modell nicht kontextsensitiv weshalb es Schwierigkeiten hat, bestimmte Eingaben korrekt mit Attributen zu versehen. In diesem Schritt wird das grundlegende Modell von Targer (also biLSTM, CNN, CRF) mit der ELMo Repräsentation als Eingabe verwendet, um so die Vorteile beider Methoden zu kombinieren. In dem vorherigen Kapitel wurde bereits beschrieben, wie das Modell aufgebaut ist und auch wie die einzelnen Schichten zusammenarbeiten. Der wesentliche Unterschied zwischen diesen beiden Ansätzen ist, dass keine Char–level features mehr verwendet werden. Dies bedeutet, dass für die Verwendung des Modells immer die GloVe Word Embeddings geladen sein müssen. Zusätzlich dazu werden die bereits trainierten Gewichtungen, sowie die dazugehörigen Einstellungen, zum Verwenden des Modells benötigt. Die Gewichtungen wurden trainiert, um den jeweiligen Kontext einer Eingabe zu bestimmen und so die dazugehörigen Wortvektoren zu ermitteln, die in dem Modell verwendet werden. Diese Vorgehensweise wurde in Abschnitt 2.2.3 genauer erläutert. Für selbst trainierte ELMo Word Embeddings wird eine große Menge von Daten und Zeit benötigt. Das in dieser Arbeit verwendete Modell , wurde auf einen deutschen Wikipedia Korpus trainiert. Zusätzlich wurden die Kommentare der verwendeten Artikel genutzt, um Umgangssprache in den Datensatz mit einzubeziehen. Außerdem werden die zum Modell gehörenden Gewichtungen und Optionen verwendet. Da das Modell immer die gesamten Word Embeddings benötigt um verwendet werden zu können, muss die 4 GB große ELMo Datei dauerhaft im RAM verfügbar sein. Aus diesem Grund wurde das Modell in eine separate Anwendung ausgelagert. Dieses bietet eine Representational State Transfer (REST) Schnittstelle, an die eine Eingabe übergeben wird. Diese wird von dem Modell verarbeitet und ein Dictionary mit dem Attributbezeichner als Schlüssel und dem gefundenen Ergebnis als Wert zurückgesendet. Der Pipeline selbst wurde ein Schritt hinzugefügt, welche diese Schnittstelle verwendet, um die entsprechenden Ergebnisse zu erhalten. Durch diese Designentscheidung war es möglich, die benötigten Ressourcen der hauptsächlichen Anwendung gering zu halten und weitere Algorithmen können mittels der REST Schnittstelle hinzugefügt werden. Abbildung 19: Auswertung des ELMo–Modells auf 70 Datensätzen Abbildung 20: Confusion Matrix des ELMo–Modells auf 70 Datensätzen Das Modell weist die höchste Genauigkeit aller verwendeten Algorithmen auf, wie den Abbildungen 19 und 20 zu entnehmen ist. Dadurch wird deutlich, dass die Betrachtung des Kontextes für den Anwendungsfall zielführend ist. Als Beispiel dient folgende Eingabe \"hallo, wir möchten am kommenden wochenende mit den nachbarn grillen und ich wollte dafür einen salat machen weshalb ich auf der suche nach einer salatschüssel bin da mir aufgefallen ist, dass ich keine habe\" in der das gesuchte Produkt \"salatschüssel\" korrekt erkannt wird. Ein anderes Produkt an derselben Stelle wird ebenfalls mit hoher Wahrscheinlichkeit vom Algorithmus korrekt erkannt. Dieses zeigt, dass der Algorithmus nicht die charakteristischen Eigenschaften eines jeden Produktes lernt, sondern die Position, an der ein Produkt stehen würde. Durch dieses Verhalten ist das Modell in der Lage, mit wenigen Trainingsdaten ein überaus gutes Ergebnis zu erzielen. Der letzte Schritt der Pipeline stellt sicher, dass bestimmte Attribute erkannt werden. So können bestimmte Werte, auf die die Pipeline bisher nicht trainiert wurde, den passenden Attributen zugewiesen werden. Dadurch ist ein schnelles Hinzufügen einzelner Werte möglich, bevor die verwendeten Modelle trainiert werden. Um verschiedene Schreibweisen des Wortes abzudecken, wird ein Fuzzy Matching verwendet. Für jedes zuvor definierte Wort wird geprüft mit welcher Wahrscheinlichkeit dieses sich in dem Satz befindet. Dafür wird die Levenshtein Entfernung zwischen einem definierten Wort und einem Wort aus dem Satz gebildet. Da Fuzzy Matching (oder auch Approximation Matching) die Eigenschaft besitzt immer Ergebnisse zu liefern, wird ein zuvor definierter Grenzwert angelegt, wie hoch die Übereinstimmung mindestens sein muss, bevor die Wörter als identisch gelten. Bei Produkten wird ein Grenzwert von 95 % Übereinstimmung angelegt, damit die Unterschiede zwischen den einzelnen Versionsnummern noch erkannt werden, wie z.B. bei Smartphones (iPhone 8 und iPhone X). Der Nachteil bei diesem hohen Grenzwert ist, dass die verschiedenen Schreibweisen für ein Produkt einzeln angegeben werden müssen (z.B. \"iphone 8\" und \"iPhone 8\"), da diese sonst unter Umständen nicht mehr erkannt werden. Bei anderen Attributen zeigte sich, dass eine Übereinstimmung von 80 % ausreicht, um ein genaues Ergebnis zu erzielen, da sich Attribute wie \"Hersteller\" üblicherweise nicht nur in einem einzelnen Buchstaben unterscheiden. Ein weiteres Problem bei Fuzzy Matching ist, dass nicht nur auf die höchste Übereinstimmung geachtet werden darf, sondern auch auf die Länge der Wörter. Befinden sich beispielsweise die Wörter \"iPhone\" und \"iPhone X\" in dem Fuzzy Matcher und als Eingabe erfolgt der Satz: \"Hey ich bin auf der suche nach einem iPhone X\" werden die hinterlegten Werte der Reihe nach mit der Eingabe auf teilweiser Übereinstimmung geprüft. Beide Werte erreichen eine Genauigkeit von 100 % und das zurückgegebene Ergebnis hängt von der Reihenfolge der Prüfung ab. Um immer den spezifischen Ausdruck zu identifizieren werden stets längere Werte den kürzeren gegenüber bevorzugt, solange diese sich noch über dem definierten Grenzwert befinden. Dadurch wird sichergestellt, dass in dem Beispielszenario der Wert \"iPhone X\" erkannt wird. Abbildung 21: Vergleich der Genauigkeit nur des ELMo–Modells (oben) und mit anschließendem Fuzzy Matching (unten) Die Abbildung 21 zeigt zwei Messungen, die korrekt erkannten Produkte ohne Fuzzy Matching (oben) und mit Fuzzy Matching als letzten Schritt (unten). Wie der Abbildung zu entnehmen ist, hat das Fuzzy Matching in diesem Fall das Ergebnis verschlechtert. Dem Fuzzy Matcher wurden Produkte hinzugefügt, welche bereits von der vorherigen Pipeline erkannt worden sind, aber der Fuzzy Matcher nicht alle Schreibweisen kennt. Das Produkt wird in einer leicht anderen Schreibweise gefunden und durch die Gewichtung der Pipeline wird das vorherige Ergebnis überschrieben. Dadurch ist das Produkt, welches am Ende von der Pipeline erkannt wurde, nicht korrekt in der Eingabe vorhanden und es wird nicht als korrekt klassifiziert gezählt. Die Laufzeit des Fuzzy Matching unter der Verwendung der Levenshtein Entfernung ist abhängig der Anzahl der Werte, auf die die Eingabe geprüft werden soll. So wird der Prototyp mit acht Werten betrieben, die mit 3 ms pro Eingabe die Laufzeit nicht wesentlich beeinflussen. Weitere Tests zeigten, dass die Laufzeit ab 250 Werten bereits durchschnittlich 254 ms beträgt. Dieses zeigt wie bereits in Abbildung 21 dargestellt, dass Fuzzy Matching nur für wenige Szenarien verwendet werden sollte, um so das Ergebnis bis zum Trainieren der neuen Modelle zu verbessern. In den vorherigen Abschnitten wurden die Funktionalitäten sowie die einzelnen Vorteile und Nachteile jeder Komponente vorgestellt. Es wurden verschiedene Algorithmen angewendet, um das Ergebnis möglichst genau abbilden zu können. Alle Algorithmen zeigten Schwierigkeiten mit dem Erkennen von Produktversionen wie sie häufig bei Smartphones zu finden sind (z.B. Galaxy S10). Um diese Szenarien abfangen zu können, wird nach der Ausführung der Pipeline eine Nachverarbeitung der Ergebnisse vorgenommen. Dazu wird jedes Token nach dem gefundenen Produkt analysiert. Es wird geprüft, ob dieses Token eine Kombination aus Buchstaben und Zahlen ist oder ob es ausschließlich aus Großbuchstaben besteht. In beiden Fällen wird das Token zu dem Produkt hinzugefügt und es wird erneut das folgende Token betrachtet. Trifft keiner der beiden Fälle zu, so wird der Prozess beendet und die Nachverarbeitung ist abgeschlossen. Dieses Vorgehen verbessert die Genauigkeit der Pipeline was im Abschnitt Evaluation zusammen mit den ELMo Algorithmus genauer betrachtet wird. Abbildung 22 zeigt den gesamten Aufbau der Pipeline mit allen Algorithmen die verwendet werden. Die Laufzeit der gesamten Pipeline beträgt 837 ms und liegt somit unter den geforderten drei Sekunden aus NF1. Abbildung 22: Pipeline mit allen Algorithmen und den ausgelegten Attributen Die Oberfläche ist als Webanwendung verfügbar und wurde mit der Programmiersprache Python und dem Webframework Flask realisiert. Flask ist ein leichtgewichtiges Framework welches nur die Template–Engine Jinja2 als Abhängigkeit besitzt. Die einzelnen Seiten wurden in HTML erstellt und mit Jinja2 dynamisch gestaltet, um eine hohe Flexibilität der Seiten zu erhalten. Jede Funktionalität ist durch einen Flask Endpoint zugänglich und wird durch die Webseite aufgerufen. Das Aussehen der Weboberfläche wurde mit dem CSS–Framework Bootstrap gestaltet. Bootstrap ist ein weit verbreitet Framework und wird von vielen verschiedenen Webseiten eingebunden. Durch die Verwendung von Bootstrap wird ein einheitliches Aussehen mit anderen Webseiten hergestellt. Dieses hat zufolge, dass durch den Wiedererkennungswert der Bedienelemente die Nutzung für Anwender erleichtert wird. Für die Persistierung der Daten wird eine Neo4J Datenbank verwendet. Neo4J ist eine Graphdatenbank und wurde von den relationalen Datenbanken gegenübergestellt und bewertet. Die verschiedenen Datenbankzugriffe wurden in Methoden gekapselt, um diese separiert von der Anwendung zu verwalten. Die Pipeline wird als eigene Komponente eingebunden und stellt zwei Methoden zur Verfügung, über die der Webserver die Funktionalität aufrufen kann. Die resolve Methode erhält als Übergabeparameter eine Anfrage, die ausgewertet wird. Zurückgegeben wird ein Dictionary bei dem der Schlüssel dem Attributbezeichner und der Wert der gefundenen Sequenz entspricht. Die zweite Methode wird zum Berechnen der Metriken verwendet und erhält eine Liste mit allen Datensätzen sowie den zugeordneten Attributen. Zurückgegeben wird ein Container, der die Ergebnisse für jeden Algorithmus sowie der gesamten Pipeline enthält. In den vorherigen Kapiteln wurde beschrieben wie die Pipeline funktioniert, die zum Erkennen der einzelnen Attribute eingesetzt wird. Simultan zur Entwicklung der Pipeline entstand ein Prototyp, der zur Verbesserung der Pipeline entstand. Später wurden weitere Funktionen ergänzt, die sowohl eine Verwendung der Pipeline zeigen als auch das Hinzufügen und Bearbeiten des Datenbestandes vereinfachen. Der Prototyp beschränkt sich auf das Erkennen und Taggen von 6 Attributen (Produkt, Hersteller, Preis, Farbe, Speicher, Kamera) welche alle in der Smartphone–Domäne vertreten sind. Auch stellt der Prototyp keine nutzerorientierte Anwendung dar, sondern lediglich eine funktionsorientierte Oberfläche zum Bedienen der Pipeline. Auf der Startseite wird dem Nutzer die Möglichkeit geboten eine Eingabe zu tätigen. Beim Bestätigen der Eingabe wird die eingegebene Anfrage von der Pipeline verarbeitet und die gefundenen Ergebnisse werden — damit diese für den Benutzer besser nachvollziehbar sind — farblich hervorgehoben. Der Anwender entscheidet, ob alle gesuchten Attribute durch die Pipeline korrekt klassifiziert werden. Ist die Klassifizierung falsch wird der Anwender aufgefordert, die eigene Anfrage selbst mit den passenden Tags zu versehen. Das Zuweisen der Attribute geschieht in der Oberfläche des Taggers. Erst wird eine Wortsequenz aus der Eingabesequenz hervorgehoben und im Anschluss die passende Schaltfläche betätigt, um der Wortkette das Attribut zuzuweisen. Der Tagger kann auf beliebige Attribute erweitert werden. Dazu reicht es aus, eine neue Schaltfläche hinzuzufügen und diese mit einem nicht verwendeten Bezeichner zu versehen. Dem Attribut wird eine neue Farbe zugeteilt. Das neue Attribut kann verwendet werden, um zukünftige Daten zu annotieren. Damit das Attribut automatisch erkannt wird, muss die Pipeline erneut trainiert werden. Wie bereits in der Datenakquise vorgestellt, wurde dieser Teil des Prototyps verwendet, um die Datenbasis zu erweitern. Die in dem Kapitel 4 gezeigten Diagramme werden durch den Prototypen, basierend auf den zugrundeliegenden Daten, automatisiert erstellt. Zum Erstellen der Diagramme werden die Daten aus der Datenbank verwendet, die zuvor von Anwendern in die Oberfläche des Prototyps eingegeben wurden. Die Sätze werden erneut an die Pipeline übergeben, sodass die Algorithmen das Ergebnis berechnen. Dieses wird mit den korrekten Attributen aus der Datenbank verglichen und die unterschiedlichen Diagramme werden berechnet. Al- le Diagramme betrachten dabei die gesamte Eingabe, bewerten also nicht einzelne Wörter. Die erstellten Diagramme sind: Confusion Matrix, Piechart und Barchart. Abbildung 23 zeigt die Confusion Matrix für das Attribut \"Produkt\" mit dem verwendeten Algorithmus ELMo. Die vertikale Achse gibt an, ob sich das Attribut in der Eingabe befindet oder nicht. Auf der horizontalen Achse wird das Ergebnis des Algorithmus angegeben. Bei einem guten Ergebnis des Algorithmus sind die Felder mit übereinstimmenden Achsen–Bezeichner höher als die übrigen Felder. Erst bei einer exakten Übereinstimmung, Algorithmus–Ergebnis und Eingabe–Ergebnis, wird der Wert des übereinstimmenden Feldes des Attributes erhöht. Durch eine Fehlermeldung verhindert die Oberfläche Eingaben die nicht mindestens das Attribut \"Produkt\" enthalten, weshalb wie auf der Abbildung 23 zu erkennen, alle Eingaben dieses Attribut besitzen. Aus dem Ergebnis der Confusion Matrix werden die Werte Accuracy, Precision, Recall und somit auch der F1–Score berechnet. Der Prototyp bietet die Möglichkeit, die Confusion Matrix für jedes Attribut mit jedem Algorithmus darzustellen. Abbildung 23: Confusion Matrix für den ELMo Algorithmus und dem Attribut Produkt Die Confusion Matrix beinhaltet nicht alle Information, die benötigt werden, um die Performance der Pipeline messen zu können. Die Balkendiagramme zeigen wie oft die einzelnen Attribute vorkommen und erkannt werden. Dabei zeigt der blaue Balken an, wie oft das einzelne Attribut in dem Datenbestand vorkommt. Der rote Balken zeigt, wie oft das einzelne Attribut komplett korrekt erkannt wird (Angaben in Prozent). Aus dieser Metrik lässt sich sehr einfach die effektive Performance der Algorithmen beurteilen, da die korrekte Klassifizierung der Attribute der Häufigkeit gegenübergestellt wird. Dieses Diagramm zeigt im Wesentlichen die Genauigkeit des Algorithmus für jedes Attribut, weshalb das Balkendiagramm für jeden Algorithmus sowie der gesamten Pipeline dargestellt werden kann. In den Kuchendiagrammen wird die Unterteilung der Klassifizierung weiter aufgesplittet. Die Attribute werden anhand von 6 Teilgruppen bewertet, um ein besseres Verständnis der Klassifizierung des Algorithmus zu erhalten. Die Attribute aus den getesteten Anfragen können dabei einer dieser Gruppen zugeordnet werden: Erkannt, richtige Klasse, Erkannt, falsche Klasse; Zu viel/wenig erkannt, richtige Klasse; Zu viel/wenig erkannt, falsche Klasse; Falsch erkanntes Wort und Attribut nicht erkannt. Durch die Aufteilung wird deutlich, dass, auch wenn der Algorithmus das Attribut nicht komplett korrekt klassifizieren konnte, dennoch akzeptable Teile der Lösung erkannt werden. So wurde häufig die Produktbezeichnung wie ı̈Phoneërkannt, nicht aber der Zusatz \"Xöder SSE\", was durch dieses Diagramm deutlich wurde. Wie die Confusion Matrix kann das Kuchendiagramm für alle Attribute und Komponenten der Pipeline erstellt werden. Als beispielhafte Anwendung bietet der Prototyp eine Chatbot Funktion. Hier können Anwender Anfragen oder Angebote stellen, auf die das System nach einem passenden Gegenstück sucht. Als Beispiel könnte die Eingabe \"Hey, letzte Woche ist mein Handy kaputt gegangen, weshalb ich jetzt auf der Suche nach einem neuen Apple iPhone X in Weiß für unter 800 e bin\" dienen. Nach dem Absenden wird die Eingabe der Pipeline übergeben und die wesentlichen Attribute werden extrahiert. Basierend auf dieser Eingabe wird eine Anfrage an die Datenbank gestellt, welche passende Gegenangebote und die dazugehörigen Ergebnisse liefert. In einem einfachen Matchmaking wird geprüft, welches dieser Angebote am ehesten zur gegebenen Anfrage passen würde. Dabei wird versucht, möglichst viele übereinstimmende Attribute zu finden. Beim Preis wird die Rolle beachtet: stellt der Anwender eine Anfrage so wird ein geringerer Preis bevorzugt. Handelt es sich hingegen um ein Angebot, wird die Kaufanfrage mit dem höchst genannten Preis bevorzugt. Auf die beispielhafte Eingabe könnte folgende Ausgabe erfolgen \"Hey ich biete hier mein neues Apple iPhone X in weiß für 800 e\". Die Antwort enthält 3 Attribute von Interesse auf die das Matchmaking prüfen kann, die alle auf die Suchanfrage zutreffen. Abbildung 24 zeigt die Oberfläche mit dem Beispiel als Eingabe. Abbildung 24: Beispielverlauf einer Anfrage über den Chatbot (gelesen von unten nach oben) Die Gruppensuche (Abschnitt 3.2) wird durch zuvor definierte Relationen in dem Knowledge Graph ermöglicht. Eine mögliche Eingabe könnte dann wie folgt aussehen: \"Hallo, für meinen Sohn bin ich auf der Suche nach einem Apple Smartphone\". Diese Eingabe enthält weniger konkrete Informationen als das vorherige Beispiel. Die Pipeline hat zwei Attribute gefunden: Hersteller Apple und Produkt Smartphone. Mit dem vorherigen Ansatz würde kein Produkt gefunden werden da die Firma Apple kein Produkt mit dem Namen Smartphone herstellt, sondern nur Geräte, die der Kategorie Smartphone angehören. Um diese Fälle zu identifizieren wird geprüft, ob das gesuchte Produkt in der Wissensdatenbank vorhanden ist. Trifft dieses zu, wird über Relationen geprüft auf welche anderen Produkte der Eintrag verweist. Basierend auf dem Beispiel könnte das Ergebnis diese Werte enthalten: \"iPhone 5\", \"iPhone 6\", \"iPhone 7\", \"iPhone 8\" oder \"iPhone X\". Im Anschluss wird der bereits oben beschriebene Vorgang wiederholt, nur dass anstelle eines Produktes in der Datenbank auf jedes dieser Attribute verglichen wird. Zum Schluss folgt ein Matchmaking, um den besten Treffer zu finden, welcher dann dem Anwender angezeigt wird wie es bereits vorgestellt wurde. Für diese Art der Produktfindung wird das genutzte Wissen vorausgesetzt. Es wurde im Vorfeld definiert, dass es die Kategorie Smartphone gibt und auch welche Produkte zu dieser Kategorie gehören. Das System lässt sich beliebig auf weitere Kategorien erweitern. So könnte die Kategorie Tablet hinzugefügt werden, in dem der Bezeichner der Kategorie (hier Tablet) der Wissensdatenbank angehangen wird. Im Anschluss müssen die möglichen Ausprägungen der Kategorie (z.B. ‘iPad’, ‘iPad Air’, ‘iPad Pro’) hinterlegt werden. Dann würde die Eingabe: \"Für meinen Neffen suche ich ein neues Tablet, gerne gebraucht aber unter 300 e\" sämtliche der Kategorie Tablet enthaltenen Produkte finden. Dieses Kapitel unterteilt sich in die Evaluation zweier Aspekte. In dem ersten Abschnitt werden die verwendete Evaluationsmethodiken vorgestellt. In dem nächsten Abschnitt wird die Performance der verwendeten Algorithmen sowie der gesamten Pipeline untersucht. Es werden die im Abschnitt 3.5 vorgestellten Bewertungskriterien verwendet, um einen genauen Einblick der Ergebnisse zu erhalten. Der dritte Abschnitt befasst sich mit der Evaluation der Oberfläche des Prototyps. Die im Abschnitt 3.2 aufgestellten Anwendungsfälle wurden mit einen Fragebogen an zehn Probanden gestellt. Die Ergebnisse werden im Abschnitt 5.3 analysiert. Das Ziel dieser Arbeit ist es, ein System zu erstellen, welches mit einer geringen Menge von Trainingsdaten dem Anwender das Gefühl vermittelt verstanden zu werden. Um dieses beurteilen zu können wurde eine Nutzerevaluation und mehrere Messungen durchgeführt. Für die Messungen werden die Daten aus beiden Iterationen der Datenakquise verwendet, um diese Ergebnisse mit den Daten aus der ersten Iteration zu vergleichen. Der F1– Score beschreibt dabei die Genauigkeit der jeweiligen Algorithmen. Zusätzlich werden die Werte: Accuracy, Precision und Recall betrachtet, um die Forschungsfrage NF1 zu beantworten und damit die Ergebnisse mit anderen Arbeiten verglichen werden können. In dem Abschnitt 3.5 wurden noch sechs weitere Klassen vorgestellt, die ebenfalls für eine ausführlichere Evaluation betrachtet werden. Die NA2 ist durch Zahlenwerte schwer zu beantworten, weshalb eine Nutzerevaluation durchgeführt wurde. Um überprüfen zu können ob Anwender das Gefühl haben von dem System verstanden zu werden reicht es nicht aus, eine bestimmte Genauigkeit zu erreichen. Anhand mehrerer Anwendungsfälle sowie einen Fragebogen wird eine Nutzerevaluation durchgeführt, um die Oberfläche sowie das Verständnis auszuwerten. Das vorherige Kapitel befasste sich mit verschiedenen Algorithmen, die zusammen kombiniert wurden, um ein optimales Ergebnis zu erzielen. Die höchste Gewichtung wurde dem ELMo Algorithmus zugeteilt, da dieser in der ersten Evaluation die höchste Genauigkeit aufweisen konnte. In diesem Abschnitt wird dieser Algorithmus, sowie die gesamte Pipeline einer ausführlichen Evaluation unterzogen, um die Performance genauer zu charakterisieren. Zu Beginn wurden alle Modelle, bis auf eines, in der Pipeline mit 70 Datensätzen verschiedener Länge und Form trainiert. Das übrige Modell wurde zunächst mit 16 Datensätzen und für eine Evaluation erneut mit 70 Datensätzen trainiert. Basierend auf diesen Ergebnissen wurden die Modelle bewertet, ob und wie diese sich in die Pipeline einbauen lassen. Der aktuelle Datensatz umfasst 193 Einträge, die im Laufe der Entwicklung gesammelt wurden. Abbildung 25: Performance der gesamten Pipeline Abbildung 25 zeigt die Performance der gesamten Pipeline mit den anfänglich trainierten Modellen. Bei dem Attribut \"Produkt\" wird eine Genauigkeit von 80,8 % erzielt, für die übrigen Attribute wurden keine Modelle trainiert. Die übrigen Werte werden nur durch Ansätze wie Reguläre Ausdrücke bzw. Fuzzy Matching ermöglicht. Dieses Ergebnis zeigt das bereits mit einer geringen Menge an Daten, das Modell in der Lage ist in 4 von 5 Fällen das Produkt korrekt zu klassifizieren. 79,8 % werden von dem ELMo Algorithmus erkannt, die Nachverarbeitung der gesamten Pipeline verbessert das Ergebnis auf 80,8 %. Szenarien, die in dieser Statistik nicht berücksichtigt werden, sind unter anderem Attribute, die nur in Teilen erkannt wurden. \"Galaxy S10\" ist ein beispielhaftes Produkt, dass hätte erkannt werden sollen, von der Pipeline wurde \"Galaxy\" erkannt, was teilweise korrekt ist. Abbildung 26 zeigt eine genauere Analyse des ELMo Algorithmus. Es werden zusätzlich zu den Klassen \"Erkannt, richtige Klasse\" und \"Attribut nicht erkannt\" noch vier weitere Klassen erfasst: \"Erkannt, falsche Klasse\", \"Zu viel/wenig erkannt, richtige Klasse\", \"Zu viel/wenig erkannt, falsche Klasse\" und \"Falsch erkanntes Wort\". Abbildung 26: Piechart Analyse der ELMo Komponente Wie der Abbildung 26 zu entnehmen ist wurden 9,3 % (\"Attribut nicht erkannt\" und \"Falsch erkanntes Wort\") der Anfragen unzureichend verarbeitet. In 10,9 % der Fälle wurde zu viel bzw. zu wenig des gesuchten Produktes erkannt, was — je nach Aufgabenstellung — bereits zielführend ist. Durch Optimierung der Nachverarbeitung wäre die Pipeline imstande gegenüber den bisher erzielten 1 %, die Eingabe um 10,9 % selbstständig zu verbessern. Alternativ könnte das Modell mit einem größeren Datensatz trainiert werden, um eine allgemeine Verbesserung der Ergebnisse zu erzielen. Durch die Datenakquise sowie den Testdaten sind insgesamt 260 Datensätze gesammelt worden, die zum Trainieren und Verifizieren verwendet wurden. Die Daten fokussierten sich hauptsächlich auf Produkte aus der Kategorie Smartphone, aber auch andere Produktkategorien waren vertreten. Die Länge der Sätze war variabel, die Rechtschreibung wird nicht beachtet. Die Daten wurden durch die Oberfläche des Prototyps gesammelt, weshalb keine Vorverarbeitung des Datensatzes notwendig war. Zum Trainieren des Modells wurden die Daten aufgeteilt. Auf 90 % der Datensätze — also 234 Einträge — wurde das Modell trainiert. Die restlichen 10 % wurden für die Verifizierung verwendet. Aufgrund der geringen Datenmenge war es möglich, dass bestimmte Formulierungen der Sätze nur zum Testen und nicht zum Trainieren verwendet wurden. Um diese Problematik zu umgehen wurde das Modell in zehn Durchgängen mit verschiedenen Daten zum Verifizieren trainiert. Für ein endgültiges Ergebnis wurde der Durchschnitt der Trainingsdurchläufe verwendet. Durchlauf 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. Avg. Precision 0,857 0,889 0,905 0,880 0,815 0,885 0,889 0,963 0,793 0,815 0,869 Recall 0,923 0,923 0,942 0,846 0,846 0,885 0,923 0,963 0,767 0,771 0,879 F1–Score 0,8889 0,9057 0,9231 0,8627 0,8302 0,8850 0,9057 0,9630 0,7797 0,7925 0,8736 Tabelle 1: Ergebnisse der 10 Trainingsdurchläufe Wie der Tabelle 1 zu entnehmen ist, wurde ein durchschnittlicher F1–Score von 87,36 % erreicht. Dieser setzt sich aus den Werten Precision sowie dem Recall zusammen und vereint beide Werte in einem. Ein höherer F1–Score bedeutet im Allgemeinen, dass die Performance des Modells gestiegen ist. In gewissen Situationen kann es Ziel sein, eine hohe Precision auf Kosten des Recalls zu erreichen und umgekehrt, was bei dieser Aufgabenstellung nicht zutrifft. Berechnet wurden die Werte basierend auf jedem Wort der Verifikationsdaten. Bei den Metriken des Prototyps wird die gesamte Sequenz der Eingabe für die Bewertung beachtet, weshalb ein direkter Vergleich nicht möglich ist. Damit die Werte verglichen werden können, wurde das vorhandene ELMo–Modell der Pipeline nacheinander mit jedem einzelnen der zehn berechneten Modelle ausgetauscht und die gleiche Metrik wurde erstellt. Durch die neu trainierten Modelle verbesserte sich die Genauigkeit der Pipeline im Durchschnitt auf 87,46 %. Das vorherige ELMo–Modell wurde durch die Nachverarbeitung um 1 % verbessert. Die durchschnittliche Genauigkeit der neu trainierten ELMo–Modelle erreichte 87,21 % was bedeutete, dass die Nachverarbeitung das Ergebnis um 0,25 % verbesserte. Durch die Tatsache, dass die gesamte Eingabe zum Berechnen der Metrik betrachtet wurde und jede Eingabesequenz mindestens das Attribut Produkt enthalten musste, war die Confusion Matrix einseitig. Die Werte Precision und Recall wurden aus den Werten der Confusion Matrix berechnet weshalb der daraus resultierende F1–Score von 93,1 % nicht mit dem vorherigen F1–Score verglichen werden konnte. Das vorherige ELMo–Modell erreichte ein F1–Score von 89 %, dieses entsprach einer Verbesserung von etwa 4 %. Die allgemeine Genauigkeit verbesserte sich um ungefähr 7 %. Daraus resultierte, dass der ELMo Algorithmus — mit fast der vierfachen Menge an Daten — nur marginal besser wurde. Die vorgestellte Anwendung soll dem Nutzer die Bedienung bzw. den Umgang mit der Pipeline näherbringen. Dazu wird ein grundlegendes Verständnis der Anwendung durch die Benutzung der einfachen Eingabe vermittelt. Dieses beinhaltet das manuelle Setzen von Attributen. Diese Funktion ist ein wesentlicher Bestandteil der Arbeit, da zum Trainieren aller Modelle mit Bezeichner versehene Daten benötigt werden. Zusätzlich wird das manuelle Setzen der Attribute in jeder Realisierung einer solchen Anwendung benötigt, da das System nicht immer alle Attribute korrekt erfasst. Eine mögliche Anwendung für die Pipeline ist ein Chatbot welcher ebenfalls Teil des Prototyps ist. Dieser soll Anwendern zeigen, welche Vorteile ein virtueller Marktplatz gegenüber herkömmlichen Marktplätzen besitzt. All diese Funktionen wurden von mehreren Probanden getestet und bewertet. Die Probanden wurden aufgrund ihrer verschiedenen Fachkenntnisse ausgewählt, so wurden User Experience (UX), User Interface (UI)-Designer sowie Software Engineering (SE) befragt um ein umfassendes Feedback zu erhalten. Insgesamt haben an der Evaluation zehn Personen teilgenommen, wobei jeder dieselben Szenarien zu bewältigen hatte. Zu Beginn wurde das Umfeld des Tests vorgestellt: Der Proband befindet sich in der Facebook Gruppe \"Flohmarkt Karlsruhe\" und versucht in dieser Gruppe sein altes Smartphone zu verkaufen. In dem ersten Durchlauf wurde das Ergebnis der Pipeline nur angezeigt, damit die Testperson sieht, wie die Anfrage verarbeitet wird und welche Attribute von der Pipeline erkannt werden. In dem zweiten Durchlauf sollten die Probanden eine Anfrage erzeugen, die nicht erkannt wird. Im Anschluss wurde das manuelle Tagging getestet. In dem letzten Testszenario sollten die Probanden die Eingabe der ersten Anfrage in dem Chatbot wiederholen. Während der Tests wurden die Probanden beobachtet wie diese den Prototypen bedienen. Den Abschluss bildete ein Fragebogen (siehe Anhang). Die Ergebnisse der Evaluation werden im folgenden Absatz vorgestellt. Das Szenario, in dem die Probanden die einfache Eingabemaske zum Erstellen eines Inserats bedienen sollten, wurde von 9 Teilnehmern direkt verstanden und es wurde eine passende Anfrage an das System geschickt. Die Ansicht mit der farblichen Hervorhebung wurde von allen Probanden auf Anhieb verstanden. 30 % der Probanden waren unsicher bezüglich der erkannten Attribute. So wurde z.B. bei der Eingabe: \"Hey ich verkaufe mein altes Handy. Es ist ein Huawei P30 preis verhandelbar\" Huawei P30 als Produkt erkannt. Für den Probanden sollte Handy das Produkt sein, Huawei die Marke und P30 das Modell. In dem Abschnitt 3.1 wurde bereits erläutert, warum von der Pipeline Huawei P30 korrekterweise als Produkt Durch die erste Aufgabe haben die Probanden erkannt, welche Attribute von der Pipeline erkannt werden sollten, was die Bearbeitung der zweiten Aufgabe erleichterte. Die Anwender werden nach der Eingabe vom System gefragt, ob die erkannten Ergebnisse korrekt sind. Bei einer Verneinung wird die Anfrage an den Tagger weitergeleitet und der Anwender wird aufgefordert seine Eingabe manuell mit Attributen zu versehen. Wird das Ergebnis abgelehnt, war jedem Probanden bewusst, dass die Attribute manuell gesetzt werden sollen, ohne dass eine solche Anweisung von der Oberfläche angezeigt wird. Alle Probanden versuchten zunächst die farblichen Schaltflächen der Attribute auf die passenden Begriffe der Eingabe per Drag–and–Drop zu ziehen. Erst nach einigen Versuchen wurde die Bedienung des Taggers verstanden. Das vorherige erwähnte Problem der Unsicherheit — was genau mit welchem Attribut zu versehen ist — hatten hier 6 von 10 Probanden. In dem letzten Szenario wurde der Chatbot evaluiert. Durch die vorherigen Aufgaben und die kleine Beschreibung des Chatbots war eine problemlose Bedienung möglich. Je nach Anfrage erhielten die Anwender eine Antwort, entweder dass ein passendes Angebot gefunden werden konnte oder die Rückmeldung, dass kein Angebot vorhanden ist. In beiden Fällen wurde deutlich welche Attribute in der Anfrage enthalten waren. Dadurch konnten die Probanden nachvollziehen, dass kein passendes Angebot für die jeweilige Eingabe gefunden werden konnte. Von allen Teilnehmern wurde die Zustandslosigkeit bzw. das nicht Fortführen der Verhandlung des Chatbots negativ wahrgenommen. Der Fragebogen teilt sich in vier Hauptkategorien auf: Oberfläche, Funktionalität, natürliche Sprachverarbeitung und Feedback. Die gesamten Antworten können dem Fragebogen aus dem Anhang entnommen werden. Im Folgenden werden einige der Antworten vorgestellt. Der Chatbot stellt eine reale Anwendung dar, der dem Nutzer die Vorteile der natürlichen Sprachverarbeitung zeigen soll. In dem Testdurchlauf wurde bereits deutlich, dass die Probanden davon ausgingen, dass der Chat fortgeführt werden würde. Diese Erkenntnis spiegelt sich deutlich in der Umfrage wieder, da die Stimmen bei \"Zielführend\" gleich zwischen -1 und +1 aufgeteilt sind. Zudem erwarteten einige Probanden, dass mehr als nur ein Resultat auf die gegebene Anfrage angezeigt werden würde. Auch wurde mehrfach versucht, die Suche durch weitere Anfragen zu spezifizieren, was aufgrund der Zustandslosigkeit nicht möglich war. Das Tagging spielt eine wesentliche Rolle bei allen NLP Anwendungen. Selbst gut funktionierende Anwendungen sollten Nutzern die Möglichkeit geben, die eigene Eingabe manuell mit Attributen zu versehen, falls diese nicht korrekt erkannt wurden. Ist dieses nicht möglich, wird der Nutzer nicht verstanden und eine Benutzung der Anwendung ist unmöglich. In der Evaluation sollte der Tagger verwendet werden, ohne dass dieser erklärt wird. Wie der Abbildung 27 zu entnehmen ist war dieser Test teilweise erfolgreich: der Tagger ist sowohl zielführend als auch optisch ansprechend. Die Intuitivität hingegen wurde besser bewertet als in der Evaluation beobachtet werden konnte, da alle Probanden erst nach einigen Versuchen die Bedienung verstanden haben. Durch die vorherige Aufgabe wurde bereits ein gewisses Verständnis der farblichen Hervorhebung vermittelt, weshalb die Verständlichkeit des Taggers gut ist. Die meisten Teilnehmer hatten das Gefühl, dass die Anwendung die gegebene Anfrage verstehen würden. Diese Frage wurde überwiegend durch den ersten Eindruck beantwortet, da die meisten Probanden weniger als 6 Anfragen an das System stellten. Dies ist ein wesentliches Kriterium für eine reale Anwendung, da der erste Eindruck entscheidend dafür ist, ob das Programm weiterhin verwendet wird oder nicht. Somit hatten Anwender das Gefühl, von dem System verstanden zu werden was NA2 erfüllt. Ebenfalls relevant ist die Frage, ob diese Art der Produktsuche gegenüber der herkömmlichen Schlagwortsuche bevorzugt werden würde. 9 von 10 Probanden stimmten dem zu. Abbildung 27: Ergebnisse der Umfrage bezüglich der Tagging Funktion Zusammenfassend war die Evaluation erfolgreich, da alle Probanden mit dem Prototyp zufrieden waren. Was genau mit welchen Attributen versehen werden sollte sowie die Oberfläche des Taggers, benötigt eine kurze Erklärung, damit Nutzer genau wissen was die Anwendung erwartet. Ebenfalls sollte das Aussehen der meisten Funktionen überarbeitet werden, da diese bei der Evaluation überwiegend neutral bewertet wurden. Diese Arbeit befasst sich mit dem Umwandeln von unstrukturierter Eingabe in strukturierte Ausgabe, die von einem Computer weiterverarbeitet wird. Um dieses Ziel zu erreichen wurden verschiedene Methoden und Algorithmen angewendet und evaluiert. Im Rahmen dieser Arbeit wurde ein generisches Konzept entwickelt, durch das ein virtueller Marktplatz mit der natürlichen Sprachverarbeitung unterstützt werden kann. Die Konzeption wurde prototypisch umgesetzt, um zusätzlich zu den Anforderungen, das Sammeln von Daten und Bewerten der Performance zu unterstützen. Die Forschungsfragen FF1 und NF2 können mit der Pipeline beantwortet werden. Im Bereich des ML ist die vorhandene Menge der Daten ein wesentlicher Faktor um zielführende Modelle zu erstellen. Der mit dieser Arbeit verbundene Datensatz umfasste lediglich 260 Einträge. Das auf diesem Datensatz erzielte Ergebnis von 87,46 % Genauigkeit, welches im Abschnitt 5.2 hergeleitet wurde, zeigt dass auch mit einer kleinen Menge von Daten ein solides Ergebnis entstehen kann, was NF1 beantwortet. Dieses Ergebnis übertrifft die anfänglichen Erwartungen deutlich, da vermutet wurde das die ganzen Produkte zu verschieden sind, um diese so präzise zu bestimmen. Somit konnten alle anfänglich gestellten Forschungsfragen beantwortet werden. Mit der prototypischen Implementierung konnte das Verständnis der Anwender gegenüber einem solchen System untersucht werden. Die Evaluation zeigt sowohl eine hohe Akzeptanz als auch das Interesse der Nutzer gegenüber der neuen Art auf virtuellen Marktplätzen zu handeln. Diese Erkenntnisse erzeugen, zusätzlich zu den bereits erwähnten aus Abschnitt 1.1, weitere Mehrwerte die ein Unternehmen erhalten würde wie z.B. ein innovatives, akzeptiertes Verfahren der Anfragenverarbeitung. Die Oberfläche sowie Funktionsweise des Taggers wurde während der Verwendung des Prototyps positiv wahrgenommen, was zeigt, dass dieser als Vorlage für eine reale Anwendung verwendet werden kann. Ein Tool das beliebige Attribute in einer Eingabesequenz klassifizieren kann, hat viele verschiedene Anwendungsbereiche. Der Prototyp umfasst bereits alle Funktionalitäten, die für eine neue Applikation benötigt werden. So werden beispielsweise Daten gesammelt und gegebenenfalls nachträglich überarbeitet. Auch wird dem Nutzer mit dem Chatbot eine Anwendung geboten, die die Vorteile eines solchen Systems darstellt. Das automatische Generieren der Metriken zeigt die Stärken des Systems und durch die verschiedenen Graphen wird das Finden von Schwächen erleichtert. Weitere Ideen für Funktionalitäten wurden bereits während der Entwicklung des Prototyps deutlich, die Aufgrund des Umfangs dieser Arbeit nicht realisiert wurden. Ein Feature war das automatische Trainieren der Pipeline, sobald eine gewisse Menge von neuen Daten zur Verfügung stand. Dieses würde dafür sorgen, dass die Performance der Anwendung sich von selbst verbessert, ohne dass das Training manuell gestartet werden müsste. Auch könnte der Tagger in seiner Funktionalität erweitert werden. So besteht für den Anwender die Möglichkeit eigene Attribute hinzuzufügen, welche dann im Tagger verwendet werden können. Durch diese Änderung könnten bereits Daten für andere Kategorien gesammelt werden, welches ein späteres Erweitern der Pipeline erleichtert. Durch das Feedback der Evaluation wurde deutlich, dass eine andere Art der Bedienung des Taggers den Umgang verbessern könnte. Eine mögliche Realisierung wäre, dass Nutzer ein Attribut auswählen und im Anschluss die passenden Wörter anklicken wodurch diese farblich hervorgehoben werden. In dem 1.1. Kapitel wurde die Bedienung über Sprachinterfaces vorgestellt, welches ebenfalls eine interessante Erweiterung für den Prototypen darstellt. Die Matchmaking Funktion des Chatbots könnte überarbeitet werden, da diese bisher nur auf die genaue Übereinstimmung von Attributen achtet. Auch könnten dem Chatbot Zustände hinzugefügt werden, wodurch dem Anwender eine echte Konversation suggeriert wird. Zum Schluss dieser Arbeit ergeben sich neue Fragen, wie z.B.: \"Wie gut skaliert die Pipeline?\" bzw. \"Wie viele Attribute können die einzelnen Modelle unterscheiden, ohne dass die Genauigkeit beeinflusst wird?\". Hierbei handelt es sich um wesentliche Aspekte, die für die Realisierung eines virtuellen Marktplatzes in dieser Form benötigt werden. Die Pipeline selbst könnte durch Ergänzen von mehreren Attributen aus verschiedenen Produktklassen erweitert werden. Dadurch wäre es mögliche, die Frage der Skalierbarkeit zu beantworten. Zadeh, Lotfi A (1965). Fuzzy sets\". In: Information and control 8.3, S. 338–353. \" Levenshtein, Vladimir I (1966). Binary codes capable of correcting deletions, insertions, \" and reversals\". In: Soviet physics doklady. Bd. 10. 8, S. 707–710. Schütze, Hinrich und Jan O Pedersen (1995). Information retrieval based on word senses\". \" In: Citeseer. Schuster, Mike und Kuldip K Paliwal (1997). Bidirectional recurrent neural networks\". \" In: IEEE transactions on Signal Processing 45.11, S. 2673–2681. Cummins, F., F.A. Gers und J. Schmidhuber (1999). Learning to forget: continual pre\" diction with LSTM\". In: IET Conference Proceedings, 850–855(5). Lafferty, John D., Andrew McCallum und Fernando C. N. Pereira (2001). Conditional \" Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data\". In: Proceedings of the Eighteenth International Conference on Machine Learning, S. 282– 289. Tjong Kim Sang, Erik F. und Fien De Meulder (2003). Introduction to the CoNLL-2003 \" Shared Task: Language-Independent Named Entity Recognition\". In: Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, S. 142–147. Mansouri, Alireza, Lilly Suriani Affendey und Ali Mamat (2008). Named entity recogni\" tion approaches\". In: International Journal of Computer Science and Network SecurityArel, Itamar, Derek C Rose und Thomas P Karnowski (2010). Deep machine learning-a \" new frontier in artificial intelligence research [research frontier]\". In: IEEE computational intelligence magazine 5.4, S. 13–18. Faruqui, Manaal und Sebastian Padó (2010). Training and Evaluating a German Named \" Entity Recognizer with Semantic Generalization\". In: KONVENS, S. 129–133. Mikolov, Tomáš u. a. (2010). Recurrent neural network based language model\". In: Ele\" venth annual conference of the international speech communication association. Vicknair, Chad u. a. (2010). A comparison of a graph database and a relational database: \" a data provenance perspective\". In: Proceedings of the 48th annual Southeast regional conference, S. 1–6. Krizhevsky, Alex, Ilya Sutskever und Geoffrey E Hinton (2012). Imagenet classification \" with deep convolutional neural networks\". In: Advances in neural information processing systems, S. 1097–1105. Mikolov, Tomas u. a. (2013). Distributed representations of words and phrases and their \" compositionality\". In: Advances in neural information processing systems, S. 3111–3119. Yoav und Omer Levy (2014). word2vec Explained: deriving Mikolov et al.’s \" negative-sampling word-embedding method\". In: arXiv preprint arXiv:1402.3722. Kalchbrenner, Nal, Edward Grefenstette und Philip Blunsom (2014). A convolutional \" neural network for modelling sentences\". In: 52nd Annual Meeting of the Association for Computational Linguistics. Pennington, Jeffrey, Richard Socher und Christopher D Manning (2014). Glove: Global \" vectors for word representation\". In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), S. 1532–1543. Olah, Christopher (2015). Understanding LSTM Networks. http://colah.github.io/ posts/2015-08-Understanding-LSTMs/. (besucht am 22.11.2019). Seatgeek (2015). Fuzzywuzzy. https://github.com/seatgeek/fuzzywuzzy. (besucht amZhang, Ye und Byron Wallace (2015). A sensitivity analysis of (and practitioners’ gui\" de to) convolutional neural networks for sentence classification\". In: arXiv preprint arXiv:1510.03820. Deepu, S, Pethuru Raj und S Rajaraajeswari (2016). A Framework for Text Analytics \" using the Bag of Words (BoW) Model for Prediction\". In: Proceedings of the 1st International Conference on Innovations in Computing & Networking, S. 12–13. Jiang, Ridong, Rafael E Banchs und Haizhou Li (2016). Evaluating and combining name \" entity recognition systems\". In: Proceedings of the Sixth Named Entity Workshop, S. 21– 27. Bai, Shaojie, J Zico Kolter und Vladlen Koltun (2018). An empirical evaluation of ge\" neric convolutional and recurrent networks for sequence modeling\". In: arXiv preprint arXiv:1803.01271. Chernodub, Artem (2018). Targer. https://github.com/achernodub/targer. (besucht am 28.10.2019). Devlin, Jacob u. a. (2018). Bert: Pre-training of deep bidirectional transformers for lan\" guage understanding\". In: arXiv preprint arXiv:1810.04805. Perone, Christian S, Roberto Silveira und Thomas S Paula (2018). Evaluation of sen\" tence embeddings in downstream and linguistic probing tasks\". In: arXiv preprint arXiv:1806.06259. Peters, Matthew E, Mark Neumann, Mohit Iyyer u. a. (2018). Deep contextualized word \" representations\". In: arXiv preprint arXiv:1802.05365. Peters, Matthew E, Mark Neumann, Luke Zettlemoyer u. a. (2018). Dissecting con\" textual word embeddings: Architecture and representation\". In: arXiv preprint arXiv:1808.08949. Alex (2018). Fundamentals of Recurrent Neural Network (RNN) and Long \" Short-Term Memory (LSTM) Network\". In: arXiv preprint arXiv:1808.03314. Chernodub, Artem u. a. (2019). Targer: Neural argument mining at your fingertips\". In: \" Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, S. 195–200. May, Philip (2019). German ELMo Model. https://github.com/t-systems-on-siteservices-gmbh/german-elmo-model. (besucht am 24.10.2019). Reimers, Nils und Iryna Gurevych (2019). Alternative Weighting Schemes for ELMo \" Embeddings\". In: arXiv preprint arXiv:1904.02954. SpaCy–Dokumentation (2019). Library architecture. https : / / spacy . io. (besucht amDeepset (o.D.). German Word Embeddings. https : / / deepset . ai / german - word embeddings. (besucht am 25.10.2019). 62 Anhang A Ergebnisse der Umfrage Anhang A. Ergebnisse der Umfrage Abbildung 28: Bewertung der Oberfläche I Anhang A Ergebnisse der Umfrage Abbildung 29: Fortsetzung der Bewertung zur Oberfläche II Anhang A Ergebnisse der Umfrage Abbildung 30: Bewertung der Funktionalität III Anhang A Ergebnisse der Umfrage Abbildung 31: Fortsetzung der Bewertung zur Funktionalität Abbildung 32: Allgemeine Fragen zur natürlichen Sprachverarbeitung IV Anhang A Ergebnisse der Umfrage Abbildung 33: Fortsetzung der natürlichen Sprachverarbeitung V Anhang A Ergebnisse der Umfrage Abbildung 34: Fortsetzung der natürlichen Sprachverarbeitung VI Anhang B Ergebnisse der Umfrage Abbildung 35: Feedback der Probanden VII Anhang B Metriken der Pipeline B. Metriken der Pipeline Abbildung 36: Confusion Matrik mit dem Attribut \"Preis\"von dem Schritt reguläre Ausdrücke Abbildung 37: Confusion Matrik mit dem Attribut \"Kamera\"von dem Schritt reguläre Ausdrücke VIII Anhang B Metriken der Pipeline Abbildung 38: Confusion Matrik mit dem Attribut \"Hersteller\"von dem Schritt Fuzzy Matching Abbildung 39: Confusion Matrik mit dem Attribut \"Farbe\"von dem Schritt Fuzzy Matching IX '"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: Nico Wellermann Thema: Natürlichsprachliche Kommunikation in virtuellen Marktplätzen Arbeitsplatz: CAS Software AG, Karlsruhe Referent: Prof. Dr. Wölfel Korreferent: Prof. Dr. Körner Abgabetermin: 09.02.2020 Karlsruhe, 09.11.2019 Der Vorsitzende des Prüfungsausschusses Prof. Dr. Heiko Körner Fakultät für Informatik und Wirtschaftsinformatik Bachelor-Thesis Erklärung Hiermit versichere ich, dass ich meine Abschlussarbeit selbständig verfasst und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt habe. Datum: ....................................................... (Unterschrift) ZUSAMMENFASSUNG I. Zusammenfassung Virtuelle Marktplätze werden von Jahr zu Jahr bedeutsamer. Auf elektronischen Marktplätzen beispielsweise finden viele Käufer–Verkäufer Situationen statt. Anwender erstellen ein digitales Angebot, das von potenziellen Käufern gefunden werden möchte. Für eine intuitive Bedienung soll die Verwendung der natürlichen Eingabe untersucht werden. Diese Arbeit befasst sich mit dem Erfassen und Strukturieren von Informationen, die in Textform vorliegen. Aus diesen Texten sollen die wesentlichen Informationen extrahiert werden und mit Hilfe der strukturierten Daten soll es dann möglich sein, passende Angebote für eine gegebene Anfrage zu finden. Dafür wird ein Prototyp in Form einer Webanwendung entwickelt, welcher die verschiedenen Aufgaben eines virtuellen Marktplatzes erfüllt. Zum Erkennen der Attribute aus der Texteingabe wurden mehrere Algorithmen verwendet, die zusammen eine Pipeline bilden. Um die Performance der Pipeline messen zu können, wurden verschiedene Metriken aufgestellt. Durch das Erstellen einer Pipeline können die Stärken der einzelnen Algorithmen kombiniert werden und somit das Ergebnis optimiert. Die verwendeten Algorithmen zum Erkennen des Hauptattribut “Produkt” erreichten eine Genauigkeit von 87,46 % auf den vorhandenen Datensatz. Beschreibbare Attribute wurden durch regelbasierte Ansätze aus dem Text extrahiert, was ein zielführendes Ergebnis erzielte. Die gemessenen Ergebnisse übertrafen deutlich die vorherigen Erwartungen und Anwender hatten das Gefühl als würde das System sie verstehen. Dieses Verfahren ermöglicht eine neue Art, um Verhandlungen in virtuellen Marktplätzen zu führen. I ZUSAMMENFASSUNG II. Abstract Since few years, virtual marketplaces are catching more and more attention. Daily, hundreds of thousands buyer and seller situations all over the world take place on so called electronic marketplaces. Users create digital offers that are found by potential buyers. Unfortunately, one online search creates numerous possible offers leading to a time-consuming comparison between many different offers. Therefore, an intuitive workflow using natural input is needed to facilitate each query. Main goal of the thesis was to collect and structure the query input, sort the collected information to present one perfect-fit offer. For this purpose, a prototype in form of a web application was developed fulfilling various tasks of a virtual marketplace. Several algorithms like ELMo and Targer were used to recognize attributes from the text input, forming a pipeline. By creating a pipeline, the strength of each individual algorithm was combined to optimize the overall result. In order to measure the performance of the pipeline, different metrics were set up. Algorithms used to recognize the main attribute “product” achieved an accuracy of 87.46 % on the existing data set. Describable attributes were extracted from the text using a rule-based system, which achieved a target-oriented result. The measured results significantly exceeded previous expectations as users have the feeling of being understood by the system. This technique enables a new way of negotiating in virtual marketplaces. II INHALTSVERZEICHNIS II Inhaltsverzeichnis I Zusammenfassung I II Abstract II II Inhaltsverzeichnis III IV Abkürzungsverzeichnis V 1 Einleitung 1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 Ziel der Arbeit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.3 Gliederung . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1 2 3 2 Grundlagen 2.1 Bag–of–words . . . . . . . . . 2.2 Word Embeddings . . . . . . 2.2.1 Word2Vec . . . . . . . 2.2.2 GloVe . . . . . . . . . 2.2.3 ELMo . . . . . . . . . 2.3 Convolutional Neural Network 2.4 Recurrent Neural Network . . 2.5 Unterschiede RNN und CNN . 2.6 Long Short–Term Memory . . 2.7 Conditional Random Fields . 2.8 Fuzzy–Suche . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 4 4 5 5 6 7 10 11 11 13 14 3 Konzeption 3.1 Annahmen . . . . . . . . . . . . . . . . . 3.2 Anforderungen . . . . . . . . . . . . . . 3.3 Komponenten . . . . . . . . . . . . . . . 3.3.1 Benutzerschnittstelle . . . . . . . 3.3.2 Hybrid named-entity recognition 3.3.3 Tagging . . . . . . . . . . . . . . 3.3.4 Persistierung . . . . . . . . . . . 3.3.5 Auswertung . . . . . . . . . . . . 3.4 Prozesse . . . . . . . . . . . . . . . . . . 3.5 Bewertungskriterien . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 16 17 20 20 21 23 24 25 25 26 4 Umsetzung 4.1 Vorgehen . . . . . . . . . . . . . . 4.2 Datenakquise . . . . . . . . . . . 4.3 Hybrid named-entity recognition . 4.3.1 SpaCy . . . . . . . . . . . 4.3.2 Reguläre Ausdrücke . . . . 4.3.3 Metadaten Analyse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 28 29 30 30 31 33 . . . . . . . . . . . . . . . . . . . . . . . . III INHALTSVERZEICHNIS 4.4 4.5 4.3.4 Targer . . . . . . . . . . . . 4.3.5 ELMo . . . . . . . . . . . . 4.3.6 Fuzzy Matching . . . . . . . 4.3.7 Zusammenspiel der Pipeline Verwendeten Technologien . . . . . Entwicklung des Prototyps . . . . . 4.5.1 Eingabemethode Inserat . . 4.5.2 Berechnung der Metriken . . 4.5.3 Chatbot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 39 41 44 45 46 46 46 48 5 Evaluation 51 5.1 Evaluationsmethodik . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 5.2 Auswertung der Algorithmen . . . . . . . . . . . . . . . . . . . . . . . . . . 51 5.3 Auswertung der Oberfläche . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 6 Fazit 58 7 Ausblick 58 8 Quellenverzeichnis 60 Anhang I A Ergebnisse der Umfrage I B Metriken der Pipeline VIII IV INHALTSVERZEICHNIS IV. Abkürzungsverzeichnis biLSTM biRNN BoW CNN CRF ELMo GB GloVe LSTM ML MP NER NET NLP POS REST RNN SE Tanh UI UX bidirectional Long Short–Term Memory bidirectional Recurrent Neural Network Bag–of–words Convolutional Neural Network Conditional Random Field Embeddings from Language Models Gigabyte Global Vectors Long Short–Term Memory Machine Learning Megapixel Named Entity Recognition Named Entity Tagging Natural Language Processing Part of Speech Representational State Transfer Recurrent Neural Network Software Engineering Tangens hyperbolicus User Interface User Experience V Kapitel 1 Einleitung 1. Einleitung 1950 erfand der britische Mathematiker Alan Turing den nach ihm benannten “TuringTest”. Dieser Test sollte feststellen ob Menschen und Maschinen über die gleiche Art von Intelligenz verfügen. Hierbei versucht der Mensch durch Konversation zu entscheiden, ob es sich bei seinem Gegenüber um eine Maschine oder eine Person handelt. Jedoch besteht dieser Test nur als theoretischer Versuch, der erst mit dem Durchbruch der künstlichen Intelligenz zu praktischen Versuchen führte. Bereits 1980 widerlegte der amerikanische Philosoph John Searle die Theorie des “TuringTests”. Mit dem Gedankenexperiment durch das sogenannte “chinesische Zimmer” zeigte er auf, dass Computer, nur weil diese immer die korrekte Antwort geben, keine Experten auf dem Gebiet sein müssen. Das Experiment sieht folgendermaßen aus: In einem geschlossenen Raum sitzt eine Person, die weder Chinesisch spricht noch versteht. Diese hat die Aufgabe, auf chinesisch gestellte Fragen anhand einer in seiner Muttersprache verfassten Anleitung auf chinesisch zu beantworten. Personen außerhalb des Zimmers denken, dass der Mensch in dem Zimmer in der Lage ist chinesisch zu sprechen, obwohl dieser nur einfache Regeln befolgt. Übertragen auf Computer zeigt das Gedankenexperiment, dass die Computer nur weil sie richtige Ergebnisse produzieren, noch lange nicht in der Lage sind die Fragen bzw. die Situationen zu verstehen. Dennoch wird daran geforscht (Arel, Rose und Karnowski 2010), (Devlin u. a. 2018), dass Computer mithilfe von verschiedensten Algorithmen eine Art Bewusstsein über Situationen erlernen und sogar verstehen — also eine künstliche Intelligenz entwickeln. Die natürliche Sprachverarbeitung ist dabei auch ein Baustein, an dem stetig geforscht wird. Dabei wird unter anderem versucht, dem Computer die menschliche Sprache zu vermitteln. Bei virtuellen Marktplätzen hat sich seit der Einführung viel verändert. So wird z.B. das Kundenverhalten mithilfe neuronaler Netze analysiert, um gezielt Vorschläge zu platzieren. Die eigentliche Produktsuche hat sich hingegen kaum verändert. In dieser Arbeit wird ein alternatives Verfahren untersucht, das mithilfe der natürlichen Sprachverarbeitung die herkömmliche Produktsuche ersetzen soll. 1.1. Motivation In der heutigen Zeit ist es schwieriger Angebot und Nachfrage im E–Commerce zusammenzubringen. Die existierenden Systeme bieten nicht genug Freiheiten, um die Anfrage genau zu spezifizieren. Daher wird nicht nur das gesuchte Produkt, sondern auch viele andere 1 Kapitel 1 Einleitung Ergebnisse gefunden. Mit verschiedenen Eingabefeldern kann die Auswahl genauer spezifiziert werden, was aber in den meisten Fällen zu generisch oder schlicht zu umständlich ist und deshalb nicht verwendet wird. Für Anwender ist es viel natürlicher etwas direkt zu beschreiben, als es in starre, vorgefertigte Formulare einzutragen. Diese Art der Eingabemethode setzt allerdings voraus, dass die Maschine den Nutzer versteht und dessen unstrukturierte Eingabe in eine für den Computer verständliche Struktur überführt. Mit diesem Vorgehen können sowohl Angebote als auch Nachfragen ohne mehrere Formularfelder auskommen, da nicht mehr für jedes Attribut ein eigenes Feld benötigt wird. Ein weiterer Vorteil dieses Vorgehens ist, dass eine eins zu eins Umsetzung zu Sprachassistenten möglich ist. Dazu wird die Sprache von einem bereits existierenden Assistenten (z.B. Amazon Alexa oder Google Assistant) erkannt und als Textform an das System übergeben. Durch den Einsatz von solchen Technologien ergeben sich Mehrwerte für Unternehmen, da Kunden bessere Vorschläge gemacht werden können und diese somit häufiger zum Kaufen angeregt werden. Auch kann durch die optimale Voraussetzung eines Sprachassistenten eine bequeme Alternative geboten werden, wodurch die Plattform häufiger verwendet und das Unternehmen attraktiver wird. Der Einsatz von virtuellen Marktplätzen erfordert ein System, welches die Anwender sowie ihre Bedürfnisse versteht und somit von Nutzern verwendet wird. Zudem werden weitere Informationen der Waren benötigt, damit die Eingabe auf diese Attribute geprüft werden kann. Um diese Herausforderungen anzugehen, könnten Technologien wie Natural Language Processing (NLP) oder Machine Learning (ML) verwendet werden. 1.2. Ziel der Arbeit Ziel dieser Arbeit ist es, ein Konzept und Prototyp für einen virtuellen Marktplatz mit natürlicher Sprachverarbeitung zu entwickeln anhand dessen die Bedienbarkeit und Performance untersucht wird. Im Vordergrund steht dabei das Erkennen bzw. Klassifizieren der Attribute aus einer Eingabesequenz. Jede Produktkategorie verfügt über eigene Attribute, die von einem solchen System erkannt werden müssen. Zu Beginn werden diese reduziert und auf die Produktkategorie der Smartphones mit den Attributen: Produkt, Hersteller, Preis, Farbe, Kamera und Speicher beschränkt. Zudem wurde als Beispiel ein virtueller Marktplatz in Form eines Chatrooms betrachtet, was bedeutet, dass keine grammatikalisch, korrekte Schreibweise den Anfragen vorausgesetzt wird. Die Anwendung sollte in der Lage sein, sowohl Abkürzungen als auch Emojis handhaben zu können, was eine größere Herausforderung an die Algorithmen stellt. Für ein besseres Verständnis kann folgendes Beispiel betrachtet werden: “Hiiii, lz woche ist mein handy kaputt gegangen :( ich suche deshalb jetzt ein iphone x gerne auch gebraucht aber nicht teurer als 500 e”. In diesem 2 Kapitel 1 Einleitung Szenario sollte das System “iphone x” als Produkt und “500 e” als Preis erkennen. Anwendern soll das Gefühl vermittelt werden, als würde das System sie verstehen. Dieses soll zudem mit einer geringen Menge von Daten erreicht werden. Beschrieben wird dieses Verhalten durch die Forschungsfrage (FF1 ): “Wie kann aus natürlicher unstrukturierter menschlicher Eingabe eine strukturierte Ausgabe erzeugt werden, die von einem Computer weiterverarbeitet werden kann?”. Begleitend dazu werden die folgenden Nebenfragen (NF1 ) beachtet: “Wie kann ein akzeptables Ergebnis mit einer sehr geringen Menge von Daten erzielt werden?” und (NF2 ) “Wie können die wichtigsten Schlüsselwörter aus einem Text ausgewertet werden?”. Um dies zu erreichen werden verschiedene Methoden und Tools untersucht. 1.3. Gliederung In Kapitel 2 werden die verwendeten Techniken und Modelle zum Erreichen des Ziels dieser Arbeit vorgestellt. Kapitel 3 beginnt mit einer ausführlichen Betrachtung der Annahmen, gefolgt von den Abschnitten: Anforderungen, Komponenten, Prozesse und Bewertungskriterien. Im Anschluss wird in Kapitel 4 eine Pipeline konzipiert, die verschiedenen Algorithmen auf das Problem anwendet. Parallel zur Entwicklung der Pipeline wurde ein Prototyp erstellt, der mehrere Aufgaben erfüllt: einfacheres Sammeln von Daten, visuell unterstützte Bedienung der Pipeline, sowie das Aufstellen von Metriken bezüglich der Pipeline. Die Implementierung des Prototyps wird im Abschnitt 4.5 beschrieben. Eine ausführliche Betrachtung bezüglich der Performance des Prototyps sowie Auswertung der Oberfläche wird in Kapitel 5 erläutert. Zum Abschluss wird in Kapitel 6 das Ergebnis zusammengefasst und weitere zukünftige mögliche Erweiterungen werden in Kapitel 7 vorgestellt. 3 Kapitel 2 Grundlagen 2. Grundlagen In diesem Kapitel werden die verwendeten Technologien und theoretischen Grundlagen vorgestellt, welche bei der Bearbeitung dieser Arbeit verwendet wurden. Zu Beginn werden Word Embeddings erläutert, welche ein wesentlicher Bestandteil dieser Arbeit darstellen. Im Anschluss werden die relevanten Eigenschaften der Modelle aus dem Bereich Machine Learning (ML) vorgestellt. 2.1. Bag–of–words Sätze bzw. Wörter können nicht direkt von einem Computer verstanden werden. Dieser benötigt eine andere Repräsentation, um den Inhalt des Satzes zu verstehen. Wenn zum Beispiel die beiden Sätze: “Das Wetter heute ist schön” und “Das Wetter heute ist toll” betrachtet werden, ergibt sich ein Vokabular mit den Worten: [Das, Wetter, heute, ist, schön, toll]. Das Vokabular muss alle Wörter umfassen, die vom System erkannt werden sollen. Mit der “Bag–of–words” Codierung können die beiden Sätze für den Computer verständlich gemacht werden (Deepu, Raj und Rajaraajeswari 2016). Dazu wird ein leerer Vektor der Dimension entsprechend der Länge des Vokabulars erstellt und mit 0 gefüllt, jede Position entspricht somit einem Wort. Im Anschluss wird für jedes Wort der Eingabe geprüft, an welcher Position es sich befindet und diese Stelle im Vektor inkrementiert. Für die beiden Beispielsätze würde die “Bag–of–words” Codierung so aussehen: [1, 1, 1, 1, 1, 0] und [1, 1, 1, 1, 0, 1]. Der Nachteil dieser Repräsentation ist, dass die Reihenfolge der Wörter verloren geht und somit keine Beziehungen mehr erkannt werden können. Auch können ähnliche Bedeutungen bei verschiedenen Wörtern nicht abgebildet werden, was bedeutet die Worte wie: “schön” und “toll” genauso verschieden sind wie “Wetter” und “heute”. 2.2. Word Embeddings Word Embeddings bestehen aus vielen Wortvektoren (Schütze und Pedersen 1995) die Wörter für den Computer verständlich darstellen. Mit Wortvektoren wird versucht die Beziehungen der Wörter zueinander beizubehalten und das Wort durch einen Vektor zu repräsentieren. Dabei liegen Wörter mit ähnlicher Bedeutung im Vektorraum nah zusammen, wohingegen Wörter mit unterschiedlicher Bedeutung weit auseinander liegen. Die Dimension des Vektors spiegelt die Genauigkeit des Word Embeddings wieder. Eine größere Dimension beschreibt jedes Wort genauer, benötigt aber auch mehr Speicherplatz sowie mehr Zeit zum Erstellen der Vektoren. Zum Erstellen der Wortvektoren können 4 Kapitel 2 Grundlagen verschiedene Verfahren verwendet werden (Perone, Silveira und Paula 2018). Die Verfahren Word2Vec und Global Vectors (GloVe) erzeugen pro Wort einen kontextbasierten Wortvektor. Die Embeddings from Language Models (ELMo) Repräsentation kann für ein Wort mehrere Wortvektoren erstellen um verschiedene Kontexte abzubilden. 2.2.1. Word2Vec Word2vec (Tomas Mikolov u. a. 2013) ist ein Verfahren, das Algorithmen verwendet die basierend auf dem Kontext einer Eingabe einen numerischen Vektor erzeugen. Im Gegensatz zu Bag–of–words (BoW) erhalten damit verschiedene Wörter in demselben Kontext einen ähnlichen Vektor. Dazu werden vier Schritte benötigt (Goldberg und Levy 2014). Als Erstes werden die Daten für eine unüberwachte Vorhersage vorbereitet, also Eingabe und Ziel der Vorhersage als Tupel. Betrachtet wird dieser Satz: “Als es an der Tür klingelte, rannte der Hund los.”, die ersten Tupel wären [als, es], [es, als] und [es, an]. Es wird über jedes Wort der Eingabe iteriert, dass aktuelle Wort ist dabei immer der erste Wert in dem Tupel. Die Fenstergröße gibt an, wie viele Wörter vor und nach dem aktuellen Wort beachtet werden sollen (hier Fenstergröße = 1). Die betrachteten Wörter durch die Fenstergröße bilden den zweiten Wert des Tupel. Ein größeres Fenster bringt bessere Performance zulasten der Berechnungszeit. Im Anschluss wird eine Matrix erstellt, die für jedes Wort der gesamten Trainingsdaten einen zufällig erstellten Wortvektor bereitstellt. Dann folgt die Optimierung durch das neuronale Netzwerk. Dazu werden die Tupel einzeln verarbeitet und für die Eingabe wird der Wortvektor aus der Matrix verwendet. Das Netz berechnet basierend auf der Eingabe eine Vorhersage des nächsten Wortes, welches mit dem tatsächlichen nächsten Wort verglichen wird. Basierend auf dem Ergebnis wird sowohl das Netz als auch der Wortvektor optimiert. Nach dem Training wird die Matrix gespeichert und kann als Word Embedding verwendet werden. 2.2.2. GloVe Ein Nachteil der Word2Vec Repräsentation ist, dass diese nur die umliegenden lokalen Wörter betrachtet, um daraus die Wortvektoren zu erstellen. Für den Satz “Der Hund spielt auf der Couch.” ist nicht eindeutig ob “der” in besonderer Beziehung zu “Hund” und “Couch” steht oder ob es sich bei “der” um ein Stoppwort handelt. Global Vectors (GloVe) betrachtet beide Repräsentationen, die globale sowie lokale Sicht. Wie von (Pennington, Socher und Manning 2014) wird das Vokabular der Trainingsdaten in einer co–occurrence Matrix abgebildet. Eine beispielhafte co–occurrence Matrix kann der Abbildung 1 entnommen werden. Durch ein stochastisches Verfahren lässt sich berechnen wie relevant ein Wort zu einem gegebenen anderen Wort ist. Dabei gilt, dass ein hoher 5 Kapitel 2 Grundlagen Wert (>1) eine hohe Relevanz und ein niedriger Wert (<1) irrelevantes Verhalten repräsentiert. Abbildung 1: Die co–occurrence Matrix für den Satz ”Heute gibt es Kuchen da es regnet”mit einer Fenstergröße von 1 2.2.3. ELMo Neuronale Netze benötigen eine spezielle Darstellung von Wörtern. Bei Word Embeddings wird die Semantik von Wörtern in Form von Vektoren dargestellt. Ein bei diesem Ansatz nicht beachtetes Problem ist die Tatsache, dass ein Wort in unterschiedlichen Kontexten verschiedene Bedeutungen haben kann. “Für meine Familie suche ich ein neues Schloss zum Wohnen” und “Für meine Tür suche ich ein neues Schloss”, in beiden Sätzen wird das Wort “Schloss” verwendet, aber die Bedeutung ist offensichtlich eine andere. Einfache Wortvektoren wie die vorgestellten GloVe und Word2Vec sind nicht in der Lage den Unterschied dieser Wörter zu erkennen, sie hätten dieselbe Bedeutung. Um dieses Verhalten besser abbilden zu können, werden contextualized word–embeddings (Peters, Neumann, Iyyer u. a. 2018) verwendet. Anders als bei einfachen Word Embeddings, wird eine ganze Sequenz anstelle eines einzelnen Wortes eingegeben. Dadurch ist das Modell in der Lage, den Kontext der einzelnen Wörter zu ermitteln und kann so genauere Wortvektoren zurückgeben. Wie bereits bei den Word Embeddings werden diese Modelle nur in Ausnahmefällen selbst trainiert. Die benötigte Datenmenge, sowie die Zeit um diese Vektoren zu berechnen ist sehr groß (Peters, Neumann, Zettlemoyer u. a. 2018). Deshalb wird auf vortrainierte Modelle zurückgegriffen, welche dann auf den eigenen Anwendungsfall optimiert werden. Zum Erstellen der Embeddings from Language Models (ELMo) wird ein Modell verwendet, dessen Aufgabe darin besteht, das nächste Wort einer Sequenz vorherzusagen. Diese Aufgabe kann unüberwacht ausgeführt werden und vereinfacht somit das Trainieren. Das 6 Kapitel 2 Grundlagen Modell setzt dabei auf bidirectional Long Short–Term Memory (biLSTM) um ein Gefühl der vorherigen sowie nachfolgenden Wörter zu erhalten. Zum Erstellen der endgültigen Wortvektoren werden die Ergebnisse der einzelnen biLSTM Zellen verwendet. Im ersten Schritt werden die Ergebnisse aus der Vorhersagerichtung sowie der Rückrichtung verkettet. Im Anschluss werden die Vektoren mit einer Gewichtung des Modells multipliziert und zum Schluss summiert. Das Ergebnis ist ein kontextsensitives Word Embedding jedes Wortes eines Satzes. Abbildung 2: Beispielhafte Visualisierung der Wortvektoren für das Wort Schloss Abbildung 2 zeigt eine stark vereinfachte Visualisierung des Word Embedding. Der Wortvektor für das Wort “Schloss” ist, basierend auf den Kontext, verschieden. Wortvektoren können beliebig viele Dimensionen haben, weshalb dieses eine vereinfachte Darstellung ist. 2.3. Convolutional Neural Network Um eine genauere Vorhersage über die Bedeutung der Worte treffen zu können, ist die Betrachtung des Kontextes hilfreich. Es gibt verschiedene modellbasierte Ansätze, um diesen Kontext zu erfassen. Für die Convolutional Neural Network (CNN) liegt der Schwerpunkt in der Bildverarbeitung (Krizhevsky, Sutskever und Hinton 2012). Die Pixelwerte eines Bildes werden verwendet, um daraus Vorhersagen zu treffen, was auf dem Bild zu erkennen ist. Um dieses Ziel zu erreichen werden einfache, Hardware unterstützte Verfahren verwendet, die im Folgenden vorgestellt werden. Generell bestehen CNN aus den drei folgenden, miteinander verknüpften Ebenen (Kalchbrenner, Grefenstette und Blunsom 2014): Eine Faltungsebene, bei der die Eingabematrix mit Hilfe eines Filterkerns auf eine kleinere Matrix reduziert wird, z.B. 5 x 5 als Eingabematrix, 3 x 3 als Filterkern bei einer Schrittweite von 1 erzeugt eine 3 x 3 Matrix. Dazu wird der Filterkern über die Eingabematrix um die Schrittweite verschoben. Bei jedem Schritt werden die übereinander liegende Werte der Filter- und Eingabematrix multipliziert und anschließend alle Werte 7 Kapitel 2 Grundlagen addiert, um den neuen Wert der Ergebnismatrix zu erhalten. Durch Reduzieren der Matrix und Beibehalten der wesentlichen Informationen verringert die Pooling–Schicht die Anzahl der Parameter für die folgenden Ebenen. Dies wird durch Unterteilung der Eingabematrix erreicht. Die Werte der einzelnen Abschnitte werden auf verschiedene Arten verarbeitet, wie z.B. Durchschnitts–Pooling oder Max–Pooling. Bei dem Durchschnitts–Pooling wird der Durchschnitt der Werte eines Abschnitts gebildet und als neuer Wert in die Ergebnismatrix eingetragen. Bei Max–Pooling wird der maximale Wert eines Abschnittes übernommen. Die vollständig verbundene Schicht bildet die vorletzte Ebene eines CNN und ist eine normale neuronale Netzstruktur. Die Matrix der vorherigen Schicht wird ausgerollt und an die Eingabe–Neuronen übergeben. Diese sind jeweils mit den Neuronen der nächsten Schicht vollständig verknüpft, bis eine Verbindung zu den Ausgabe–Neuronen besteht. Zuletzt wird die Aktivierungsfunktion z.B. Softmax aufgerufen. Softmax–Aktivierung sorgt dafür, dass alle Werte der Ausgabe–Neuronen sich zu 1 addieren und so die Wahrscheinlichkeit der jeweiligen Ausgabe repräsentieren. 8 Kapitel 2 Grundlagen Abbildung 3: Visualisierung eines CNN–Modells zur Satz Klassifizierung (Zhang und Wallace 2015) Abbildung 3 zeigt, wie Convolutional Neural Networks in der Sprachverarbeitung verwendet werden können. Die Eingabe muss dafür in Form einer Matrix vorhanden sein. Texte müssen zum Erfüllen dieses Kriteriums zunächst vorverarbeitet werden. Wie im Abschnitt 2.2 beschrieben, können Wörter auch als Vektoren repräsentiert werden. Für jedes Wort der Eingabe wird der zugehörige Vektor verwendet. Die resultierende Matrix hat die Größe n x m, wobei n der Länge des Satzes und m der Dimension des Wortvektors entspricht. Der Filterkern, der in der Faltungsebene angewendet wird, umfasst alle Dimensionen der Wortvektoren in x–Richtung. Die y–Richtung umfasst typischerweise 2 – 5 Wörter. Die nachfolgenden Schichten funktionieren wie für Pixel bereits beschrieben. Im letzten Schritt wird die Eingabe in Wahrscheinlichkeitswerten den möglichen Klassen zugewiesen. 9 Kapitel 2 Grundlagen 2.4. Recurrent Neural Network In vielen Fällen der Sprachverarbeitung ist der Kontext der Eingabe essenziell für das Erzielen des gewünschten Ergebnisses. Um diesen Kontext in einem neuronalen Netz darstellen zu können, muss eine gewisse Abhängigkeit bei den Eingabe–Neuronen gegeben sein. Bei anderen neuronalen Netzen agieren die Neuronen unabhängig voneinander. In der aktuellen Verarbeitung wird der vorherigen Eingabe sowie deren Ergebnis keine Bedeutung zuteil. Recurrent Neural Networks (RNNs) nach (Tomáš Mikolov u. a. 2010) beziehen diese Informationen der vorherigen Schritte in die folgenden Verarbeitungen mit ein um ein kontextsensitives Ergebnis zu erzielen. Um die vorherige Sequenz von Wörtern mit einzubeziehen ist die grundlegende Architektur von RNN eine Schleife. Abbildung 4: Ausgerollte Darstellung eines RNN–Modells (Olah 2015) Wie in der Abbildung 4 zu erkennen ist, wird zunächst das erste Wort der Sequenz als Eingabe an das RNN übergeben. Das Netz berechnet basierend auf der Eingabe ein Ergebnis, welches zusätzlich mit dem nächsten Wort der Sequenz erneut an das Modell gegeben wird. Dieser Prozess wiederholt sich bis das letzte Wort der Sequenz verarbeitet wurde und ein endgültiges Ergebnis entsteht. Durch dieses Verfahren ist das Ergebnis abhängig von dem vorherigen Ergebnis, welches wiederum selbst abhängig von seinem vorherigen Ergebnis ist. Dadurch wird die gesamte Sequenz beachtet. Diese Art der Struktur lässt sich auch als Weiterleitung mit Speicherfunktion betrachten. Zum Anwenden von RNN–Modellen werden die Wörter der Eingabesequenz in Vektoren umgewandelt. Dafür können verschiedene Repräsentationen verwendet werden, welche bereits im Abschnitt Word Embeddings vorgestellt wurden. Die Sequenz von Vektoren wird nacheinander von dem RNN verarbeitet. Dazu wird der Vektor der Eingabe mit dem vorherigen Ergebnis verbunden und der entstehende Vektor wird in die Aktivierungsfunktion Tangens hyperbolicus (Tanh) gereicht. Die Funktion Tanh sorgt dafür, dass die Werte in dem Vektor zwischen -1 und +1 bleiben, da ohne diese Funktion einzelne Werte eine zu starke Gewichtung bekommen und die übrigen Werte keine Auswirkung haben. Das 10 Kapitel 2 Grundlagen Ergebnis der Tanh Funktion ist die Ausgabe für den Schritt, der in der nächsten Iteration wieder als Eingabe verwendet wird. Ein Nachteil dieses Modells ist das Trainieren, da jede Eingabe von demselben Modell verarbeitet wird. So haben längere Sequenzen, bei denen das Ergebnis über den hinteren Teil der Eingabe entschieden wird, mehr Einfluss auf die Bewertung der einzelnen Neuronen als Wörter zu Beginn der Sequenz. Dadurch entsteht ein Ungleichgewicht und Sequenzen mit stark variierender Länge werden von dem Modell nur schwer bis gar nicht erlernt. Dieses Problem ist unter dem Namen vanishing gradient problem bekannt. Der Gradient, der für das Lernen verantwortlich ist, wird durch backpropagation so weit verkleinert, bis dieser keine Auswirkung mehr auf die Gewichtung der Neuronen nimmt. Durch diesen Effekt ist das Modell nicht in der Lage Neues zu erlernen. 2.5. Unterschiede RNN und CNN Auf den ersten Blick wirken RNNs und CNNs identisch, da beide den Kontext der Eingabe betrachten. Der wesentliche Unterschied ist, dass RNNs nur aus einer Schicht bestehen und das Ergebnis der vorherigen Berechnung als Eingabe für das nächste Wort betrachten. RNNs werden meistens für die Bearbeitung von Sequenzen verwendet. Bei CNNs wird die Eingabe durch mehrere Schichten verarbeitet und es werden direkt mehrere Wörter in einem Durchlauf betrachtet. Durch dieses Vorgehen wird der lokale, umliegende Kontext berücksichtigt und nicht die gesamte Eingabe. Der Hauptanwendungsbereich dieses Modells liegt in der Bildverarbeitung. In der Arbeit von (Bai, Kolter und Koltun 2018) wurde gezeigt, dass RNNs durch CNNs ersetzt werden können und diese bei Sequenzmodellierung deutlich bessere Ergebnisse erzielen als die betrachteten RNN–Modelle. 2.6. Long Short–Term Memory RNN–Modelle haben Schwierigkeiten, die Informationen der längeren Sequenzen von früheren Schritten bis hin zu den späteren zu propagieren. Um das Problem zu lösen werden Long Short–Term Memory (LSTM)–Modelle (Cummins, Gers und Schmidhuber 1999) eingesetzt, welche eine Ergänzung zu RNN darstellen (Sherstinsky 2018). Diese Modelle verwenden eine Art Schalter, mit dem reguliert werden kann, ob und welche Informationen gespeichert werden sollen. Mit diesem Vorgehen können wesentliche Informationen der Sequenz gezielt gespeichert werden. Das Modell bezieht nicht mehr die volle Sequenz zur Verarbeitung ein, wodurch das Auftreten des vanishing gradient problem reduziert wird. 11 Kapitel 2 Grundlagen Die Architektur des Modells basiert auf der Verwendung von drei Gates. Diese entscheiden was mit der aktuellen Eingabe geschehen soll. Zusätzlich bietet die Architektur einen Zustand, der als Gedächtnis verwendet wird. Wie bereits RNN–Modelle verwenden auch LSTM–Modelle zusätzlich das vorherige Ergebnis um die Ausgabe zu erzeugen. Als Eingabe wird auch eine Vektor Repräsentation der Wörter verwendet, wie sie im Abschnitt Word Embeddings erläutert wurde. Alle Gates erhalten die Wortvektoren und das Ergebnis aus der vorherigen Berechnung als Eingabe. Für jedes Wort einer Sequenz wird das LSTM–Modell aufgerufen und ab dem ersten Wort wird der Zustand und das vorherige Ergebnis in die nächste Berechnung übergeben. Das Forget Gate entscheidet, welche Informationen der vorherigen Schritte behalten werden. Das Input Gate bestimmt, welche Informationen aktuell relevant sind und im Gedächtniszustand gespeichert werden sollen. Das Output Gate berechnet das Ergebnis, welches für das nächste Wort wiederverwendet wird. Abbildung 5: Verkettung und Aufbau der einzelnen LSTM Elemente (Olah 2015) Das Forget Gate entscheidet welche Informationen beibehalten oder verworfen werden. Dazu wird eine Sigmoidfunktion auf den Eingabevektor angewendet, um die Werte des Vektors zwischen 0 und 1 abzubilden. Dabei bedeutet eine 1, dass die Informationen beibehalten und die 0 das diese verworfen werden. Bei dem Input Gate wird der Eingabevektor von zwei Aktivierungsfunktionen verarbeitet. Die Sigmoidfunktion entscheidet, welche Informationen wichtig (1) oder unwichtig (0) sind. Die Tanh Funktion reguliert die Werte, damit sich diese zwischen -1 und 1 befinden und sich besser für die spätere Verarbeitung eignen. Im Anschluss werden die Ergebnisvektoren beider Funktionen multipliziert, um einen Vektor zu erhalten. Um den neuen Gedächtniszustand zu berechnen, wird der vorherige Zustand mit dem 12 Kapitel 2 Grundlagen Ergebnis des Forget Gates multipliziert. Der daraus resultierende Vektor wird mit dem Ergebnis des Input Gates addiert, daraus ergibt sich ein neuer Zustand. Als letztes wird das Output Gate verwendet. Eine Sigmoidfunktion wird auf den Eingabevektor angewendet, der aktuelle Zustand wird an eine Tanh Funktion gereicht. Die Ergebnisse beider Funktionen werden multipliziert und wird als vorheriges Ergebnis in dem folgenden Schritt wieder verwendet. Das Output Gate berechnet also die Vektoren, die für den nächsten Schritt benötigt werden. Für ein besseres Verständnis der Elemente kann Abbildung 5 betrachtet werden. Eine Erweiterung sind Bidirektionalen Netze (Schuster und Paliwal 1997), welche auch die vorherigen Wörter für die Verarbeitung betrachten. Sowohl LSTM als auch RNN Netze können um die bidirektionale Komponente erweitert werden, um ein besseres Ergebnis zu erzielen. Dazu wird die Anzahl der verwendeten Zellen dupliziert und in umgekehrter Reihenfolge miteinander verbunden. Dadurch wird die Sequenz in beide Richtungen verarbeitet und Beziehungen — sowohl vor als auch nach dem Wort — werden beachtet. 2.7. Conditional Random Fields Conditional Random Field (CRF) sind diskriminierend und modellieren die bedingte Wahrscheinlichkeitsverteilung (Lafferty, McCallum und Pereira 2001). Eingesetzt werden diese Modelle unter anderem in der Bild- und Textverarbeitung. In der grundlegenden Funktionsweise beschreibt das Modell die Abhängigkeiten sowie Unabhängigkeiten zwischen zufälligen Variablen. Diese Variablen bilden einen Graphen, aus dem sich die Wahrscheinlichkeiten berechnen lassen mit der die jeweilige Variable zutrifft. Bei CRF wird die bedingte Wahrscheinlichkeitsverteilung betrachtet, dazu wird die Wahrscheinlichkeit der Klasse Y — unter der Annahme, dass die Eingabe X gilt — gesucht (Abbildung 6). Für ein besseres Verständnis wird im folgendem ein Beispiel aus dem Bereich der natürlichen Sprachverarbeitung betrachtet. 13 Kapitel 2 Grundlagen Abbildung 6: Beispielhafte Darstellung eines CRF Die Eingabedaten der CRFs sind sequentiell und der frühere Kontext wird berücksichtigt um eine Vorhersage treffen zu können. Um dieses Verhalten modellieren zu können, werden Feature–Funktionen mit vier Eingabewerten verwendet. Diese sind: • die Wortvektoren für jedes Wort der Eingabe • die Position des Wortes, für die der Bezeichner bestimmt werden soll • die korrekte Bezeichnung des vorherigen Wortes • die korrekte Bezeichnung des gesuchten Wortes Im Anschluss wird eine Merkmalsfunktion definiert, die das gewünschte Verhalten abbildet. Zum Trainieren werden die Gewichtungen zufällig bestimmt und mit dem Gradientenabstiegsverfahren optimiert bis die Parameterwerte konvergieren. Dieses Verfahren ist der logistischen Regression ähnlich, da beide die bedingte Wahrscheinlichkeitsverteilung verwenden. Der Unterschied besteht darin, dass durch die Erweiterung von Feature– Funktionen eine sequenzielle Eingabe möglich ist. 2.8. Fuzzy–Suche Die zweiwertige Logik ermöglicht das Modellieren von Verhalten und umfasst die Wahrheitswerte “wahr” und “falsch”. Fuzzylogik erweitert die Menge der Wahrheitswerte (z.B. “ein bisschen”, “wenig” und “sehr”) um eine unscharfe Beschreibung zu ermöglichen (Zadeh 1965). Abgebildet auf reelle Zahlen bedeutet das die Werte in dem Intervall [0,1]. Fuzzy (Unschärfe) ist eine Form der Ungenauigkeit bei der Abbildung eines Sachverhalts. 14 Kapitel 2 Grundlagen Als Beispiel wird ein Zimmer betrachtet welches zwei Zustände haben kann: warm und kalt. Die zweiwertige Logik legt einen Grenzwert fest, ab wann der Übergang zwischen kalt zu warm ist z.B. 20 Grad Celsius. Bei Fuzzylogik wird eine weiche Grenze zwischen den Zuständen definiert und Werte wie 18,9 Grad Celsius werden beschrieben durch z.B. ein bisschen warm oder weniger kalt. Bei der unscharfen Suche auf Zeichenketten wird nicht auf die exakte Zeichenfolge, sondern ähnliche Zeichenketten geprüft. Die Levenshtein–Distanz ist ein Verfahren zur Messung der Differenz zwischen zwei Sequenzen (Levenshtein 1966). Die gesamte Distanz setzt sich dabei aus der benötigten Anzahl von Einfüge-, Lösch- und Ersetzung-Operationen zusammen, die benötigt werden, um ein Wort in das andere zu ändern. Betrachtet man die Wörter “Tier” und “Tor” kann der Buchstabe “i” durch ein “o” ersetzt werden und das “e” muss gelöscht werden. Somit beträgt die Levenshtein–Distanz 2. Die Distanz repräsentiert wie hoch eine Übereinstimmung dieser Wörter ist. Eine geringe Levenshtein– Distanz bedeutet dabei hohe Übereinstimmung. 15 Kapitel 3 Konzeption 3. Konzeption Ziel dieser Arbeit ist es, aus natürlicher unstrukturierter menschlicher Eingabe eine strukturierte Ausgabe zu erzeugen, die von einem Computer weiterverarbeitet werden kann. Hier könnte es sich beispielsweise um Empfehlungen von Produkten auf eine Suchanfrage handeln. Dabei ist es wichtig, dass die natürliche Eingabe des Menschen korrekt verstanden und ausgewertet wird. 3.1. Annahmen Im folgendem werden verschiedene Annahmen vorgestellt, die im Rahmen dieser Arbeit getroffen wurden. 1. Annahme: Festlegen der Sprache Sprachmodelle die mit mehreren Sprachen interagieren sollen sind wesentlich komplexer. Die Charakteristiken einer Sprache variieren sehr stark, weshalb ein Modell, welches auf die englische Sprache trainiert wurde, nicht direkt mit deutscher Eingabe bedient werden kann. Um dieser Problematik nicht zu begegnen wird nur die deutsche Sprache unterstützt. Für eine Lösung muss das Modell selbst in der Lage sein, auf die verschiedenen Sprachen zu reagieren oder es wird ein Modell verwendet, welches die eingegebene Sprache ermittelt und abhängig davon das passende Sprachmodell bereitstellt. 2. Annahme: Eingrenzung der Domäne Über virtuelle Marktplätze werden sämtliche Produkte gehandelt, die verschiedensten Attribute besitzen. Im Rahmen dieser Arbeit wird die Domäne beschränkt, mit der Möglichkeit diese nach Belieben zu erweitern. Technologische Artikel sind beliebte Produkte welche häufig über online Marktplätzen gehandelt werden, weshalb die Domäne zu Beginn auf diese Produkte begrenzt wird. Um diese noch weiter einzuschränken wurde sich an der Produktgruppe der Smartphones orientiert. Basierend auf dieser Gruppe wurden 6 Attribute — die häufig verwendet werden — gewählt, um Smartphones zu beschreiben. Diese sind in der Regel: Produkt, Hersteller, Preis, Farbe, Speicher und Kamera. Das in dieser Arbeit beschriebene Verfahren kann verwendet werden, um weitere Attribute bzw. Produktgruppen zu ergänzen. 3. Annahme: Attribute sind zusammenstehend Ein bekanntes Problem bei Sprachmodellen ist, dass diese Schwierigkeiten haben Attribute zu bestimmen, bei Wörtern die nicht zusammenstehen. Aus diesem Grund wird angenommen, dass mehrere Wörter, die zu einem Attribut gehören, zusammen 16 Kapitel 3 Konzeption stehen und nicht von anderen Wörtern unterbrochen werden. Ein Gegenbeispiel dafür ist: “Ich suche ein iPhone, am besten das 10.”, da hier das gesuchte Produkt “iPhone 10” nicht zusammenhängt. Wird diese Annahme nicht getroffen, könnte die Eingabe durch eine Vorverarbeitung umstrukturiert werden, sodass die Attribute wieder zusammenstehen. 4. Annahme: Eingaben sind nur Handelsanfragen Eingaben zwischen normaler Konversation und Handelsanfragen zu unterscheiden ist ein zusätzliches Problem, welches nicht im Fokus dieser Arbeit steht. Es wird angenommen, dass jede Eingabe mindestens das Attribut “Produkt” enthält und somit eine Handelsanfrage darstellt — dadurch kann sich auf das Klassifizieren der Attribute fokussiert werden. Alternativ müsste ein zusätzliches Modell eingesetzt werden, welches auf die Differenzierung zwischen normaler Konversation und Handelsanfragen trainiert ist. 3.2. Anforderungen Virtuelle Marktplätze werden von Jahr zu Jahr bedeutsamer. Auf elektronischen Marktplätzen finden viele Käufer–Verkäufer Situationen statt. Anwender erstellen ein digitales Angebot, das von potenziellen Käufern gefunden werden möchte. Dieser Ablauf beschreibt grob einen Anwendungsfall, welcher durch die natürliche Sprachverarbeitung unterstützt werden soll. Die genauen Abläufe könnten wie folgt aussehen. 1. Szenario: Angebotssuche Ein Anwender kann eine Anfrage in Form einer Texteingabe an das System stellen. Die Eingabe wird von einer Komponente verarbeitet, in der die wesentlichen Informationen extrahiert werden. Das Ergebnis der Verarbeitung wird mit der Eingabe in einer Datenbank abgespeichert, damit zukünftige Handelsanfragen diese Anfrage finden können. Ebenfalls wird das Ergebnis an eine andere Komponente übergeben, welche basierend auf den extrahierten Informationen ein passendes Gegenangebot zurückgibt. Dieses wird im Anschluss dem Anwender mit den erkannten Attributen aus seiner Eingabe präsentiert. 2. Szenario: Gruppensuche In einem spezielleren Szenario sucht ein Anwender nach Produkten von einem bestimmten Hersteller, ohne das gesuchte Produkt genau zu spezifizieren. Die Attribute aus der Eingabe werden wie in dem vorherigen Szenario von einer Komponente bestimmt. Die zweite Komponente, welche das passende Gegenangebot ermittelt reagiert auf die Suche nach einer Menge von Produkten. Basierend auf die übrigen 17 Kapitel 3 Konzeption Attribute der Eingabe wird dem Anwender ein passendes Gegenangebot angezeigt. Diese Szenarien verdeutlichen den Ablauf einer Suche über einen virtuellen Marktplatz. 3. Szenario: Inserat Erstellung In dem letzten Szenario erstellt ein Anwender ein Inserat. Die Eingabe wird verarbeitet und das eigene Ergebnis dem Anwender präsentiert. Dieser entscheidet dann, ob die erkannten Attribute korrekt sind. Ist dies nicht der Fall, so soll dem Anwender die Möglichkeit geboten werden die Attribute in seiner Eingabe manuell zu bestimmen. Im Anschluss werden die Attribute sowie die Eingabe in einer Datenbank gespeichert. Der Nutzer erhält kein Gegenangebot. Anwender sollen mit dem System interagieren können. Deshalb wird die direkte Eingabe der Nutzer verwendet, was als natürliche Eingabe bezeichnet wird. Als Beispiel dafür wird eine virtuelle Verhandlung über einen Chat betrachtet in dem Produkte gehandelt werden. Jeder Mensch verfügt über eine eigene Art wie er sich in einem Chatroom ausdrückt, weshalb keine korrekte Rechtschreibung angenommen wird. Auch muss das System in der Lage sein mit Abkürzungen sowie Emojis umzugehen. Aus den genannten Anforderungen wird deutlich, welche Funktionen der Prototyp bereitstellen muss. Für die ersten beiden Anwendungsfälle wird eine Oberfläche erwartet, in der ein Anwender Eingaben tätigen kann und die Möglichkeit hat eine Antwort zu erhalten. Chatsysteme werden häufig bei Consumer–to–Consumer Transaktionen in virtuellen Marktplätzen eingesetzt, weshalb der Aufbau dem eines Chatrooms ähnlich sein soll. Der Anwender kann seine Anfrage dann in Form einer Nachricht in diesem Chat an das System senden. Wie es in einen Chatroom üblich ist, wird dem Anwender seine eigene Nachricht angezeigt und nach der Verarbeitung auch die Antwort des Systems. Die Verarbeitung darf nicht zu lange dauern, da sonst das Interesse der Anwender verloren geht. In der Oberfläche soll es dem Benutzer möglich sein, mehrere Anfragen nacheinander an das System zu senden mit der Möglichkeit weiterhin die vorherigen Ergebnisse angezeigt zu bekommen. Die Nachrichten des Anwenders und Systems sollten farblich differenzierbar und links bzw. rechtsbündig ausgerichtet sein. Anwender sind an diesen Aufbau von anderen Nachrichtensystem vertraut, wodurch der Einstieg in die Bedienung erleichtert wird. Für das letzte Szenario wird eine andere, simplere Oberfläche verwendet. Es wird keine Antwort von dem System erwartet, weshalb nur die Eingabe des Anwenders im Vordergrund steht. Durch eine Texteingabe wird die Anfrage des Anwenders entgegengenommen und ausgewertet. Der Prototyp stellt im Anschluss die Eingabe mit den gefundenen Attributen sowie der Möglichkeit das Ergebnis zu bestätigen oder abzulehnen dem Anwender dar. Der Anwendungsfall bietet dem Anwender die Möglichkeit die eigene Anfrage manuell 18 Kapitel 3 Konzeption mit Attributen zu versehen. Um diese Funktionalität anzubieten wird für jedes Attribut eine Schaltfläche verwendet, mit der die ausgewählte Sequenz dem jeweiligen Attribut zugewiesen werden kann. Formalisiert lassen sich aus den Beschreibungen der Szenarien sowie dem Ziel der Arbeit die folgenden Anforderungen erfassen. Die Anforderungen werden mit den Buchstaben FA und NA für funktionale bzw. nicht funktionale Anforderungen gekennzeichnet. FA1: Das System muss Eingaben in Form von Handelsanfragen von Anwendern ermöglichen. FA2: Das System muss basierend auf die Handelsanfragen passend antworten. FA3: Das System muss den Anwendern die Möglichkeit bieten, die Eingabe manuell mit Attributen zu versehen. FA4: Das System muss Gruppensuchen ermöglichen und mit einem passenden Angebot reagieren. FA5: Das System muss alle Anfragen persistieren. FA6: Das System muss mit bestimmte Anwenderfehler wie z.B. Rechtschreibfehler umgehen können. NA1: Aufrufe der Anwender müssen schnell (<3 s) verarbeitet und beantwortet werden. NA2: Anwendern wird das Gefühl vermittelt, von dem System verstanden zu werden. NA3: Die Antworten des Systems sind begründet und können von den Anwendern nachvollzogen werden. 19 Kapitel 3 Konzeption 3.3. Komponenten Abbildung 7: Aufbau der Anwendung mit den Komponenten Um die vorgestellten Szenarien aus Abschnitt 3.2 zu erfüllen werden fünf Komponenten benötigt, um einen Prototyp zu erstellen wie in Abbildung 7 dargestellt. In den folgenden Abschnitten werden die Komponenten genauer vorgestellt. 3.3.1. Benutzerschnittstelle Der Prototyp kann mit verschiedenen Oberflächen realisiert werden z.B. Webanwendung, Desktop–Anwendung, Android–App oder Kommandozeile. Was verwendet wird ist abhängig von dem Ziel, dass der Prototyp verfolgt. Ein Kriterium ist die einfache Zugänglichkeit, sodass viele Anwender den Prototypen problemlos benutzen können. Am einfachsten zugänglich ist eine Webanwendung, da diese im Webbrowser aufgerufen werden kann. Die anderen Möglichkeiten benötigen eine Installation oder zumindest eine ausführbare Projektdatei auf dem Endgerät. Die wesentliche Aufgabe des Prototyps ist es, Text entgegenzunehmen und zu verarbeiten (FA1 ). Aus diesem Grund ist ein weiteres Kriterium die Unterstützung der einfachen Texteingabe von der Oberfläche. In Smartphone Apps wird Text meist nur über die Bildschirmtastatur eingegeben was umständlicher ist als z.B. an einen Computer. In einer Kommandozeilen–Anwendung ist der Umgang mit langen Texteingaben ebenfalls nicht optimal da per Mausklick nicht an die gewünschte Stelle gesprungen wird. Das nächste Kriterium ist eine leicht verständliche Oberfläche, die intuitiv bedient werden kann. Die Oberfläche der Kommandozeile ist nicht benutzerfreundlich, weshalb viele Anwender davor zurückschrecken würden eine solche Anwendung zu benutzen. Bei Desktop– Anwendungen neigen Entwickler dazu ein komplett eigenes und kein einheitliches Design zu verwenden. Die Folge davon ist, dass Anwender sich erst an die Bedienung gewöhnen 20 Kapitel 3 Konzeption müssen. Bei Webanwendungen und Smartphone Apps wird mehr Wert auf ein einheitliches Design gelegt und sich an bereits existierende Anwendungen orientiert. Zum Erstellen dieser Arbeit wird eine Webanwendung erstellt. Webanwendungen bieten den Anwendern eine vertraute Oberfläche, die einfach zu bedienen ist. Webseiten können von den meisten Endgeräten aus aufgerufen werden, was die Verwendung von Smartphones miteinschließt. Das häufige Eingeben von Text wird durch die Verwendung einer Computertastatur erleichtert. Anwender müssen keinen Client updaten, um die neueste Version des Prototyps verwenden zu können. Es wird ein Server benötigt, auf dem die Webanwendung ausgeführt wird damit diese verfügbar ist. 3.3.2. Hybrid named-entity recognition Die natürliche Eingabe des Anwenders wird an eine Komponente übergeben, welche die Attribute bestimmt um später eine passende Antwort generieren zu können. Bei der erhaltenen Eingabe kann von keiner korrekten Rechtschreibung ausgegangen werden (FA6 ). Die Attribute zeichnen sich durch besondere Charaktereigenschaften aus. So besteht der Preis meist aus einem beschreibenden Wort z.B. “mindestens” oder “maximal” gefolgt von einer Zahl (z.B. 300) mit einer abschließenden Einheit (z.B. “e”, “euro”). Der strukturelle Aufbau der Attribute SSpeicheründ ”Kameraı̈st identisch, nur die Einheit ist eine Andere (“Gigabyte (GB)” bzw. “Megapixel (MP)”). Attribute wie Hersteller und Farbe bestehen in den allermeisten Fällen nur aus einzelnen Wörtern oder einer kleinen Wortkette z.B. “Apple” und “helles grau”. Das Produkt hingegen besteht nicht nur aus Wörtern oder Wortketten, sondern es beinhaltet häufig genaue Artikelbezeichnungen, die in Form von Buchstaben konkateniert mit Zahlen dargestellt werden (z.B. “Galaxy S10”). Auch können Produkte den Namen des Herstellers beinhalten, was das Differenzieren beider Attribute erschwert. Abbildung 8 zeigt eine beispielhafte Eingabe in der die Attribute farblich hervorgehoben wurden: Hersteller (grau), Produkt (blau), Farbe (schwarz), Kamera (türkis), Speicher (gelb) und Preis (rot). Abbildung 8: Visuelle Unterstützung einer beispielhaften Eingabe Aufgrund der unterschiedlichen Eigenschaften der Attribute, wird eine Menge von Algorithmen verwendet, die gemeinsam eine Pipeline bilden. Diese erzeugt aus einer unstruktu- 21 Kapitel 3 Konzeption rierten Eingabe eine mit korrekten Bezeichnungen versehene Ausgabe. Die Pipeline setzt sich dabei aus unterschiedlichen Algorithmen zusammen, die nacheinander angewendet werden, um so ein optimales Ergebnis zu erzielen. Der Fokus dieser Arbeit beschränkt sich auf das Erkennen des Attributes “Produkt”, da dieses bei allen Handelsanfragen enthalten sein muss. Die übrigen Attribute (Hersteller, Preis, Farbe, Speicher und Kamera) sollen zeigen, dass eine Erweiterung auf mehrere Attribute möglich ist. Die einzelnen Schritte der Pipeline sind aufsteigend gewichtet. Das bedeutet, dass die späteren Schritte Teilergebnisse der vorherigen überschreiben, weshalb die Stärken verschiedener Algorithmen kombiniert werden können. Der Aufbau der Pipeline ermöglicht ein einfaches Hinzufügen, Verschieben oder Entfernen von Algorithmen, welches Anpassungen an spezielle Anforderungen ermöglicht. Abbildung 9: Aufbau der Pipeline Abbildung 9 zeigt den konzeptionellen Aufbau der Pipeline. Wie zu erkennen besteht die Pipeline aus mehreren Bauteilen, die sich auf verschiedene charakteristische Eigenschaften der Attribute fokussieren. Mit diesem Aufbau werden die Stärken der einzelnen Schritte kombiniert, wodurch die Erkennung optimiert werden kann. Den ersten Schritt bildet ein neuronales Netz, dessen Hauptaufgabe das Erkennen des Attributes “Produkt” ist. Es gibt sehr viele verschiedene Produktbezeichnungen mit variierender Länge und Anzahl der Wörter, sodass neuronale Netze für diese Aufgabe benötigt werden. Die Netze sind durch Trainieren in der Lage, bestimmte Muster in den verschiedenen Eingaben zu erkennen, die zuvor von Menschen nicht erkannt wurden. Durch die Vielzahl von möglichen Produkten und dem Ziel alle möglichen Produkte zu erkennen wurde ein einfaches Vergleichsverfahren an dieser Stelle ausgeschlossen. Das System würde nur die zuvor definierten Produkte erkennen und neue Produkte müssten dauerhaft manuell hinzugefügt werden was nicht zielführend ist. Das neuronale Netz muss in der Lage sein, Wörter bzw. ganze Sätze als Eingabe entgegenzunehmen und eine Wortsequenz — die dem passenden Attribut zugewiesen wird — als Ausgabe zu erzeugen. Der nächste Schritt dient zum Erkennen von Attributen, die durch einen regelbasierten Ansatz erkannt werden können. Einige Attribute wie Preis, Speicher und Kamera folgen 22 Kapitel 3 Konzeption immer einem ähnlichen Muster, das durch reguläre Ausdrücke beschrieben werden kann. Die Werte dieser Attribute sind deutlich weniger variabel als die der übrigen Attribute. Zudem ist es unwahrscheinlich, dass in naher Zukunft neue Werte zu den Attributen hinzugefügt werden und diese somit sehr starr sind. Ein alternatives Vorgehen zur Erkennung dieser Attribute ist mit neuronalen Netzen. Das Trainieren eines neuronalen Netzes zum Erkennen dieser Attribute ist wesentlich aufwändiger und fehleranfälliger. Aus diesem Grund wurde sich gegen dieses Vorgehen und für die regulären Ausdrücke entschieden. In dem letzten Schritt wird ein einfacher Vergleich der Eingabe mit zuvor definierten Attribut Ausprägungen vorgenommen. Dieser Schritt wird für Attribute verwendet, welche nicht durch reguläre Ausdrücke beschrieben werden können. Die Eingabe wird mit zuvor definierten Werten, wie z.B. “iPhone” abgeglichen, um sicher zu stellen das zumindest diese Werte korrekt klassifiziert werden. Anders als der erste Schritt, in dem sämtliche Produkte identifiziert werden sollen, ist das Ziel des letzten Schrittes nur wenige, bestimmte Attribute zu erkennen. Aufgrund des anderen Zieles wird das folgende Verfahren in Betracht gezogen. Die gegebene Eingabe wird Wort für Wort mit den zuvor definierten Ausprägungen abgeglichen und bei einer Übereinstimmung ist das Attribut in der Eingabe enthalten. Der Nachteil dieses Ansatzes ist, dass die definierten Werte eine exakte Übereinstimmung in der Eingabe voraussetzen, da diese sonst nicht gefunden werden. Einen besseren Ansatz verfolgt die Fuzzylogik (Zadeh 1965). Die Wörter benötigen keine exakte Übereinstimmung da auch ungenaue Ergebnisse zugelassen werden. Der Nachteil der exakten Übereinstimmung ist dadurch nicht mehr gegeben, weshalb für den letzten Schritt die Fuzzylogik betrachtet wird (Mansouri, Affendey und Mamat 2008). 3.3.3. Tagging Falsche Ergebnisse sind bei der Verwendung von neuronalen Netzen nicht auszuschließen, weshalb diese berücksichtigt werden müssen. Um zu verhindern, das falschen Daten persistiert werden wird eine Komponente benötigt, mit der das Ergebnis der Pipeline manuell nachgebessert werden kann (FA3 ). Die Komponente unterteilt sich dabei in zwei Unterfunktionen, zum einen das manuelle Setzen von Attributen und zum anderen das Bestimmen der Rolle (Käufer oder Verkäufer) aus einer Anfrage. Durch das richtige Setzen von Attributen können diese Anfragen korrekt in der Auswertungs–Komponente verwendet werden. Aus falsch persistiert Daten können unpassende Ergebnisse entstehen die dem Anwender auf seine Anfrage als Antwort vorgeschlagen werden. Selbiges gilt für falsch zugewiesene Rollen, auf eine Kaufanfrage könnte mit einer weiteren Kaufanfrage von dem System geantwortet werden. 23 Kapitel 3 Konzeption Eine solche Komponente, in der Anwender die Daten selbst annotieren können, kann verwendet werden, um Daten zu sammeln. Es kann auf keine Datengrundlage aufgebaut werden, die die Anforderungen erfüllen, weshalb dieses Vorgehen geeignet ist. Die gesammelten Daten können verwendet werden, um das benötigte neuronale Netz zu trainieren und so die Performance zu verbessern. Durch die manuelle Annotation der Daten können diese, ohne zusätzliche Bearbeitungsschritte verwendet werden, um den Ablauf zu vereinfachen. Mit dieser Struktur kann ein iterativer Ansatz zum Verbessern der Modelle erstellt werden. Es werden Daten gesammelt, die durch den Anwender korrekt annotiert sind. Ab einer gewissen Menge von neuen Daten wird der gesamte Datenbestand verwendet, um ein neues Modell zu trainieren welches das vorherige ersetzt. Anhand der Daten aus den späteren Iterationen lassen sich die Modelle evaluieren und können so verglichen werden. Zudem kann die Skalierung der Modelle mit mehr Daten gemessen werden, indem diese erneut trainiert und auf eine Verbesserung der Genauigkeit geprüft werden. Durch eine Automatisierung der Iteration wäre das System in der Lage sich kontinuierlich selbst zu verbessern und so immer mehr Eingaben korrekt zu erkennen. 3.3.4. Persistierung Der Handel zwischen Kunden ist ein wesentlicher Bestandteil eines virtuellen Marktplatzes. Um diesen zu ermöglichen, muss das System in der Lage sein, die Angebote und ggf. Nachfragen zu persistieren, damit diese von Kunden gefunden werden können (FA5 ). Zudem ist ein virtueller Marktplatz erst dann für Kunden attraktiv, wenn dieser über eine Vielzahl von Produkten im Sortiment verfügt. Zum Verwalten von großen Mengen an Daten wird meist auf Datenbanken zurückgegriffen. In der Datenbank werden die Benutzereingaben sowie die erkannten Attribute gespeichert. Wurde das Ergebnis der Verarbeitung von dem Anwender manuell verbessert wird das ebenfalls persistiert, um einen korrekt klassifizierten Datensatz zu erhalten. Für diese Arbeit wurde zwischen Graphdatenbanken und relationale Datenbank nach (Vicknair u. a. 2010) entschieden. Aus den vorgestellten Anwendungsfällen in Abschnitt 3.2 ist ersichtlich, dass die Daten keine Indizierung benötigen und überwiegend nur aus Text bestehen. Bezogen auf die Performance, sind Relationale Datenbanken den Graphdatenbanken in diesem Szenario unterlegen. Der zweite Anwendungsfall bezieht sich auf eine Gruppensuche, die viele Beziehungen der einzelnen Daten voraussetzt (z.B. Knowledge Graph). Vordefinierte Produkte können Beziehungen zu Herstellern haben und es können Obergruppen für bestimmte Kategorien angelegt werden. Die Komponente welche passende Gegenangebote ermittelt kann auf diese Daten zurückgreifen, um so bessere Ergebnisse zu erzielen. Dieses ist ein weiterer Vorteil der Graphdatenbanken nach (Vicknair u. a. 2010) weshalb diese im Rahmen dieser 24 Kapitel 3 Konzeption Arbeit verwendet werden. 3.3.5. Auswertung Die letzte Komponente erstellt die Antwort zu einer gegebenen Eingabe. Damit vorherigen Anfragen ausgewertet werden können, benötigt die Komponente Zugriff auf die Datenbank. Dabei muss die Rolle des Benutzers (Käufer oder Verkäufer) beachtet werden damit nur Anfragen der anderen Rolle ausgewertet werden. In dem speziellen Szenario aus Abschnitt 3.2 muss die Komponente erkennen, dass eine Gruppensuche erwartet wird (FA4 ). Aus den gefundenen Mengen der Ergebnisse muss anhand einer Strategie ein optimales Gegenangebot gefunden werden, welches zurückgegeben wird (FA2 ). Eine geeignete Strategie sucht das Angebot mit den meisten übereinstimmenden Attributen aus der Anfrage. Das Attribut “Produkt” muss mindestens übereinstimmen da es sich sonst um verschiedene Produkte handelt. Anhand der Rolle kann eine Regel für den Preis erstellt werden, als Verkäufer wird der höchste Preis bevorzugt, als Käufer der niedrigste. Mit dieser Strategie kann ein Matchmaking erstellt werden welches immer das passende Gegenangebot für eine Anfrage findet. Wenn kein Angebot gefunden werden konnte, sollte dieses ebenfalls wiedergegeben werden. Die erkannten Attribute aus der Eingabe sind auch in der zurückgegeben Antwort enthalten, um diese dem Anwender zu zeigen (NA3 ). 3.4. Prozesse Aus den in Abschnitt 3.2 vorgestellten Anwendungsfällen sind die im Abschnitt 3.3 erläuterten Komponente entstanden, welche alle Anforderungen (FA1 - FA6 ) erfüllen. Die Anwendungsfälle beschreiben bereits einen groben Ablauf in welcher Reihenfolge die Komponente aufgerufen werden. Im folgendem wird anhand der Szenarien deutlich, wie die Kommunikation zwischen den Komponenten dargestellt wird und welche Funktionen diese bereitstellen müssen. 1. / 2. Szenario: Die Szenarien 1. und 2. unterscheiden sich nur in der Art der Suche. In dem einen Szenario wird ein explizites Produkt gesucht und in dem anderen wird nach einem Produkt aus einer Produktgruppe gesucht. Der Ablauf der Szenarien wird durch diesen Unterschied nicht beeinflusst, weshalb beide Anwendungsfälle gemeinsam betrachtet werden. Eingeleitet wird die Suche durch das Eingeben einer Anfrage durch den Anwender. Diese wird an die Hybrid Named Entity Recognition (NER)–Komponente übergeben, welche eine Funktion bereitstellen muss, die eine Anfrage entgegennimmt. Die Eingabe sowie das Ergebnis der Verarbeitung wird an die Datenverwaltung zum Persistieren übergeben. Die 25 Kapitel 3 Konzeption Daten aus diesen Szenarien müssen mit einem zusätzlichen Vermerk gespeichert werden, da das Ergebnis nicht manuell verifiziert wurde und möglicherweise fehlerhaft sein kann. Im Anschluss wird das Ergebnis an die Auswertungs–Komponente übergeben. Dort wird bestimmt ob es sich um eine Gruppen- oder Produktsuche handelt und eine passende Anfrage an die Datenbank gestellt. Die Datenverwaltung benötigt zwei Funktionen. Die erste Funktion nimmt das gesuchte Produkt und die Rolle der Anfrage entgegen und gibt die gefundenen Anfragen mit enthaltenen Attributen zurück. Der zweiten Funktion wird eine Produktgruppe anstelle eines bestimmten Produktes mit der Rolle übergeben, die Rückgabe bleibt identisch. Die Ergebnisse der Datenbankanfrage werden ausgewertet und ein mögliches Gegenangebot mit den erkannten Attributen der eigenen Eingabe an die Oberfläche zurück übergeben. Die Oberfläche wird um die Antwort erweitert und die Suchanfrage ist beendet. 3. Szenario: Die Erstellung eines Inserats wird durch eine Benutzereingabe eingeleitet. Die Hybrid NER–Komponente muss eine Schnittstelle bereitstellen, die diese Eingabe entgegennimmt. Das Ergebnis wird zurück an die Oberfläche übergeben, um es dem Anwender zu präsentieren. Basierend auf der Rückmeldung wird die Eingabe sofort gespeichert oder einer manuellen Bearbeitung unterzogen. Der Tagging–Komponente wird die Eingabe übergeben und bietet dem Anwender in einer Oberfläche die Möglichkeit die Attribute der Eingabe manuell zu bestimmen. Die Datenverwaltung benötigt eine Funktion, an die die Eingabe, sowie das erkannte bzw. manuell bestimmte Ergebnis übergeben werden kann. Das Inserat wurde erstellt und der Ablauf ist abgeschlossen. Die Auswertungs– Komponente wird in diesem Szenario nicht benötigt, da der Anwender nach dem Erstellen eines Inserats keine Antwort des Systems erhält. 3.5. Bewertungskriterien Es werden verschiedene Algorithmen verwendet, um alle Attribute aus einer Eingabe zu erkennen. Damit die Algorithmen verglichen werden können, werden einheitliche Metriken verwendet. Die meisten neuronalen Netze werden anhand der Werte: Accuracy, Precision, Recall und F1–Score bemessen (Faruqui und Padó 2010). Diese Werte werden aus den Feldern einer Confusion Matrix berechnet. Damit die berechneten Ergebnisse aus der Evaluation auch mit anderen Arbeiten vergleichbar sind, werden dieselben Metriken aufgestellt. Für eine ausführliche Evaluation wird eine zusätzliche Bewertung nach (Jiang, Banchs und Li 2016) durchgeführt. Die Attribute können dabei in folgende Klassen eingeordnet werden: 26 Kapitel 3 Konzeption • Erkannt, richtige Klasse • Erkannt, falsche Klasse • Zu viel/wenig erkannt, richtige Klasse • Zu viel/wenig erkannt, falsche Klasse • Falsch erkanntes Wort • Attribut nicht erkannt Durch eine genauere Aufteilung können so die Algorithmen gezielter untersucht werden, um eine mögliche spätere Nachverarbeitung zu vereinfachen. In dem Bereich der natürlichen Sprachverarbeitung gibt es viele Modelle die Named Entity Tagging (NET) unterstützen. Aus diesem Grund soll der Prototyp die Metriken automatisch generieren. Dadurch wären die Modelle einheitlich gestaltet und sind somit besser nachvollziehbar. So kann die Performance der Pipeline jederzeit nachvollzogen werden. Zeitmessungen wurden auf einem Laptop (i7-4710MQ mit 2,50 GHz und 16 GB RAM) mit den vorhandenen Testdaten ausgeführt. Dabei wurden externe Einflussfaktoren so weit wie möglich ausgeschlossen (keine weiteren laufenden Programme, keine Internetverbindung) und die Testdurchläufe wurden zehnmal wiederholt, um ein möglichst genaues Ergebnis zu erzielen. 27 Kapitel 4 Umsetzung 4. Umsetzung Der folgende Abschnitt beschreibt das Sammeln der Daten, die in dieser Arbeit verwendet wurden. Im Anschluss wird die Implementierung der einzelnen Komponenten, die zusammen eine Pipeline bilden, vorgestellt. Abschließend wird auf die Oberfläche des Prototyps eingegangen. 4.1. Vorgehen Die Umsetzung erfolgt in zwei Iterationsschritten, die auf Abbildung 10 dargestellt werden. In der ersten Iteration wird eine gewisse Menge von Daten akquiriert, auf die die verschiedenen Modelle trainiert und evaluiert werden, um diese vergleichen zu können. Anhand dieser Ergebnisse werden die Modelle gewichtet und zusammen kombiniert, um die Pipeline zu erstellen. Im Anschluss wird die erste Version des Prototyps realisiert, der das Erstellen von Inserats unterstützt. Abbildung 10: Ablaufdiagramm der Umsetzung dieser Arbeit In der zweiten Iteration werden mehr Daten mithilfe des Prototyps gesammelt. Die bereits trainierten Modelle werden anhand dieser Daten evaluiert, um die Genauigkeit der Modelle besser zu bestimmen. Die verwendeten Modelle werden mit dem gesamten Datensatz erneut trainiert und die vorherigen Ergebnisse mit den neuen verglichen. Die neuen Modelle werden mit den vorhandenen in der Pipeline ausgetauscht, um das Ergebnis des Prototyps zu verbessern. 28 Kapitel 4 Umsetzung 4.2. Datenakquise Ein wichtiger Bestandteil, um mit maschinellen Lernen Probleme lösen zu können sind relevante Daten. Mit einer großen Menge von Daten können die verwendeten Modelle bessere Ergebnisse erzielen, da Abweichungen weniger Gewichtung haben. Zudem sind die Daten vielfältiger und können somit mehrere verschiedene Situationen abdecken. Zu Beginn der Arbeit waren keine Daten vorhanden weshalb die Datenakquise ein wesentlicher Bestandteil darstellt. Die ersten Daten wurden in einer Umfrage erhoben. Die Arbeit beschränkt sich auf das Erkennen des Attributes “Produkt” weshalb die Teilnehmer Sätze bilden sollten, in denen nach einem Produkt gesucht wird. Das enthaltene Produkt sollte zudem in einem zusätzlichen Feld eingetragen werden, um so die spätere Vorverarbeitung der Daten zu vereinfachen. In der ersten Iteration wurden so 70 Datensätze erhoben, die in dem nächsten Schritt vorverarbeitet wurden. Die Algorithmen sollen nur Produkte erkennen, welche in dem gegebenen Satz enthalten sind. So wurden Sätze wie: “Am Wochenende lade ich wieder zu einer Grillparty ein, ich suche noch jemand der Fleisch mitbringen kann” mit dem angegebenen Produkt “Grillfleisch” geändert, sodass das gesuchte Wort exakt in dem Satz enthalten ist, in diesem Fall: “Fleisch”. Basierend auf diesen Daten wurden die Algorithmen in der ersten Iteration trainiert. In dem zweiten Iterationsschritt konnte auf einen lauffähigen Prototyp aufgebaut werden, um so das Sammeln der Daten zu unterstützen. Die Probanden wurden gebeten, sich auf die Produktgruppe der Smartphones zu fokussieren damit die vorgegebenen möglichen Attribute in der Eingabe enthalten sein können. In einer Eingabemaske wird das Angebot bzw. die Nachfrage eingegeben und auf der nächsten Seite wird das Ergebnis des Algorithmus dargestellt. Dabei wurde sich auf 6 mögliche Attribute beschränkt: Produkt, Hersteller, Preis, Farbe, Speicher und Kamera. Die Probanden sollten entscheiden, ob ihre Eingabe richtig erkannt wurde, oder ob Attribute falsch gesetzt wurden bzw. komplett fehlen. Im letzten Fall sollten die Anwender selbst die Attribute markieren, um die Daten für eine spätere Verarbeitung vorzubereiten. Der Vorteil in dieser Methode liegt darin, dass deutlich mehr Datensätze direkt verwendet werden können und keine Vorverarbeitung der Daten wie in der ersten Iteration nötig ist. Die Oberfläche erlaubt nur das Markieren von zusammenstehenden Wörtern, die tatsächlich in dem Satz enthalten sind und so das Attribut bilden. Beides sind Annahmen die im Rahmen dieser Arbeit getroffen wurden und im Abschnitt 3.1 genauer beschrieben werden. Dieses Vorgehen ermöglicht das Sammeln eines dynamisch wachsenden Datensatzes, welcher zum Evaluieren und Optimieren der Algorithmen verwendet wird. 29 Kapitel 4 Umsetzung 4.3. Hybrid named-entity recognition Zum Erkennen der Attribute werden verschiedene, bereits existierende Verfahren kombiniert, um gemeinsam eine Pipeline zu bilden. Die Algorithmen können vorherige Teilergebnisse überschreiben, um das endgültige Resultat zu verbessern. Die Verfahren werden passend zu der Reihenfolge in der Pipeline in den folgenden Abschnitten vorgestellt. Abbildung 11 zeigt einen Überblick, welche Algorithmen verwendet werden. Abbildung 11: Reihenfolge der verwendeten Algorithmen 4.3.1. SpaCy Zu Beginn der Arbeit wurde das vorhandene und bereits trainierte deutsche Modell von SpaCy evaluiert. Das Modell unterstützt das Setzen von Part of Speech (POS), Abhängigkeiten und NER welches für diesen Teil verwendet wurde. Das Modell verwendet eine eigene Word Embedding Strategie mit Unterwortmerkmalen und “Bloom”–Einbettungen sowie ein tiefes CNN um die Ergebnisse zu berechnen (SpaCy– Dokumentation 2019). Trainiert wurde das Modell auf einem Korpus von mehreren tausend, deutschen Wikipedia Artikeln mit 4 ausschlaggebenden Attributen: Lokation, Organisation, Personen und sonstigen. Getestet wurde auf Erkennen der Organisation, welches — in dem gegebenen Anwendungsfall — gleichbedeutend mit dem Hersteller des Produktes ist. Das Ergebnis eines Testszenarios zeigte, dass die Satzstruktur zwischen dem Anwendungsfall und dem Wikipediakorpus zu unterschiedlich ist, sodass das Attribut nicht erkannt wurde. Basierend auf dem Testszenario–Datensatz, der 14 verschiedene Eingaben enthält, wurde ein neues Modell trainiert. Abbildung 12 zeigt die Ergebnisse beider Modelle mit derselben Eingabe. 30 Kapitel 4 Umsetzung Abbildung 12: Ergebnisse der verschiedenen SpaCy–Modelle Das neue Modell wurde auf das Erkennen aller sechs Attribute trainiert. Dieses sollte zeigen, ob das Framework mit einer sehr geringen Menge an Daten lernen kann. Wie der Abbildung 12 zu entnehmen ist hat sich das Ergebnis gegenüber dem vortrainierten Modell deutlich verbessert. Das bedeutet, dass SpaCy, selbst mit einer geringen Menge an Daten, in der Lage ist entsprechende Ergebnisse zu erzeugen. Mit den Trainingsdaten aus dem ersten Iterationsschritt zeigte sich, dass sich das Modell stark verbesserte. Basierend auf der Annahme, dass mehr Trainingsdaten zu einem deutlich besseren und weiterhin performanten Modell führen, bildet der Named Enitity Tagger aus dem SpaCy Framework den ersten Schritt in der Pipeline. SpaCy bietet zudem eine Tokenizing Funktion welche die Eingabe in einzelne Token (z.B. Wörter, Satzzeichen usw.) unterteilt die in den nachfolgenden Schritten verwendet werden. Ein wesentlicher Nachteil der SpaCy NER Funktion ist, dass es die Eingaben von Bezeichnungen an der falschen Stelle erkennt. So werden Wörter als Produkt klassifiziert die keine sind. Aus diesem Grund wurde in der Pipeline eine Gewichtung eingebaut, die den folgenden Algorithmen das Recht gibt, vorherige Teilergebnisse zu überschreiben nicht jedoch zu löschen. Dieses ist von Vorteil da sich einzelne Schritte nur auf das Finden einiger Attribute fokussieren können, welche zum Ende der Pipeline ausgeführt werden, die zum Ausbessern vorheriger Fehler geeignet sind. 4.3.2. Reguläre Ausdrücke Um eine der Schwächen des SpaCy–Modells auszugleichen, wurde ein regelbasierter Ansatz verwendet. Das sprachliche Modell ist für das Erkennen und richtige Unterscheiden von Zahlenwerten weniger geeignet. So wurden die Attribute Preis, Speicher und Kamera von der SpaCy Komponente häufig falsch klassifiziert. Der einzige wesentliche Unterschied ist die Einheit nach dem Zahlenwert wie z.B. 300 e, 256 GB und 13 MP. Zum Erkennen 31 Kapitel 4 Umsetzung solcher Attribute werden reguläre Ausdrücke verwendet. Die Eingabe wird nach einem Zahlenwert durchsucht und anhand der folgenden Einheit klassifiziert. Um das Erstellen der regulären Ausdrücke zu vereinfachen, wird ein Ausdruck dreigeteilt. Das Präfix steht vor dem gesuchten Wert und beschreibt diesen z.B. “max”, “höchstens” oder “mid”. Nach dem Präfix kommt der Stamm, dieser beschreibt den Aufbau des gesuchten Zahlenwertes, in Python könnte es für europäische Preise wie folgt aussehen: 1 r e T a g g e r = ReTagging ( ) 2 3 4 5 p r e f i x = [ ’ max ’ , ’ maximal ’ , ’ b i s zu ’ , ’ ab ’ , ’ mid ’ , stam = [ ’ ( \\\\ d+\\\\ ,\\\\d { 1 , 2 } ) ’ , ’ ( \\\\ d+) ’ ] s u f f i x = [ ’ e ’ , ’ euro ’ ] ’ ’] 6 7 r e T a g g e r . add ( ’MONEY’ , p r e f i x , s u f f i x , stam ) Listing 1: Regel zum Erstellen des regulären Ausdruck zum Erkennen des Preises Zum Schluss folgt das Suffix, es beschreibt die Einheit, die dieses Attribut haben könnte. Diese drei Mengen werden mit dem Attributbezeichner (z.B “MONEY”) der Methode übergeben, die alle Kombinationen aus den Mengen bildet, um daraus den regulären Ausdruck zu erzeugen. Die Tatsache das es ein regelbasierter Ansatz ist, erübrigt das genaue Evaluieren. Dieses Verfahren bietet sich für Attribute an, die hauptsächlich aus Zahlen bestehen und diese Anhand von Regeln erkannt werden können. Es ist von Vorteil, da keine Modelle trainiert werden müssen und direkt eingesetzt werden können. Dadurch wird ein schnelles Reagieren auf Ausdrücke, die neu hinzugefügt werden müssen, ermöglicht. Selbiges zeigt die Unflexibilität von regulären Ausdrücken, da diese nur Werte erkennen, die zuvor mit Regeln beschrieben wurden und zudem manuell gepflegt werden müssen. Zum Klassifizieren von Produkten sind reguläre Ausdrücke nicht geeignet, da weder ein Stamm noch ein Suffix genau definiert werden kann. Das Präfix allein ist nicht ausreichend (z.B. “suche []”, “verkaufe []”). Reguläre Ausdrücke sind schlicht zu unflexibel. Der regelbasierte Ansatz bildet den zweiten Schritt der Pipeline, damit dieser mögliche Fehler bei Attributen mit Zahlenwerten der SpaCy Komponente korrigieren kann. In der ersten Iterationsstufe wurden einige Regeln zum passenden Erkennen der Testdaten erstellt. Diese wurden geringfügig in der späteren Iterationen der Pipeline modifiziert. 32 Kapitel 4 Umsetzung 4.3.3. Metadaten Analyse Wörter haben neben ihrer Bedeutung noch Wortklassen, die basierend der grammatikalischen Eigenschaften des Wortes gesetzt werden. Abbildung 13 zeigt die erkannten Wortklassen (z.B.Verben, Nomen, numerisch oder Satzzeichen) sowie die Beziehungen zwischen den einzelnen Wörter. Das trainierte Modell von SpaCy ist in der Lage die Wortklassen den Wörtern zuzuordnen. Auch werden weitere Metadaten von SpaCy erkannt und den einzelnen Wörtern der Eingabe zugeordnet. Diese sind unter anderem, ob das Token nur aus Buchstaben besteht und ob es sich hierbei um ein Stoppwort handelt. Stoppwörter sind Wörter, die keinen Mehrwert für die Aussage beinhalten, weshalb diese in der Regel in der Datenauswertung ignoriert werden. Auch sind es Wörter, die überdurchschnittlich häufig in einer Sprache vorkommen und häufig als Füllwörter eingesetzt werden. Diese Wörter sind unter anderem: “also”, “bei” oder “hat” und sind bei SpaCy in dem Modell hinterlegt. Weitere Metadaten, die erfasst werden, sind die Position des Wortes in dem Text, die Länge des Wortes und ob der erste Buchstabe des Wortes großgeschrieben ist. Abbildung 13: Beispiel für erkannte Wortklassen des SpaCy Frameworks Basierend auf diese Informationen wurde ein einfaches, neuronales Netz trainiert, dass die Metadaten des einzelnen Wortes als Eingabe verwendet. Das Modell besteht aus drei voll vernetzten Schichten, von der eine Schicht ein versteckter Layer ist. Der letzte Layer des Netzes verwendet die “Binärer Schritt” Aktivierungsfunktion welche das Ergebnis auf die Werte 0 (kein Produkt) und 1 (Produkt) beschränkt. Trainiert wurde das Modell auf demselben Datensatz wie auch schon zuvor das SpaCy–Modell. Der Datensatz besteht aus ausschließlich grammatikalisch, korrekten Anfragen mit einer Maximallänge von 12 Wörtern. Die folgenden Abbildungen 14 und 15 zeigen die Performance des Modells nach der ersten Iteration der Datenakquise. 33 Kapitel 4 Umsetzung Abbildung 14: Auswertung der Metadaten Analyse auf 70 Datensätzen Abbildung 15: Confusion Matrix der Metadaten Analyse auf 70 Datensätzen Kürzere Anfragen sind die Stärke des Netzes, da es bei diesen über eine höhere Genauigkeit verfügt. Wie der Metrik zu entnehmen ist, erkennt das Modell etwas mehr als ein Drittel der Produkte korrekt. Unter optimalen Bedingungen ist die Erkennung des Produktes 34 Kapitel 4 Umsetzung sehr hoch und die Laufzeit des Algorithmus mit im Durchschnitt 25 ms sehr kurz. Einer genaueren Evaluation der Daten (Abbildung 14) zeigte, dass das Modell keine Produkte erkennt, die aus mehreren Wörtern bestehen. So wird zum Beispiel “iPhone 8” nicht erkannt, sondern nur der erste Teil des Wortes: “iPhone”. Dieses ist auf die Auswertung, in der die Tokens einzeln bewertet werden, zurückzuführen. Auch setzt das Modell eine korrekte Rechtschreibung der Eingabe voraus, da sonst den Wörtern falsche Metadaten zugeordnet werden, die das Ergebnis verfälschen. In der zweiten Iteration der Datenakquise wurde das Modell erneut auf die dann vorhandenen Daten trainiert. Der Datensatz erhöhte die maximale Länge der Eingabe und beinhaltete grammatikalisch, inkorrekte Eingaben. Das Ergebnis verschlechterte sich im Vergleich zum vorherigen erheblich, was auf die grammatikalisch, inkorrekte Eingabe zurückzuführen ist. Nach einer Vorverarbeitung, in der fehlerhafte Daten entfernt wurden, wurde das Modell erneut trainiert. Dieses erbrachte keine Verbesserung des ursprünglichen Modells, weshalb das erste Modell in dem Prototyp verwendet wurde. Werden nur die Metadaten betrachtet, gehen viele Informationen der Eingabe verloren, die bei einer Auswertung nicht beachtet werden. Auch werden Produkte, die aus mehreren Wörtern bestehen nie erkannt, da sich die Metadaten zu stark zu den einfachen Produkten unterscheiden. Aus diesen Gründen ist die Genauigkeit der Vorgehensweise limitiert. Durch die gute Performance bei optimaler Eingabe wird dieses Verfahren an dritter Stelle der Pipeline verwendet. Fehler der vorherigen Schritte werden korrigiert, wobei die Klassifizierung der Metadaten kaum bis gar keine Wörter falsch zuordnet. 4.3.4. Targer Das Attribut Produkt wurde in den vorherigen Schritten nur unter bestimmten Umständen richtig klassifiziert, da unter anderem der Aufbau der Eingabe nicht beachtet wurde. In dem Abschnitt 2.2 wurden Word Embeddings vorgestellt, welche Wörter in Form eines Vektors beschreiben. Diese Repräsentation beinhaltet Informationen, anhand das folgende Modell in der Lage ist, Wörter mit Attributen zu versehen. 35 Kapitel 4 Umsetzung Abbildung 16: Aufbau und Funktionsweise des Targer–Modells nach (Chernodub 2018) Das Targer–Modell (Chernodub u. a. 2019) besteht aus der Kombination der Architekturen bidirectional Recurrent Neural Network (biRNN), CNN und CRF. Als RNN wird ein biLSTM–Modell verwendet welche im Kapitel 2 genauer erläutert wurde. Eine beispielhafte visuelle Darstellung des Modells kann Abbildung 16 entnommen werden. Bei dem Trainieren des Modells werden die GloVe Word Embeddings verwendet, um mit diesen Informationen die Buchstaben–Features der Eingabe zu erstellen. Die Buchstaben–Features oder auch Char–level features werden nach dem Trainieren mit dem Modell gespeichert. Bei der Verwendung des Modells werden nur die Char–level features benötigt und nicht mehr die gesamten Wortvektoren. Für den Ablauf des Netzes wird zunächst die Eingabe in die Buchstaben–Features umgewandelt, welche dann an die erste Ebene — also biLSTM — übergeben werden. Dort werden die Vektoren wie bereits in dem Abschnitt 2.6 erläutert verarbeitet. Die daraus resultierenden Ergebnisse werden im Anschluss von dem CNN Netz verarbeitet. In dem letzten Schritt werden die Werte von einem CRF verarbeitet. Für diese Arbeit wurden GloVe Wortvektoren mit 300–Dimension verwendet, um die höchstmögliche Genauigkeit zu erzielen. Aufgrund der langen Trainingsdauer, sowie der großen Menge an benötigten Trainingsdaten wurde ein bereits trainiertes Modell verwendet (Deepset o.D.). Das Vokabular des Modells umfasst 400000 Wörter und wurde auf deutsche Wikipedia Artikel trainiert. Dem trainierten Modell wird eine Liste mit den einzelnen Wörtern der Eingabe übergeben. Die Rückgabe ist eine Liste mit gesetzten Attributen in dem CoNLL Format (Tjong Kim Sang und De Meulder 2003). In diesem Format entspricht ein “O” das für dieses Token kein Attribut gefunden wurde. Für das Produkt “iPhone X” werden die Bezeichnungen “B– 36 Kapitel 4 Umsetzung PRODUCT” und “I–PRODUCT” verwendet. Der erste Teil der Bezeichnung beschreibt, ob es sich um einen Anfang des gesuchten Wortes handelt (“B–”). Besteht das gefundene Attribut aus mehreren Wörtern, werden alle folgenden Wörter mit “I–” als Präfix markiert.Trainiert wurde das Modell nur auf das Erkennen des Attributes Produkt welches in den folgenden Metriken (Abbildung 17 und 18) evaluiert wurde. Abbildung 17: Auswertung des Targer–Modells auf 70 Datensätzen 37 Kapitel 4 Umsetzung Abbildung 18: Confusion Matrix des Targer–Modells auf 70 Datensätzen Wie auf der Abbildung 17 zu erkennen erreicht das Modell eine hohe Genauigkeit, bei dem Erkennen des Produktes. Dieses ist darauf zurückzuführen, dass der Aufbau bzw. die Bedeutung des Satzes betrachtet wird und nicht die einzelnen Wörter. Die benötigte Laufzeit, um die Eingabe mit den entsprechenden Attributen zu versehen, wird durch diesen Schritt geringfügig (im Durchschnitt 125 ms pro Anfrage) beeinflusst, welches bei der Verarbeitung von einzelnen Anfragen den Ablauf nicht merkbar verlängert. Die Berechnung der Metriken wird stärker beeinflusst, da der gesamte bisherige Datensatz von der Pipeline nacheinander verarbeitet wird. Die gute Performance ist auf die Verwendung der Char–level features zurückzuführen, da nicht mehr die gesamten Wortvektoren benötigt werden. Das Targer–Modell ist nicht kontextsensitiv weshalb es Schwierigkeiten hat, Sätze mit mehreren Produkten bzw. Beziehungen zwischen den Produkten korrekt zu klassifizieren. Als Beispiel dient die folgende Eingabe “Mein altes Smartphone ist leider kaputt gegangen weshalb ich dringend ein neues iPhone X benötige!” richtig zu erkennen. Es tendiert dazu, das Attribut ”Produkt”doppelt zu setzen, zum einen für das Wort “Smartphone” und “iPhone X”. Ein alternatives Verhalten ist, dass nur das erste Wort der beiden Elemente mit dem Attribut versehen wird, was in diesem Fall “Smartphone” wäre und somit falsch ist. Durch eine Regel, die besagt, dass immer das zuletzt gefundene Element für ein 38 Kapitel 4 Umsetzung Attribut wiedergegeben werden soll, konnte das erste Szenario teilweise gelöst werden, aber nicht alle Eingaben folgen dieser Regel. 4.3.5. ELMo In dem Abschnitt 2.2.3 wurde das ELMo Word Embedding vorgestellt. Der Vorteil dieser Repräsentation liegt darin, dass die Vektoren der Wörter abhängig von dem Kontext sind und somit das Szenario expliziter beschreiben. Wie bereits erwähnt, ist das Targer– Modell nicht kontextsensitiv weshalb es Schwierigkeiten hat, bestimmte Eingaben korrekt mit Attributen zu versehen. In diesem Schritt wird das grundlegende Modell von Targer (also biLSTM, CNN, CRF) mit der ELMo Repräsentation als Eingabe verwendet, um so die Vorteile beider Methoden zu kombinieren. In dem vorherigen Kapitel wurde bereits beschrieben, wie das Modell aufgebaut ist und auch wie die einzelnen Schichten zusammenarbeiten. Der wesentliche Unterschied zwischen diesen beiden Ansätzen ist, dass keine Char–level features mehr verwendet werden. Dies bedeutet, dass für die Verwendung des Modells immer die GloVe Word Embeddings geladen sein müssen. Zusätzlich dazu werden die bereits trainierten Gewichtungen, sowie die dazugehörigen Einstellungen, zum Verwenden des Modells benötigt. Die Gewichtungen wurden trainiert, um den jeweiligen Kontext einer Eingabe zu bestimmen und so die dazugehörigen Wortvektoren zu ermitteln, die in dem Modell verwendet werden. Diese Vorgehensweise wurde in Abschnitt 2.2.3 genauer erläutert. Für selbst trainierte ELMo Word Embeddings wird eine große Menge von Daten und Zeit benötigt. Das in dieser Arbeit verwendete Modell (May 2019), (Reimers und Gurevych 2019) wurde auf einen deutschen Wikipedia Korpus trainiert. Zusätzlich wurden die Kommentare der verwendeten Artikel genutzt, um Umgangssprache in den Datensatz mit einzubeziehen. Außerdem werden die zum Modell gehörenden Gewichtungen und Optionen verwendet. Da das Modell immer die gesamten Word Embeddings benötigt um verwendet werden zu können, muss die 4 GB große ELMo Datei dauerhaft im RAM verfügbar sein. Aus diesem Grund wurde das Modell in eine separate Anwendung ausgelagert. Dieses bietet eine Representational State Transfer (REST) Schnittstelle, an die eine Eingabe übergeben wird. Diese wird von dem Modell verarbeitet und ein Dictionary mit dem Attributbezeichner als Schlüssel und dem gefundenen Ergebnis als Wert zurückgesendet. Der Pipeline selbst wurde ein Schritt hinzugefügt, welche diese Schnittstelle verwendet, um die entsprechenden Ergebnisse zu erhalten. Durch diese Designentscheidung war es möglich, die benötigten Ressourcen der hauptsächlichen Anwendung gering zu halten und weitere 39 Kapitel 4 Umsetzung Algorithmen können mittels der REST Schnittstelle hinzugefügt werden. Abbildung 19: Auswertung des ELMo–Modells auf 70 Datensätzen Abbildung 20: Confusion Matrix des ELMo–Modells auf 70 Datensätzen 40 Kapitel 4 Umsetzung Das Modell weist die höchste Genauigkeit aller verwendeten Algorithmen auf, wie den Abbildungen 19 und 20 zu entnehmen ist. Dadurch wird deutlich, dass die Betrachtung des Kontextes für den Anwendungsfall zielführend ist. Als Beispiel dient folgende Eingabe “hallo, wir möchten am kommenden wochenende mit den nachbarn grillen und ich wollte dafür einen salat machen weshalb ich auf der suche nach einer salatschüssel bin da mir aufgefallen ist, dass ich keine habe” in der das gesuchte Produkt “salatschüssel” korrekt erkannt wird. Ein anderes Produkt an derselben Stelle wird ebenfalls mit hoher Wahrscheinlichkeit vom Algorithmus korrekt erkannt. Dieses zeigt, dass der Algorithmus nicht die charakteristischen Eigenschaften eines jeden Produktes lernt, sondern die Position, an der ein Produkt stehen würde. Durch dieses Verhalten ist das Modell in der Lage, mit wenigen Trainingsdaten ein überaus gutes Ergebnis zu erzielen. 4.3.6. Fuzzy Matching Der letzte Schritt der Pipeline stellt sicher, dass bestimmte Attribute erkannt werden. So können bestimmte Werte, auf die die Pipeline bisher nicht trainiert wurde, den passenden Attributen zugewiesen werden. Dadurch ist ein schnelles Hinzufügen einzelner Werte möglich, bevor die verwendeten Modelle trainiert werden. Um verschiedene Schreibweisen des Wortes abzudecken, wird ein Fuzzy Matching (Seatgeek 2015) verwendet. Für jedes zuvor definierte Wort wird geprüft mit welcher Wahrscheinlichkeit dieses sich in dem Satz befindet. Dafür wird die Levenshtein Entfernung zwischen einem definierten Wort und einem Wort aus dem Satz gebildet. Da Fuzzy Matching (oder auch Approximation Matching) die Eigenschaft besitzt immer Ergebnisse zu liefern, wird ein zuvor definierter Grenzwert angelegt, wie hoch die Übereinstimmung mindestens sein muss, bevor die Wörter als identisch gelten. Bei Produkten wird ein Grenzwert von 95 % Übereinstimmung angelegt, damit die Unterschiede zwischen den einzelnen Versionsnummern noch erkannt werden, wie z.B. bei Smartphones (iPhone 8 und iPhone X). Der Nachteil bei diesem hohen Grenzwert ist, dass die verschiedenen Schreibweisen für ein Produkt einzeln angegeben werden müssen (z.B. “iphone 8” und “iPhone 8”), da diese sonst unter Umständen nicht mehr erkannt werden. Bei anderen Attributen zeigte sich, dass eine Übereinstimmung von 80 % ausreicht, um ein genaues Ergebnis zu erzielen, da sich Attribute wie “Hersteller” üblicherweise nicht nur in einem einzelnen Buchstaben unterscheiden. Ein weiteres Problem bei Fuzzy Matching ist, dass nicht nur auf die höchste Übereinstimmung geachtet werden darf, sondern auch auf die Länge der Wörter. Befinden sich beispielsweise die Wörter “iPhone” und “iPhone X” in dem Fuzzy Matcher 41 Kapitel 4 Umsetzung und als Eingabe erfolgt der Satz: “Hey ich bin auf der suche nach einem iPhone X” werden die hinterlegten Werte der Reihe nach mit der Eingabe auf teilweiser Übereinstimmung geprüft. Beide Werte erreichen eine Genauigkeit von 100 % und das zurückgegebene Ergebnis hängt von der Reihenfolge der Prüfung ab. Um immer den spezifischen Ausdruck zu identifizieren werden stets längere Werte den kürzeren gegenüber bevorzugt, solange diese sich noch über dem definierten Grenzwert befinden. Dadurch wird sichergestellt, dass in dem Beispielszenario der Wert “iPhone X” erkannt wird. 42 Kapitel 4 Umsetzung Abbildung 21: Vergleich der Genauigkeit nur des ELMo–Modells (oben) und mit anschließendem Fuzzy Matching (unten) 43 Kapitel 4 Umsetzung Die Abbildung 21 zeigt zwei Messungen, die korrekt erkannten Produkte ohne Fuzzy Matching (oben) und mit Fuzzy Matching als letzten Schritt (unten). Wie der Abbildung zu entnehmen ist, hat das Fuzzy Matching in diesem Fall das Ergebnis verschlechtert. Dem Fuzzy Matcher wurden Produkte hinzugefügt, welche bereits von der vorherigen Pipeline erkannt worden sind, aber der Fuzzy Matcher nicht alle Schreibweisen kennt. Das Produkt wird in einer leicht anderen Schreibweise gefunden und durch die Gewichtung der Pipeline wird das vorherige Ergebnis überschrieben. Dadurch ist das Produkt, welches am Ende von der Pipeline erkannt wurde, nicht korrekt in der Eingabe vorhanden und es wird nicht als korrekt klassifiziert gezählt. Die Laufzeit des Fuzzy Matching unter der Verwendung der Levenshtein Entfernung ist abhängig der Anzahl der Werte, auf die die Eingabe geprüft werden soll. So wird der Prototyp mit acht Werten betrieben, die mit 3 ms pro Eingabe die Laufzeit nicht wesentlich beeinflussen. Weitere Tests zeigten, dass die Laufzeit ab 250 Werten bereits durchschnittlich 254 ms beträgt. Dieses zeigt wie bereits in Abbildung 21 dargestellt, dass Fuzzy Matching nur für wenige Szenarien verwendet werden sollte, um so das Ergebnis bis zum Trainieren der neuen Modelle zu verbessern. 4.3.7. Zusammenspiel der Pipeline In den vorherigen Abschnitten wurden die Funktionalitäten sowie die einzelnen Vorteile und Nachteile jeder Komponente vorgestellt. Es wurden verschiedene Algorithmen angewendet, um das Ergebnis möglichst genau abbilden zu können. Alle Algorithmen zeigten Schwierigkeiten mit dem Erkennen von Produktversionen wie sie häufig bei Smartphones zu finden sind (z.B. Galaxy S10). Um diese Szenarien abfangen zu können, wird nach der Ausführung der Pipeline eine Nachverarbeitung der Ergebnisse vorgenommen. Dazu wird jedes Token nach dem gefundenen Produkt analysiert. Es wird geprüft, ob dieses Token eine Kombination aus Buchstaben und Zahlen ist oder ob es ausschließlich aus Großbuchstaben besteht. In beiden Fällen wird das Token zu dem Produkt hinzugefügt und es wird erneut das folgende Token betrachtet. Trifft keiner der beiden Fälle zu, so wird der Prozess beendet und die Nachverarbeitung ist abgeschlossen. Dieses Vorgehen verbessert die Genauigkeit der Pipeline was im Abschnitt Evaluation zusammen mit den ELMo Algorithmus genauer betrachtet wird. Abbildung 22 zeigt den gesamten Aufbau der Pipeline mit allen Algorithmen die verwendet werden. Die Laufzeit der gesamten Pipeline beträgt 837 ms und liegt somit unter den geforderten drei Sekunden aus NF1. 44 Kapitel 4 Umsetzung Abbildung 22: Pipeline mit allen Algorithmen und den ausgelegten Attributen 4.4. Verwendeten Technologien Die Oberfläche ist als Webanwendung verfügbar und wurde mit der Programmiersprache Python und dem Webframework Flask realisiert. Flask ist ein leichtgewichtiges Framework welches nur die Template–Engine Jinja2 als Abhängigkeit besitzt. Die einzelnen Seiten wurden in HTML erstellt und mit Jinja2 dynamisch gestaltet, um eine hohe Flexibilität der Seiten zu erhalten. Jede Funktionalität ist durch einen Flask Endpoint zugänglich und wird durch die Webseite aufgerufen. Das Aussehen der Weboberfläche wurde mit dem CSS–Framework Bootstrap gestaltet. Bootstrap ist ein weit verbreitet Framework und wird von vielen verschiedenen Webseiten eingebunden. Durch die Verwendung von Bootstrap wird ein einheitliches Aussehen mit anderen Webseiten hergestellt. Dieses hat zufolge, dass durch den Wiedererkennungswert der Bedienelemente die Nutzung für Anwender erleichtert wird. Für die Persistierung der Daten wird eine Neo4J Datenbank verwendet. Neo4J ist eine Graphdatenbank und wurde von (Vicknair u. a. 2010) den relationalen Datenbanken gegenübergestellt und bewertet. Die verschiedenen Datenbankzugriffe wurden in Methoden gekapselt, um diese separiert von der Anwendung zu verwalten. Die Pipeline wird als eigene Komponente eingebunden und stellt zwei Methoden zur Verfügung, über die der Webserver die Funktionalität aufrufen kann. Die resolve Methode erhält als Übergabeparameter eine Anfrage, die ausgewertet wird. Zurückgegeben wird ein Dictionary bei dem der Schlüssel dem Attributbezeichner und der Wert der gefundenen Sequenz entspricht. Die zweite Methode wird zum Berechnen der Metriken verwendet und erhält eine Liste mit allen Datensätzen sowie den zugeordneten Attributen. Zurückgegeben wird ein Container, der die Ergebnisse für jeden Algorithmus sowie der gesamten Pipeline enthält. 45 Kapitel 4 Umsetzung 4.5. Entwicklung des Prototyps In den vorherigen Kapiteln wurde beschrieben wie die Pipeline funktioniert, die zum Erkennen der einzelnen Attribute eingesetzt wird. Simultan zur Entwicklung der Pipeline entstand ein Prototyp, der zur Verbesserung der Pipeline entstand. Später wurden weitere Funktionen ergänzt, die sowohl eine Verwendung der Pipeline zeigen als auch das Hinzufügen und Bearbeiten des Datenbestandes vereinfachen. Der Prototyp beschränkt sich auf das Erkennen und Taggen von 6 Attributen (Produkt, Hersteller, Preis, Farbe, Speicher, Kamera) welche alle in der Smartphone–Domäne vertreten sind. Auch stellt der Prototyp keine nutzerorientierte Anwendung dar, sondern lediglich eine funktionsorientierte Oberfläche zum Bedienen der Pipeline. 4.5.1. Eingabemethode Inserat Auf der Startseite wird dem Nutzer die Möglichkeit geboten eine Eingabe zu tätigen. Beim Bestätigen der Eingabe wird die eingegebene Anfrage von der Pipeline verarbeitet und die gefundenen Ergebnisse werden — damit diese für den Benutzer besser nachvollziehbar sind — farblich hervorgehoben. Der Anwender entscheidet, ob alle gesuchten Attribute durch die Pipeline korrekt klassifiziert werden. Ist die Klassifizierung falsch wird der Anwender aufgefordert, die eigene Anfrage selbst mit den passenden Tags zu versehen. Das Zuweisen der Attribute geschieht in der Oberfläche des Taggers. Erst wird eine Wortsequenz aus der Eingabesequenz hervorgehoben und im Anschluss die passende Schaltfläche betätigt, um der Wortkette das Attribut zuzuweisen. Der Tagger kann auf beliebige Attribute erweitert werden. Dazu reicht es aus, eine neue Schaltfläche hinzuzufügen und diese mit einem nicht verwendeten Bezeichner zu versehen. Dem Attribut wird eine neue Farbe zugeteilt. Das neue Attribut kann verwendet werden, um zukünftige Daten zu annotieren. Damit das Attribut automatisch erkannt wird, muss die Pipeline erneut trainiert werden. Wie bereits in der Datenakquise vorgestellt, wurde dieser Teil des Prototyps verwendet, um die Datenbasis zu erweitern. 4.5.2. Berechnung der Metriken Die in dem Kapitel 4 gezeigten Diagramme werden durch den Prototypen, basierend auf den zugrundeliegenden Daten, automatisiert erstellt. Zum Erstellen der Diagramme werden die Daten aus der Datenbank verwendet, die zuvor von Anwendern in die Oberfläche des Prototyps eingegeben wurden. Die Sätze werden erneut an die Pipeline übergeben, sodass die Algorithmen das Ergebnis berechnen. Dieses wird mit den korrekten Attributen aus der Datenbank verglichen und die unterschiedlichen Diagramme werden berechnet. Al- 46 Kapitel 4 Umsetzung le Diagramme betrachten dabei die gesamte Eingabe, bewerten also nicht einzelne Wörter. Die erstellten Diagramme sind: Confusion Matrix, Piechart und Barchart. Abbildung 23 zeigt die Confusion Matrix für das Attribut “Produkt” mit dem verwendeten Algorithmus ELMo. Die vertikale Achse gibt an, ob sich das Attribut in der Eingabe befindet oder nicht. Auf der horizontalen Achse wird das Ergebnis des Algorithmus angegeben. Bei einem guten Ergebnis des Algorithmus sind die Felder mit übereinstimmenden Achsen–Bezeichner höher als die übrigen Felder. Erst bei einer exakten Übereinstimmung, Algorithmus–Ergebnis und Eingabe–Ergebnis, wird der Wert des übereinstimmenden Feldes des Attributes erhöht. Durch eine Fehlermeldung verhindert die Oberfläche Eingaben die nicht mindestens das Attribut “Produkt” enthalten, weshalb wie auf der Abbildung 23 zu erkennen, alle Eingaben dieses Attribut besitzen. Aus dem Ergebnis der Confusion Matrix werden die Werte Accuracy, Precision, Recall und somit auch der F1–Score berechnet. Der Prototyp bietet die Möglichkeit, die Confusion Matrix für jedes Attribut mit jedem Algorithmus darzustellen. Abbildung 23: Confusion Matrix für den ELMo Algorithmus und dem Attribut Produkt Die Confusion Matrix beinhaltet nicht alle Information, die benötigt werden, um die Performance der Pipeline messen zu können. Die Balkendiagramme zeigen wie oft die 47 Kapitel 4 Umsetzung einzelnen Attribute vorkommen und erkannt werden. Dabei zeigt der blaue Balken an, wie oft das einzelne Attribut in dem Datenbestand vorkommt. Der rote Balken zeigt, wie oft das einzelne Attribut komplett korrekt erkannt wird (Angaben in Prozent). Aus dieser Metrik lässt sich sehr einfach die effektive Performance der Algorithmen beurteilen, da die korrekte Klassifizierung der Attribute der Häufigkeit gegenübergestellt wird. Dieses Diagramm zeigt im Wesentlichen die Genauigkeit des Algorithmus für jedes Attribut, weshalb das Balkendiagramm für jeden Algorithmus sowie der gesamten Pipeline dargestellt werden kann. In den Kuchendiagrammen wird die Unterteilung der Klassifizierung weiter aufgesplittet. Die Attribute werden anhand von 6 Teilgruppen bewertet, um ein besseres Verständnis der Klassifizierung des Algorithmus zu erhalten. Die Attribute aus den getesteten Anfragen können dabei einer dieser Gruppen zugeordnet werden: Erkannt, richtige Klasse, Erkannt, falsche Klasse; Zu viel/wenig erkannt, richtige Klasse; Zu viel/wenig erkannt, falsche Klasse; Falsch erkanntes Wort und Attribut nicht erkannt. Durch die Aufteilung wird deutlich, dass, auch wenn der Algorithmus das Attribut nicht komplett korrekt klassifizieren konnte, dennoch akzeptable Teile der Lösung erkannt werden. So wurde häufig die Produktbezeichnung wie ı̈Phoneërkannt, nicht aber der Zusatz ”Xöder SSE”, was durch dieses Diagramm deutlich wurde. Wie die Confusion Matrix kann das Kuchendiagramm für alle Attribute und Komponenten der Pipeline erstellt werden. 4.5.3. Chatbot Als beispielhafte Anwendung bietet der Prototyp eine Chatbot Funktion. Hier können Anwender Anfragen oder Angebote stellen, auf die das System nach einem passenden Gegenstück sucht. Als Beispiel könnte die Eingabe “Hey, letzte Woche ist mein Handy kaputt gegangen, weshalb ich jetzt auf der Suche nach einem neuen Apple iPhone X in Weiß für unter 800 e bin” dienen. Nach dem Absenden wird die Eingabe der Pipeline übergeben und die wesentlichen Attribute werden extrahiert. Basierend auf dieser Eingabe wird eine Anfrage an die Datenbank gestellt, welche passende Gegenangebote und die dazugehörigen Ergebnisse liefert. In einem einfachen Matchmaking wird geprüft, welches dieser Angebote am ehesten zur gegebenen Anfrage passen würde. Dabei wird versucht, möglichst viele übereinstimmende Attribute zu finden. Beim Preis wird die Rolle beachtet: stellt der Anwender eine Anfrage so wird ein geringerer Preis bevorzugt. Handelt es sich hingegen um ein Angebot, wird die Kaufanfrage mit dem höchst genannten Preis bevorzugt. Auf die beispielhafte Eingabe könnte folgende Ausgabe erfolgen “Hey ich biete hier mein neues Apple iPhone X in weiß für 800 e”. Die Antwort enthält 3 Attribute von Interesse auf die das Matchmaking prüfen kann, die alle auf die Suchanfrage zutreffen. 48 Kapitel 4 Umsetzung Abbildung 24 zeigt die Oberfläche mit dem Beispiel als Eingabe. Abbildung 24: Beispielverlauf einer Anfrage über den Chatbot (gelesen von unten nach oben) Die Gruppensuche (Abschnitt 3.2) wird durch zuvor definierte Relationen in dem Knowledge Graph ermöglicht. Eine mögliche Eingabe könnte dann wie folgt aussehen: “Hallo, für meinen Sohn bin ich auf der Suche nach einem Apple Smartphone”. Diese Eingabe enthält weniger konkrete Informationen als das vorherige Beispiel. Die Pipeline hat zwei Attribute gefunden: Hersteller Apple und Produkt Smartphone. Mit dem vorherigen Ansatz würde kein Produkt gefunden werden da die Firma Apple kein Produkt mit dem Namen Smartphone herstellt, sondern nur Geräte, die der Kategorie Smartphone angehören. Um diese Fälle zu identifizieren wird geprüft, ob das gesuchte Produkt in der Wissensdatenbank vorhanden ist. Trifft dieses zu, wird über Relationen geprüft auf welche anderen Produkte der Eintrag verweist. Basierend auf dem Beispiel könnte das Ergebnis diese Werte enthalten: “iPhone 5”, “iPhone 6”, “iPhone 7”, “iPhone 8” oder “iPhone X”. Im Anschluss wird der bereits oben beschriebene Vorgang wiederholt, nur dass anstelle eines Produktes in der Datenbank auf jedes dieser Attribute verglichen wird. 49 Kapitel 4 Umsetzung Zum Schluss folgt ein Matchmaking, um den besten Treffer zu finden, welcher dann dem Anwender angezeigt wird wie es bereits vorgestellt wurde. Für diese Art der Produktfindung wird das genutzte Wissen vorausgesetzt. Es wurde im Vorfeld definiert, dass es die Kategorie Smartphone gibt und auch welche Produkte zu dieser Kategorie gehören. Das System lässt sich beliebig auf weitere Kategorien erweitern. So könnte die Kategorie Tablet hinzugefügt werden, in dem der Bezeichner der Kategorie (hier Tablet) der Wissensdatenbank angehangen wird. Im Anschluss müssen die möglichen Ausprägungen der Kategorie (z.B. ‘iPad’, ‘iPad Air’, ‘iPad Pro’) hinterlegt werden. Dann würde die Eingabe: “Für meinen Neffen suche ich ein neues Tablet, gerne gebraucht aber unter 300 e” sämtliche der Kategorie Tablet enthaltenen Produkte finden. 50 Kapitel 5 Evaluation 5. Evaluation Dieses Kapitel unterteilt sich in die Evaluation zweier Aspekte. In dem ersten Abschnitt werden die verwendete Evaluationsmethodiken vorgestellt. In dem nächsten Abschnitt wird die Performance der verwendeten Algorithmen sowie der gesamten Pipeline untersucht. Es werden die im Abschnitt 3.5 vorgestellten Bewertungskriterien verwendet, um einen genauen Einblick der Ergebnisse zu erhalten. Der dritte Abschnitt befasst sich mit der Evaluation der Oberfläche des Prototyps. Die im Abschnitt 3.2 aufgestellten Anwendungsfälle wurden mit einen Fragebogen an zehn Probanden gestellt. Die Ergebnisse werden im Abschnitt 5.3 analysiert. 5.1. Evaluationsmethodik Das Ziel dieser Arbeit ist es, ein System zu erstellen, welches mit einer geringen Menge von Trainingsdaten dem Anwender das Gefühl vermittelt verstanden zu werden. Um dieses beurteilen zu können wurde eine Nutzerevaluation und mehrere Messungen durchgeführt. Für die Messungen werden die Daten aus beiden Iterationen der Datenakquise verwendet, um diese Ergebnisse mit den Daten aus der ersten Iteration zu vergleichen. Der F1– Score beschreibt dabei die Genauigkeit der jeweiligen Algorithmen. Zusätzlich werden die Werte: Accuracy, Precision und Recall betrachtet, um die Forschungsfrage NF1 zu beantworten und damit die Ergebnisse mit anderen Arbeiten verglichen werden können. In dem Abschnitt 3.5 wurden noch sechs weitere Klassen vorgestellt, die ebenfalls für eine ausführlichere Evaluation betrachtet werden. Die NA2 ist durch Zahlenwerte schwer zu beantworten, weshalb eine Nutzerevaluation durchgeführt wurde. Um überprüfen zu können ob Anwender das Gefühl haben von dem System verstanden zu werden reicht es nicht aus, eine bestimmte Genauigkeit zu erreichen. Anhand mehrerer Anwendungsfälle sowie einen Fragebogen wird eine Nutzerevaluation durchgeführt, um die Oberfläche sowie das Verständnis auszuwerten. 5.2. Auswertung der Algorithmen Das vorherige Kapitel befasste sich mit verschiedenen Algorithmen, die zusammen kombiniert wurden, um ein optimales Ergebnis zu erzielen. Die höchste Gewichtung wurde dem ELMo Algorithmus zugeteilt, da dieser in der ersten Evaluation die höchste Genauigkeit aufweisen konnte. In diesem Abschnitt wird dieser Algorithmus, sowie die gesamte Pipeline einer ausführlichen Evaluation unterzogen, um die Performance genauer zu charakterisieren. 51 Kapitel 5 Evaluation Zu Beginn wurden alle Modelle, bis auf eines, in der Pipeline mit 70 Datensätzen verschiedener Länge und Form trainiert. Das übrige Modell wurde zunächst mit 16 Datensätzen und für eine Evaluation erneut mit 70 Datensätzen trainiert. Basierend auf diesen Ergebnissen wurden die Modelle bewertet, ob und wie diese sich in die Pipeline einbauen lassen. Der aktuelle Datensatz umfasst 193 Einträge, die im Laufe der Entwicklung gesammelt wurden. Abbildung 25: Performance der gesamten Pipeline Abbildung 25 zeigt die Performance der gesamten Pipeline mit den anfänglich trainierten Modellen. Bei dem Attribut “Produkt” wird eine Genauigkeit von 80,8 % erzielt, für die übrigen Attribute wurden keine Modelle trainiert. Die übrigen Werte werden nur durch Ansätze wie Reguläre Ausdrücke bzw. Fuzzy Matching ermöglicht. Dieses Ergebnis zeigt das bereits mit einer geringen Menge an Daten, das Modell in der Lage ist in 4 von 5 Fällen das Produkt korrekt zu klassifizieren. 79,8 % werden von dem ELMo Algorithmus erkannt, die Nachverarbeitung der gesamten Pipeline verbessert das Ergebnis auf 80,8 %. Szenarien, die in dieser Statistik nicht berücksichtigt werden, sind unter anderem Attribute, die nur in Teilen erkannt wurden. “Galaxy S10” ist ein beispielhaftes Produkt, dass hätte erkannt werden sollen, von der Pipeline wurde “Galaxy” erkannt, was teilweise korrekt ist. Abbildung 26 zeigt eine genauere Analyse des ELMo Algorithmus. Es werden zusätzlich zu den Klassen “Erkannt, richtige Klasse” und “Attribut nicht erkannt” noch vier weitere Klassen erfasst: “Erkannt, falsche Klasse”, “Zu viel/wenig erkannt, richtige Klasse”, “Zu viel/wenig erkannt, falsche Klasse” und “Falsch erkanntes Wort”. 52 Kapitel 5 Evaluation Abbildung 26: Piechart Analyse der ELMo Komponente Wie der Abbildung 26 zu entnehmen ist wurden 9,3 % (“Attribut nicht erkannt” und “Falsch erkanntes Wort”) der Anfragen unzureichend verarbeitet. In 10,9 % der Fälle wurde zu viel bzw. zu wenig des gesuchten Produktes erkannt, was — je nach Aufgabenstellung — bereits zielführend ist. Durch Optimierung der Nachverarbeitung wäre die Pipeline imstande gegenüber den bisher erzielten 1 %, die Eingabe um 10,9 % selbstständig zu verbessern. Alternativ könnte das Modell mit einem größeren Datensatz trainiert werden, um eine allgemeine Verbesserung der Ergebnisse zu erzielen. Durch die Datenakquise sowie den Testdaten sind insgesamt 260 Datensätze gesammelt worden, die zum Trainieren und Verifizieren verwendet wurden. Die Daten fokussierten sich hauptsächlich auf Produkte aus der Kategorie Smartphone, aber auch andere Produktkategorien waren vertreten. Die Länge der Sätze war variabel, die Rechtschreibung wird nicht beachtet. Die Daten wurden durch die Oberfläche des Prototyps gesammelt, weshalb keine Vorverarbeitung des Datensatzes notwendig war. Zum Trainieren des Modells wurden die Daten aufgeteilt. Auf 90 % der Datensätze — also 234 Einträge — wurde das Modell trainiert. Die restlichen 10 % wurden für die Verifizierung verwendet. Aufgrund der geringen Datenmenge war es möglich, dass bestimmte Formulierungen der Sätze nur zum Testen und nicht zum Trainieren verwendet wurden. Um diese Problematik zu umgehen wurde das Modell in zehn Durchgängen mit verschiedenen Daten zum Verifizieren trainiert. Für ein endgültiges Ergebnis wurde der Durchschnitt der Trainingsdurchläufe verwendet. 53 Kapitel 5 Evaluation Durchlauf 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. Avg. Precision 0,857 0,889 0,905 0,880 0,815 0,885 0,889 0,963 0,793 0,815 0,869 Recall 0,923 0,923 0,942 0,846 0,846 0,885 0,923 0,963 0,767 0,771 0,879 F1–Score 0,8889 0,9057 0,9231 0,8627 0,8302 0,8850 0,9057 0,9630 0,7797 0,7925 0,8736 Tabelle 1: Ergebnisse der 10 Trainingsdurchläufe Wie der Tabelle 1 zu entnehmen ist, wurde ein durchschnittlicher F1–Score von 87,36 % erreicht. Dieser setzt sich aus den Werten Precision sowie dem Recall zusammen und vereint beide Werte in einem. Ein höherer F1–Score bedeutet im Allgemeinen, dass die Performance des Modells gestiegen ist. In gewissen Situationen kann es Ziel sein, eine hohe Precision auf Kosten des Recalls zu erreichen und umgekehrt, was bei dieser Aufgabenstellung nicht zutrifft. Berechnet wurden die Werte basierend auf jedem Wort der Verifikationsdaten. Bei den Metriken des Prototyps wird die gesamte Sequenz der Eingabe für die Bewertung beachtet, weshalb ein direkter Vergleich nicht möglich ist. Damit die Werte verglichen werden können, wurde das vorhandene ELMo–Modell der Pipeline nacheinander mit jedem einzelnen der zehn berechneten Modelle ausgetauscht und die gleiche Metrik wurde erstellt. Durch die neu trainierten Modelle verbesserte sich die Genauigkeit der Pipeline im Durchschnitt auf 87,46 %. Das vorherige ELMo–Modell wurde durch die Nachverarbeitung um 1 % verbessert. Die durchschnittliche Genauigkeit der neu trainierten ELMo–Modelle erreichte 87,21 % was bedeutete, dass die Nachverarbeitung das Ergebnis um 0,25 % verbesserte. Durch die Tatsache, dass die gesamte Eingabe zum Berechnen der Metrik betrachtet wurde und jede Eingabesequenz mindestens das Attribut Produkt enthalten musste, war die Confusion Matrix einseitig. Die Werte Precision und Recall wurden aus den Werten der Confusion Matrix berechnet weshalb der daraus resultierende F1–Score von 93,1 % nicht mit dem vorherigen F1–Score verglichen werden konnte. Das vorherige ELMo–Modell erreichte ein F1–Score von 89 %, dieses entsprach einer Verbesserung von etwa 4 %. Die allgemeine Genauigkeit verbesserte sich um ungefähr 7 %. Daraus resultierte, dass der ELMo Algorithmus — mit fast der vierfachen Menge an Daten — nur marginal besser wurde. 54 Kapitel 5 Evaluation 5.3. Auswertung der Oberfläche Die vorgestellte Anwendung soll dem Nutzer die Bedienung bzw. den Umgang mit der Pipeline näherbringen. Dazu wird ein grundlegendes Verständnis der Anwendung durch die Benutzung der einfachen Eingabe vermittelt. Dieses beinhaltet das manuelle Setzen von Attributen. Diese Funktion ist ein wesentlicher Bestandteil der Arbeit, da zum Trainieren aller Modelle mit Bezeichner versehene Daten benötigt werden. Zusätzlich wird das manuelle Setzen der Attribute in jeder Realisierung einer solchen Anwendung benötigt, da das System nicht immer alle Attribute korrekt erfasst. Eine mögliche Anwendung für die Pipeline ist ein Chatbot welcher ebenfalls Teil des Prototyps ist. Dieser soll Anwendern zeigen, welche Vorteile ein virtueller Marktplatz gegenüber herkömmlichen Marktplätzen besitzt. All diese Funktionen wurden von mehreren Probanden getestet und bewertet. Die Probanden wurden aufgrund ihrer verschiedenen Fachkenntnisse ausgewählt, so wurden User Experience (UX), User Interface (UI)-Designer sowie Software Engineering (SE) befragt um ein umfassendes Feedback zu erhalten. Insgesamt haben an der Evaluation zehn Personen teilgenommen, wobei jeder dieselben Szenarien zu bewältigen hatte. Zu Beginn wurde das Umfeld des Tests vorgestellt: Der Proband befindet sich in der Facebook Gruppe “Flohmarkt Karlsruhe” und versucht in dieser Gruppe sein altes Smartphone zu verkaufen. In dem ersten Durchlauf wurde das Ergebnis der Pipeline nur angezeigt, damit die Testperson sieht, wie die Anfrage verarbeitet wird und welche Attribute von der Pipeline erkannt werden. In dem zweiten Durchlauf sollten die Probanden eine Anfrage erzeugen, die nicht erkannt wird. Im Anschluss wurde das manuelle Tagging getestet. In dem letzten Testszenario sollten die Probanden die Eingabe der ersten Anfrage in dem Chatbot wiederholen. Während der Tests wurden die Probanden beobachtet wie diese den Prototypen bedienen. Den Abschluss bildete ein Fragebogen (siehe Anhang). Die Ergebnisse der Evaluation werden im folgenden Absatz vorgestellt. Das Szenario, in dem die Probanden die einfache Eingabemaske zum Erstellen eines Inserats bedienen sollten, wurde von 9 Teilnehmern direkt verstanden und es wurde eine passende Anfrage an das System geschickt. Die Ansicht mit der farblichen Hervorhebung wurde von allen Probanden auf Anhieb verstanden. 30 % der Probanden waren unsicher bezüglich der erkannten Attribute. So wurde z.B. bei der Eingabe: “Hey ich verkaufe mein altes Handy. Es ist ein Huawei P30 preis verhandelbar” Huawei P30 als Produkt erkannt. Für den Probanden sollte Handy das Produkt sein, Huawei die Marke und P30 das Modell. In dem Abschnitt 3.1 wurde bereits erläutert, warum von der Pipeline Huawei P30 korrekterweise als Produkt 55 Kapitel 5 Evaluation Durch die erste Aufgabe haben die Probanden erkannt, welche Attribute von der Pipeline erkannt werden sollten, was die Bearbeitung der zweiten Aufgabe erleichterte. Die Anwender werden nach der Eingabe vom System gefragt, ob die erkannten Ergebnisse korrekt sind. Bei einer Verneinung wird die Anfrage an den Tagger weitergeleitet und der Anwender wird aufgefordert seine Eingabe manuell mit Attributen zu versehen. Wird das Ergebnis abgelehnt, war jedem Probanden bewusst, dass die Attribute manuell gesetzt werden sollen, ohne dass eine solche Anweisung von der Oberfläche angezeigt wird. Alle Probanden versuchten zunächst die farblichen Schaltflächen der Attribute auf die passenden Begriffe der Eingabe per Drag–and–Drop zu ziehen. Erst nach einigen Versuchen wurde die Bedienung des Taggers verstanden. Das vorherige erwähnte Problem der Unsicherheit — was genau mit welchem Attribut zu versehen ist — hatten hier 6 von 10 Probanden. In dem letzten Szenario wurde der Chatbot evaluiert. Durch die vorherigen Aufgaben und die kleine Beschreibung des Chatbots war eine problemlose Bedienung möglich. Je nach Anfrage erhielten die Anwender eine Antwort, entweder dass ein passendes Angebot gefunden werden konnte oder die Rückmeldung, dass kein Angebot vorhanden ist. In beiden Fällen wurde deutlich welche Attribute in der Anfrage enthalten waren. Dadurch konnten die Probanden nachvollziehen, dass kein passendes Angebot für die jeweilige Eingabe gefunden werden konnte. Von allen Teilnehmern wurde die Zustandslosigkeit bzw. das nicht Fortführen der Verhandlung des Chatbots negativ wahrgenommen. Der Fragebogen teilt sich in vier Hauptkategorien auf: Oberfläche, Funktionalität, natürliche Sprachverarbeitung und Feedback. Die gesamten Antworten können dem Fragebogen aus dem Anhang entnommen werden. Im Folgenden werden einige der Antworten vorgestellt. Der Chatbot stellt eine reale Anwendung dar, der dem Nutzer die Vorteile der natürlichen Sprachverarbeitung zeigen soll. In dem Testdurchlauf wurde bereits deutlich, dass die Probanden davon ausgingen, dass der Chat fortgeführt werden würde. Diese Erkenntnis spiegelt sich deutlich in der Umfrage wieder, da die Stimmen bei “Zielführend” gleich zwischen -1 und +1 aufgeteilt sind. Zudem erwarteten einige Probanden, dass mehr als nur ein Resultat auf die gegebene Anfrage angezeigt werden würde. Auch wurde mehrfach versucht, die Suche durch weitere Anfragen zu spezifizieren, was aufgrund der Zustandslosigkeit nicht möglich war. Das Tagging spielt eine wesentliche Rolle bei allen NLP Anwendungen. Selbst gut funktionierende Anwendungen sollten Nutzern die Möglichkeit geben, die eigene Eingabe manuell mit Attributen zu versehen, falls diese nicht korrekt erkannt wurden. Ist dieses 56 Kapitel 5 Evaluation nicht möglich, wird der Nutzer nicht verstanden und eine Benutzung der Anwendung ist unmöglich. In der Evaluation sollte der Tagger verwendet werden, ohne dass dieser erklärt wird. Wie der Abbildung 27 zu entnehmen ist war dieser Test teilweise erfolgreich: der Tagger ist sowohl zielführend als auch optisch ansprechend. Die Intuitivität hingegen wurde besser bewertet als in der Evaluation beobachtet werden konnte, da alle Probanden erst nach einigen Versuchen die Bedienung verstanden haben. Durch die vorherige Aufgabe wurde bereits ein gewisses Verständnis der farblichen Hervorhebung vermittelt, weshalb die Verständlichkeit des Taggers gut ist. Die meisten Teilnehmer hatten das Gefühl, dass die Anwendung die gegebene Anfrage verstehen würden. Diese Frage wurde überwiegend durch den ersten Eindruck beantwortet, da die meisten Probanden weniger als 6 Anfragen an das System stellten. Dies ist ein wesentliches Kriterium für eine reale Anwendung, da der erste Eindruck entscheidend dafür ist, ob das Programm weiterhin verwendet wird oder nicht. Somit hatten Anwender das Gefühl, von dem System verstanden zu werden was NA2 erfüllt. Ebenfalls relevant ist die Frage, ob diese Art der Produktsuche gegenüber der herkömmlichen Schlagwortsuche bevorzugt werden würde. 9 von 10 Probanden stimmten dem zu. Abbildung 27: Ergebnisse der Umfrage bezüglich der Tagging Funktion Zusammenfassend war die Evaluation erfolgreich, da alle Probanden mit dem Prototyp zufrieden waren. Was genau mit welchen Attributen versehen werden sollte sowie die Oberfläche des Taggers, benötigt eine kurze Erklärung, damit Nutzer genau wissen was die Anwendung erwartet. Ebenfalls sollte das Aussehen der meisten Funktionen überarbeitet werden, da diese bei der Evaluation überwiegend neutral bewertet wurden. 57 Kapitel 7 Fazit 6. Fazit Diese Arbeit befasst sich mit dem Umwandeln von unstrukturierter Eingabe in strukturierte Ausgabe, die von einem Computer weiterverarbeitet wird. Um dieses Ziel zu erreichen wurden verschiedene Methoden und Algorithmen angewendet und evaluiert. Im Rahmen dieser Arbeit wurde ein generisches Konzept entwickelt, durch das ein virtueller Marktplatz mit der natürlichen Sprachverarbeitung unterstützt werden kann. Die Konzeption wurde prototypisch umgesetzt, um zusätzlich zu den Anforderungen, das Sammeln von Daten und Bewerten der Performance zu unterstützen. Die Forschungsfragen FF1 und NF2 können mit der Pipeline beantwortet werden. Im Bereich des ML ist die vorhandene Menge der Daten ein wesentlicher Faktor um zielführende Modelle zu erstellen. Der mit dieser Arbeit verbundene Datensatz umfasste lediglich 260 Einträge. Das auf diesem Datensatz erzielte Ergebnis von 87,46 % Genauigkeit, welches im Abschnitt 5.2 hergeleitet wurde, zeigt dass auch mit einer kleinen Menge von Daten ein solides Ergebnis entstehen kann, was NF1 beantwortet. Dieses Ergebnis übertrifft die anfänglichen Erwartungen deutlich, da vermutet wurde das die ganzen Produkte zu verschieden sind, um diese so präzise zu bestimmen. Somit konnten alle anfänglich gestellten Forschungsfragen beantwortet werden. Mit der prototypischen Implementierung konnte das Verständnis der Anwender gegenüber einem solchen System untersucht werden. Die Evaluation zeigt sowohl eine hohe Akzeptanz als auch das Interesse der Nutzer gegenüber der neuen Art auf virtuellen Marktplätzen zu handeln. Diese Erkenntnisse erzeugen, zusätzlich zu den bereits erwähnten aus Abschnitt 1.1, weitere Mehrwerte die ein Unternehmen erhalten würde wie z.B. ein innovatives, akzeptiertes Verfahren der Anfragenverarbeitung. Die Oberfläche sowie Funktionsweise des Taggers wurde während der Verwendung des Prototyps positiv wahrgenommen, was zeigt, dass dieser als Vorlage für eine reale Anwendung verwendet werden kann. 7. Ausblick Ein Tool das beliebige Attribute in einer Eingabesequenz klassifizieren kann, hat viele verschiedene Anwendungsbereiche. Der Prototyp umfasst bereits alle Funktionalitäten, die für eine neue Applikation benötigt werden. So werden beispielsweise Daten gesammelt und gegebenenfalls nachträglich überarbeitet. Auch wird dem Nutzer mit dem Chatbot eine Anwendung geboten, die die Vorteile eines solchen Systems darstellt. Das automatische Generieren der Metriken zeigt die Stärken des Systems und durch die verschiedenen Graphen wird das Finden von Schwächen erleichtert. Weitere Ideen für Funktionalitäten 58 Kapitel 7 Ausblick wurden bereits während der Entwicklung des Prototyps deutlich, die Aufgrund des Umfangs dieser Arbeit nicht realisiert wurden. Ein Feature war das automatische Trainieren der Pipeline, sobald eine gewisse Menge von neuen Daten zur Verfügung stand. Dieses würde dafür sorgen, dass die Performance der Anwendung sich von selbst verbessert, ohne dass das Training manuell gestartet werden müsste. Auch könnte der Tagger in seiner Funktionalität erweitert werden. So besteht für den Anwender die Möglichkeit eigene Attribute hinzuzufügen, welche dann im Tagger verwendet werden können. Durch diese Änderung könnten bereits Daten für andere Kategorien gesammelt werden, welches ein späteres Erweitern der Pipeline erleichtert. Durch das Feedback der Evaluation wurde deutlich, dass eine andere Art der Bedienung des Taggers den Umgang verbessern könnte. Eine mögliche Realisierung wäre, dass Nutzer ein Attribut auswählen und im Anschluss die passenden Wörter anklicken wodurch diese farblich hervorgehoben werden. In dem 1.1. Kapitel wurde die Bedienung über Sprachinterfaces vorgestellt, welches ebenfalls eine interessante Erweiterung für den Prototypen darstellt. Die Matchmaking Funktion des Chatbots könnte überarbeitet werden, da diese bisher nur auf die genaue Übereinstimmung von Attributen achtet. Auch könnten dem Chatbot Zustände hinzugefügt werden, wodurch dem Anwender eine echte Konversation suggeriert wird. Zum Schluss dieser Arbeit ergeben sich neue Fragen, wie z.B.: “Wie gut skaliert die Pipeline?” bzw. “Wie viele Attribute können die einzelnen Modelle unterscheiden, ohne dass die Genauigkeit beeinflusst wird?”. Hierbei handelt es sich um wesentliche Aspekte, die für die Realisierung eines virtuellen Marktplatzes in dieser Form benötigt werden. Die Pipeline selbst könnte durch Ergänzen von mehreren Attributen aus verschiedenen Produktklassen erweitert werden. Dadurch wäre es mögliche, die Frage der Skalierbarkeit zu beantworten. 59 Kapitel 8 Quellenverzeichnis 8. Quellenverzeichnis Zadeh, Lotfi A (1965). Fuzzy sets“. In: Information and control 8.3, S. 338–353. ” Levenshtein, Vladimir I (1966). Binary codes capable of correcting deletions, insertions, ” and reversals“. In: Soviet physics doklady. Bd. 10. 8, S. 707–710. Schütze, Hinrich und Jan O Pedersen (1995). Information retrieval based on word senses“. ” In: Citeseer. Schuster, Mike und Kuldip K Paliwal (1997). Bidirectional recurrent neural networks“. ” In: IEEE transactions on Signal Processing 45.11, S. 2673–2681. Cummins, F., F.A. Gers und J. Schmidhuber (1999). Learning to forget: continual pre” diction with LSTM“. In: IET Conference Proceedings, 850–855(5). Lafferty, John D., Andrew McCallum und Fernando C. N. Pereira (2001). Conditional ” Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data“. In: Proceedings of the Eighteenth International Conference on Machine Learning, S. 282– 289. Tjong Kim Sang, Erik F. und Fien De Meulder (2003). Introduction to the CoNLL-2003 ” Shared Task: Language-Independent Named Entity Recognition“. In: Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, S. 142–147. Mansouri, Alireza, Lilly Suriani Affendey und Ali Mamat (2008). Named entity recogni” tion approaches“. In: International Journal of Computer Science and Network Security 8.2, S. 339–344. Arel, Itamar, Derek C Rose und Thomas P Karnowski (2010). Deep machine learning-a ” new frontier in artificial intelligence research [research frontier]“. In: IEEE computational intelligence magazine 5.4, S. 13–18. Faruqui, Manaal und Sebastian Padó (2010). Training and Evaluating a German Named ” Entity Recognizer with Semantic Generalization“. In: KONVENS, S. 129–133. Mikolov, Tomáš u. a. (2010). Recurrent neural network based language model“. In: Ele” venth annual conference of the international speech communication association. Vicknair, Chad u. a. (2010). A comparison of a graph database and a relational database: ” a data provenance perspective“. In: Proceedings of the 48th annual Southeast regional conference, S. 1–6. Krizhevsky, Alex, Ilya Sutskever und Geoffrey E Hinton (2012). Imagenet classification ” with deep convolutional neural networks“. In: Advances in neural information processing systems, S. 1097–1105. Mikolov, Tomas u. a. (2013). Distributed representations of words and phrases and their ” compositionality“. In: Advances in neural information processing systems, S. 3111–3119. 60 Kapitel 8 8. Quellenverzeichnis Goldberg, Yoav und Omer Levy (2014). word2vec Explained: deriving Mikolov et al.’s ” negative-sampling word-embedding method“. In: arXiv preprint arXiv:1402.3722. Kalchbrenner, Nal, Edward Grefenstette und Philip Blunsom (2014). A convolutional ” neural network for modelling sentences“. In: 52nd Annual Meeting of the Association for Computational Linguistics. Pennington, Jeffrey, Richard Socher und Christopher D Manning (2014). Glove: Global ” vectors for word representation“. In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), S. 1532–1543. Olah, Christopher (2015). Understanding LSTM Networks. http://colah.github.io/ posts/2015-08-Understanding-LSTMs/. (besucht am 22.11.2019). Seatgeek (2015). Fuzzywuzzy. https://github.com/seatgeek/fuzzywuzzy. (besucht am 16.10.2019). Zhang, Ye und Byron Wallace (2015). A sensitivity analysis of (and practitioners’ gui” de to) convolutional neural networks for sentence classification“. In: arXiv preprint arXiv:1510.03820. Deepu, S, Pethuru Raj und S Rajaraajeswari (2016). A Framework for Text Analytics ” using the Bag of Words (BoW) Model for Prediction“. In: Proceedings of the 1st International Conference on Innovations in Computing & Networking, S. 12–13. Jiang, Ridong, Rafael E Banchs und Haizhou Li (2016). Evaluating and combining name ” entity recognition systems“. In: Proceedings of the Sixth Named Entity Workshop, S. 21– 27. Bai, Shaojie, J Zico Kolter und Vladlen Koltun (2018). An empirical evaluation of ge” neric convolutional and recurrent networks for sequence modeling“. In: arXiv preprint arXiv:1803.01271. Chernodub, Artem (2018). Targer. https://github.com/achernodub/targer. (besucht am 28.10.2019). Devlin, Jacob u. a. (2018). Bert: Pre-training of deep bidirectional transformers for lan” guage understanding“. In: arXiv preprint arXiv:1810.04805. Perone, Christian S, Roberto Silveira und Thomas S Paula (2018). Evaluation of sen” tence embeddings in downstream and linguistic probing tasks“. In: arXiv preprint arXiv:1806.06259. Peters, Matthew E, Mark Neumann, Mohit Iyyer u. a. (2018). Deep contextualized word ” representations“. In: arXiv preprint arXiv:1802.05365. Peters, Matthew E, Mark Neumann, Luke Zettlemoyer u. a. (2018). Dissecting con” textual word embeddings: Architecture and representation“. In: arXiv preprint arXiv:1808.08949. 61 Kapitel 8 8. Quellenverzeichnis Sherstinsky, Alex (2018). Fundamentals of Recurrent Neural Network (RNN) and Long ” Short-Term Memory (LSTM) Network“. In: arXiv preprint arXiv:1808.03314. Chernodub, Artem u. a. (2019). Targer: Neural argument mining at your fingertips“. In: ” Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, S. 195–200. May, Philip (2019). German ELMo Model. https://github.com/t-systems-on-siteservices-gmbh/german-elmo-model. (besucht am 24.10.2019). Reimers, Nils und Iryna Gurevych (2019). Alternative Weighting Schemes for ELMo ” Embeddings“. In: arXiv preprint arXiv:1904.02954. SpaCy–Dokumentation (2019). Library architecture. https : / / spacy . io. (besucht am 14.10.2019). Deepset (o.D.). German Word Embeddings. https : / / deepset . ai / german - word embeddings. (besucht am 25.10.2019). 62 Anhang A Ergebnisse der Umfrage Anhang A. Ergebnisse der Umfrage Abbildung 28: Bewertung der Oberfläche I Anhang A Ergebnisse der Umfrage Abbildung 29: Fortsetzung der Bewertung zur Oberfläche II Anhang A Ergebnisse der Umfrage Abbildung 30: Bewertung der Funktionalität III Anhang A Ergebnisse der Umfrage Abbildung 31: Fortsetzung der Bewertung zur Funktionalität Abbildung 32: Allgemeine Fragen zur natürlichen Sprachverarbeitung IV Anhang A Ergebnisse der Umfrage Abbildung 33: Fortsetzung der natürlichen Sprachverarbeitung V Anhang A Ergebnisse der Umfrage Abbildung 34: Fortsetzung der natürlichen Sprachverarbeitung VI Anhang B Ergebnisse der Umfrage Abbildung 35: Feedback der Probanden VII Anhang B Metriken der Pipeline B. Metriken der Pipeline Abbildung 36: Confusion Matrik mit dem Attribut ”Preis”von dem Schritt reguläre Ausdrücke Abbildung 37: Confusion Matrik mit dem Attribut ”Kamera”von dem Schritt reguläre Ausdrücke VIII Anhang B Metriken der Pipeline Abbildung 38: Confusion Matrik mit dem Attribut ”Hersteller”von dem Schritt Fuzzy Matching Abbildung 39: Confusion Matrik mit dem Attribut ”Farbe”von dem Schritt Fuzzy Matching IX '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
