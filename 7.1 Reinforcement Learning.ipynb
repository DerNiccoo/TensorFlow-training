{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "# Warum auch immer, bekomme ich einen fehler wenn ich das hier nicht drinne habe. \n",
    "config = ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.85\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Set seed for experiment reproducibility\n",
    "seed = 42\n",
    "env.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Small epsilon value for stabilizing division operations\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "    \"\"\"Combined actor-critic network.\"\"\"\n",
    "\n",
    "    def __init__(self, num_actions: int, num_hidden_units: int):\n",
    "        \"\"\"Initialize.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n",
    "        self.actor = layers.Dense(num_actions)\n",
    "        self.critic = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        x = self.common(inputs)\n",
    "        return self.actor(x), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n  # 2\n",
    "num_hidden_units = 128\n",
    "\n",
    "model = ActorCritic(num_actions, num_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
    "\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    return (state.astype(np.float32), np.array(reward, np.int32), np.array(done, np.int32))\n",
    "\n",
    "\n",
    "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
    "    return tf.numpy_function(env_step, [action], [tf.float32, tf.int32, tf.int32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(initial_state: tf.Tensor, model: tf.keras.Model, max_steps: int) -> List[tf.Tensor]:\n",
    "    \"\"\"Runs a single episode to collect training data.\"\"\"\n",
    "\n",
    "    action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "\n",
    "    initial_state_shape = initial_state.shape\n",
    "    state = initial_state\n",
    "\n",
    "    for t in tf.range(max_steps):\n",
    "        # Convert state into a batched tensor (batch size = 1)\n",
    "        state = tf.expand_dims(state, 0)\n",
    "\n",
    "        # Run the model and to get action probabilities and critic value\n",
    "        action_logits_t, value = model(state)\n",
    "\n",
    "        # Sample next action from the action probability distribution\n",
    "        action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
    "        action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "\n",
    "        # Store critic values\n",
    "        values = values.write(t, tf.squeeze(value))\n",
    "\n",
    "        # Store log probability of the action chosen\n",
    "        action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "\n",
    "        # Apply action to the environment to get next state and reward\n",
    "        state, reward, done = tf_env_step(action)\n",
    "        state.set_shape(initial_state_shape)\n",
    "\n",
    "        # Store reward\n",
    "        rewards = rewards.write(t, reward)\n",
    "\n",
    "        if tf.cast(done, tf.bool):\n",
    "            break\n",
    "\n",
    "    action_probs = action_probs.stack()\n",
    "    values = values.stack()\n",
    "    rewards = rewards.stack()\n",
    "\n",
    "    return action_probs, values, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_return(rewards: tf.Tensor, gamma: float, standardize: bool = True) -> tf.Tensor:\n",
    "    \"\"\"Compute expected returns per timestep.\"\"\"\n",
    "\n",
    "    n = tf.shape(rewards)[0]\n",
    "    returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "    # Start from the end of `rewards` and accumulate reward sums\n",
    "    # into the `returns` array\n",
    "    rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    discounted_sum_shape = discounted_sum.shape\n",
    "    for i in tf.range(n):\n",
    "        reward = rewards[i]\n",
    "        discounted_sum = reward + gamma * discounted_sum\n",
    "        discounted_sum.set_shape(discounted_sum_shape)\n",
    "        returns = returns.write(i, discounted_sum)\n",
    "    returns = returns.stack()[::-1]\n",
    "\n",
    "    if standardize:\n",
    "        returns = ((returns - tf.math.reduce_mean(returns)) / (tf.math.reduce_std(returns) + eps))\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def compute_loss(action_probs: tf.Tensor, values: tf.Tensor, returns: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
    "\n",
    "    advantage = returns - values\n",
    "\n",
    "    action_log_probs = tf.math.log(action_probs)\n",
    "    actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "\n",
    "    critic_loss = huber_loss(values, returns)\n",
    "\n",
    "    return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(initial_state: tf.Tensor, model: tf.keras.Model, optimizer: tf.keras.optimizers.Optimizer, gamma: float, max_steps_per_episode: int) -> tf.Tensor:\n",
    "    \"\"\"Runs a model training step.\"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Run the model for one episode to collect training data\n",
    "        action_probs, values, rewards = run_episode(initial_state, model, max_steps_per_episode) \n",
    "\n",
    "        # Calculate expected returns\n",
    "        returns = get_expected_return(rewards, gamma)\n",
    "\n",
    "        # Convert training data to appropriate TF tensor shapes\n",
    "        action_probs, values, returns = [tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
    "\n",
    "        # Calculating loss values to update our network\n",
    "        loss = compute_loss(action_probs, values, returns)\n",
    "\n",
    "    # Compute the gradients from the loss\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # Apply the gradients to the model's parameters\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    episode_reward = tf.math.reduce_sum(rewards)\n",
    "\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 914:   9%|██▎                      | 914/10000 [05:54<58:45,  2.58it/s, episode_reward=200, running_reward=195]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solved at episode 914: average reward: 195.02!\n",
      "Wall time: 5min 54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "max_episodes = 10000\n",
    "max_steps_per_episode = 1000\n",
    "\n",
    "# Cartpole-v0 is considered solved if average reward is >= 195 over 100 \n",
    "# consecutive trials\n",
    "reward_threshold = 195\n",
    "running_reward = 0\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "\n",
    "with tqdm.trange(max_episodes) as t:\n",
    "    for i in t:\n",
    "        initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "        episode_reward = int(train_step(initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
    "\n",
    "        running_reward = episode_reward*0.01 + running_reward*.99\n",
    "\n",
    "        t.set_description(f'Episode {i}')\n",
    "        t.set_postfix(episode_reward=episode_reward, running_reward=running_reward)\n",
    "\n",
    "        # Show average episode reward every 10 episodes\n",
    "        if i % 10 == 0:\n",
    "            pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
    "\n",
    "        if running_reward > reward_threshold:  \n",
    "            break\n",
    "\n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
